<html class="notion-html"><head lang="en"><meta charset="utf-8"/><meta content="width=device-width,height=device-height,initial-scale=1,maximum-scale=1,user-scalable=no,viewport-fit=cover" name="viewport"/><title>10. Interpretability in Data-Centric ML</title><meta content="en_US" property="og:locale"/><link href="data:image/png;base64,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" rel="shortcut icon" type="image/x-icon"/><link href="/images/logo-ios.png" rel="apple-touch-icon"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="telephone=no" name="format-detection"/><meta content="no" name="msapplication-tap-highlight"/><link href="assets/css/e218a1aef6df86309cb95b61e446888efa3b0379.css" media="print" rel="stylesheet"/><link href="assets/css/e7df85b6a5b33bc52629df6d3bd37d197c7a873d.css" rel="stylesheet"/><meta content="Edouard d'Archimbaud" name="title"/><meta content="Edouard d'Archimbaud official website" name="description"/><link href="assets/css/e1d809d762eeca23edf0cb31bb17bf3c703085f5.css" rel="stylesheet"/><style type="text/css"></style></head><body class="notion-body"><style>body{background:#fff}body.dark{background:#191919}@keyframes startup-shimmer-animation{0%{transform:translateX(-100%) translateZ(0)}100%{transform:translateX(100%) translateZ(0)}}@keyframes startup-shimmer-fade-in{0%{opacity:0}100%{opacity:1}}@keyframes startup-spinner-rotate{0%{transform:rotate(0) translateZ(0)}100%{transform:rotate(360deg) translateZ(0)}}#initial-loading-spinner{position:fixed;height:100vh;width:100vw;z-index:-1;display:none;align-items:center;justify-content:center;opacity:.5}#initial-loading-spinner svg{height:24px;width:24px;animation:startup-spinner-rotate 1s linear infinite;transform-origin:center center;pointer-events:none}#skeleton{background:#fff;position:fixed;height:100vh;width:100vw;z-index:-1;display:none;overflow:hidden}#initial-loading-spinner.show,#skeleton.show{display:flex}body.dark #skeleton{background:#191919}.notion-front-page #skeleton,.notion-mobile #skeleton{display:none}#skeleton-sidebar{background-color:#fbfbfa;box-shadow:inset -1px 0 0 0 rgba(0,0,0,.025);display:flex;width:240px;flex-direction:column;padding:12px 14px;overflow:hidden}body.dark #skeleton-sidebar{background-color:#202020;box-shadow:inset -1px 0 0 0 rgba(255,255,255,.05)}#skeleton.isElectron #skeleton-sidebar{padding-top:46px}#skeleton .row{display:flex;margin-bottom:8px;align-items:center}#skeleton .row.fadein{animation:1s ease-in 0s 1 normal both running startup-shimmer-fade-in}#skeleton .chevron{width:12px;height:12px;display:block;margin-right:4px;fill:rgba(227,226,224,.5)}body.dark #skeleton .chevron{fill:#2f2f2f}.startup-shimmer{background:rgba(227,226,224,.5);overflow:hidden;position:relative}body.dark .startup-shimmer{background:#2f2f2f}.startup-shimmer::before{content:"";position:absolute;height:100%;width:100%;z-index:1;animation:1s linear infinite startup-shimmer-animation;background:linear-gradient(90deg,transparent 0,rgba(255,255,255,.4) 50%,transparent 100%)}body.dark .startup-shimmer::before{background:linear-gradient(90deg,transparent 0,rgba(86,86,86,.4) 50%,transparent 100%)}#skeleton .icon{width:20px;height:20px;border-radius:4px}#skeleton .text{height:10px;border-radius:10px}#skeleton .draggable{-webkit-app-region:drag;position:absolute;top:0;left:0;width:100%;height:36px;display:none}#skeleton.isElectron .draggable{display:block}</style><style id="scroll-properties"></style><div id="notion-app"><div class="notion-app-inner notion-light-theme" style='color: rgb(55, 53, 47); fill: currentcolor; line-height: 1.5; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; -webkit-font-smoothing: auto; background-color: white;'><div style="height: 100%;"><div class="notion-cursor-listener" style="width: 100vw; height: 100%; position: relative; display: flex; flex: 1 1 0%; background: white; cursor: text;"><div class="" style="display: flex; flex-direction: column; width: 100%; overflow: hidden;"><div style="max-width: 100vw; z-index: 100; background: white; user-select: none;"><div class="notion-topbar" style="width: 100%; max-width: 100vw; height: 45px; opacity: 1; transition: opacity 700ms ease 0s, color 700ms ease 0s; position: relative;"><div style="display: flex; justify-content: space-between; align-items: center; overflow: hidden; height: 45px; padding-left: 12px; padding-right: 10px;"><div class="notranslate" style="display: flex; align-items: center; line-height: 1.2; font-size: 14px; height: 100%; flex-grow: 0; margin-right: 8px; min-width: 0px;"><div class="notion-selectable notion-page-block" data-block-id="d8397a78-1321-456c-9c10-1feea7a42b60" style="display: flex; align-items: center; min-width: 0px;"><a href="edouard-d-archimbaud.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üë®‚Äçüíª" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üë®‚Äçüíª</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Edouard d‚ÄôArchimbaud</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div class="notion-selectable notion-page-block" data-block-id="bb024d08-b3b1-4dbd-bafc-0fa8d6a316c2" style="display: flex; align-items: center; min-width: 0px;"><a href="data-centric-ai.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéØ" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéØ</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Data-centric AI</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 6px; padding-right: 6px; color: rgb(55, 53, 47);" tabindex="0"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><span class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 240px;">10. Interpretability in Data-Centric ML</span></div></div><div style="flex-grow: 1; flex-shrink: 1;"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="searchNew" style="width: 14px; height: 14px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg>Search</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0">Duplicate</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: flex; align-items: center; justify-content: center; width: 32px; height: 28px; border-radius: 3px;" tabindex="0"><svg class="dots" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden;" viewBox="0 0 13 3"><g><path d="M3,1.5A1.5,1.5,0,1,1,1.5,0,1.5,1.5,0,0,1,3,1.5Z"></path><path d="M8,1.5A1.5,1.5,0,1,1,6.5,0,1.5,1.5,0,0,1,8,1.5Z"></path><path d="M13,1.5A1.5,1.5,0,1,1,11.5,0,1.5,1.5,0,0,1,13,1.5Z"></path></g></svg></div><div style="flex: 0 0 auto; width: 1px; height: 16px; margin-left: 8px; margin-right: 8px; background: rgba(55, 53, 47, 0.16);"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="notionLogo" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 120 126"><path d="M 20.6927 21.9315C 24.5836 25.0924 26.0432 24.8512 33.3492 24.3638L 102.228 20.2279C 103.689 20.2279 102.474 18.7705 101.987 18.5283L 90.5477 10.2586C 88.3558 8.55699 85.4356 6.60818 79.8387 7.09563L 13.1433 11.9602C 10.711 12.2014 10.2251 13.4175 11.1939 14.3924L 20.6927 21.9315ZM 24.8281 37.9835L 24.8281 110.456C 24.8281 114.351 26.7745 115.808 31.1553 115.567L 106.853 111.187C 111.236 110.946 111.724 108.267 111.724 105.103L 111.724 33.1169C 111.724 29.958 110.509 28.2544 107.826 28.4976L 28.721 33.1169C 25.8018 33.3622 24.8281 34.8225 24.8281 37.9835ZM 99.5567 41.8711C 100.042 44.0622 99.5567 46.2512 97.3618 46.4974L 93.7143 47.2241L 93.7143 100.728C 90.5477 102.43 87.6275 103.403 85.1942 103.403C 81.2983 103.403 80.3226 102.186 77.4044 98.54L 53.5471 61.087L 53.5471 97.3239L 61.0964 99.0275C 61.0964 99.0275 61.0964 103.403 55.0057 103.403L 38.2148 104.377C 37.727 103.403 38.2148 100.973 39.9179 100.486L 44.2996 99.2717L 44.2996 51.36L 38.2158 50.8725C 37.728 48.6815 38.9431 45.5225 42.3532 45.2773L 60.3661 44.0631L 85.1942 82.0036L 85.1942 48.4402L 78.864 47.7136C 78.3781 45.0351 80.3226 43.0902 82.7569 42.849L 99.5567 41.8711ZM 7.5434 5.39404L 76.9175 0.285276C 85.4366 -0.445402 87.6285 0.0440428 92.983 3.93368L 115.128 19.4982C 118.782 22.1747 120 22.9034 120 25.8211L 120 111.187C 120 116.537 118.051 119.701 111.237 120.185L 30.6734 125.05C 25.5584 125.294 23.124 124.565 20.4453 121.158L 4.13735 99.9994C 1.21516 96.1048 0 93.191 0 89.7819L 0 13.903C 0 9.5279 1.94945 5.8785 7.5434 5.39404Z"></path></svg>Try Notion</div></div></div><div style="width: calc(100% - 0px); user-select: none;"></div></div><div class="notion-frame" style="flex-grow: 0; flex-shrink: 1; display: flex; flex-direction: column; background: white; z-index: 1; height: calc(100vh - 45px); max-height: 100%; position: relative; width: 1920px;"><div class="notion-scroller vertical" style="display: flex; flex-direction: column; z-index: 1; flex-grow: 1; position: relative; align-items: center; margin-right: 0px; margin-bottom: 0px; overflow: hidden auto;"><div style="position: absolute; top: 0px; left: 0px;"><div></div></div><div class="whenContentEditable" data-content-editable-root="true" style="caret-color: rgb(55, 53, 47); width: 100%; display: flex; flex-direction: column; position: relative; align-items: center; flex-grow: 1; --whenContentEditable--WebkitUserModify:read-write-plaintext-only;"><span style="height: 1px; width: 1px;"></span><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0; z-index: 2;"></div><div style="width: 100%; display: flex; justify-content: center; z-index: 3; flex-shrink: 0;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div style="max-width: 100%; padding-left: calc(96px + env(safe-area-inset-left)); width: 100%;"><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; pointer-events: none;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px; border-radius: 0.25em; flex-shrink: 0; position: relative; z-index: 1; margin-left: 3px; margin-bottom: 0px; margin-top: 96px; pointer-events: auto;"><div style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px;"><div style="height: 78px; width: 78px; font-size: 78px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notion-page-controls" style='display: flex; justify-content: flex-start; flex-wrap: wrap; margin-top: 8px; margin-bottom: 4px; margin-left: -1px; color: rgba(55, 53, 47, 0.5); font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; height: 24px; pointer-events: auto;'></div></div><div style="padding-right: calc(96px + env(safe-area-inset-right));"><div><div class="notion-selectable notion-page-block" data-block-id="bb8f21af-4bff-4085-b2f3-0222d5cd94bc" style='color: rgb(55, 53, 47); font-weight: 700; line-height: 1.2; font-size: 40px; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; cursor: text; display: flex; align-items: center;'><div contenteditable="false" data-content-editable-leaf="true" placeholder="Untitled" spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">10. Interpretability in Data-Centric ML</div></div><div style="margin-left: 4px;"></div></div></div></div></div><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div contenteditable="false" data-content-editable-void="true" style="padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right)); max-width: 100%; width: 100%;"></div></div></div></div><main style="display: flex; width: 100%; justify-content: center; padding-top: 5px;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div class="notion-page-content" style="flex-shrink: 0; flex-grow: 1; max-width: 100%; display: flex; align-items: flex-start; flex-direction: column; font-size: 16px; line-height: 1.5; width: 100%; z-index: 4; padding-bottom: 30vh; padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right));"><div class="notion-selectable notion-text-block" data-block-id="266240e8-14fa-4ebd-afaa-4802ae180e39" style="width: 100%; max-width: 1728px; margin-top: 2px; margin-bottom: 0px;"><div style="color: inherit; fill: inherit;"><div style="display: flex;"><div contenteditable="false" data-content-editable-leaf="true" placeholder=" " spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">Sure, All right we'll get started then. Hello everyone! Um, thank you all for being here. Hope you learned something today. Uh so I'm Ola I am a PhD student in the data to AI lab here at MIT So my lab does a lot of this like data Centric AI stuff you've been learning about and in particular we do a lot of focus on like deploying machine learning models into real domains and seeing what happens. And one of the things that happens is issues with interpretable features and interpretable data. So that's what we're going to talk about today. So if you start like reading into the data Centric AI literature which I hope you all spend a little time doing after this class, you might start to notice these references here and there to this concept of interpretable features. Like there's a lot of algorithms that claim to generate interpretable features or users who are like we need human understandable features, interpretable feature selection, and extraction. This is like this topic that kind of gets mentioned in the background a lot. So today we're going to talk about what that actually means, what are interpretal features and why do we care so quick. Road map I'm going to start off by talking about interpretable machine learning in general because I'm not sure if everyone in here has even heard of that concept. Then we're going to talk a bit about why we care about interpretable features when we care about them. Um, we're going to talk about what exactly it means for a feature to be interpretable because that's kind of like a buzzword interpretable. And then we'll talk a little bit about what steps we need to do and what we need to be careful about to actually get interpretable features in our machine learning. All right. Can I get a quick show of hands? How many people here like know what interpretable machine learning means A little bit. All right. Awesome. I'm glad I have this section then so we're going to talk a bit. This is a maybe a phrase at some point you'll run into. It's kind of getting a lot of heat these days. People think it's important. It is important to give you the quick spiel about what it means from machine learning to be interpretable as I'm hoping you guys sort of have learned by this point in machine learning. we take data so features, we input them into a machine learning model. The machine learning model does some math and it gives us an output. So for example, maybe we have some information about houses or blocks of houses and then we're going to get a house price prediction coming out. Um, and the question then is in this black box in the middle what exactly is happening? Because that could be like a linear regression model where we're just multiplying a number by every feature and we get an output. Or that could be like a multi-billion parameter neural network. Um, depending on what we have, it might be very easy or very hard to understand how this machine learning model is actually getting that prediction of 198 000. So why do we care? If we know what's happening on the inside? Like if the model is doing well, why does it matter how it's doing it well? There's like three major reasons so the first comes to debugging and validation. So maybe we've trained our model and we get like a good training performance. We get a great testing performance, but does that mean it's actually going to work when we release it into the world? [Music] Um, it could be as I Think you've been learning in this class that the data is not great. It's showing around the data and that can come back to bite you. So for example, here we have a machine learning model that's trained to predict the risk of death from pneumonia of patients being admitted to the hospital. We have two forms of explanations of that machine learning's logic. Or we have two different like explanations of features, right? So this first chart here. what way we can interpret this is as the age of patients increase. This model tends to predict that they're more likely to die of pneumonia, which is exactly what we know in the medical community. That makes sense. What the second chart is saying is that if a patient has asthma, their risk of dying from pneumonia decreases a little bit, which is the exact opposite of what we think we know from the machine learning from the medical community. And the issue here is that in the past when patients come into the hospital with asthma, they immediately get intensely treated for pneumonia because their risk is so high, which actually results in a slightly reduced death rate for patients with asthma. But it also means that if we were to deploy this model into the real world, we would suddenly have a lot of people with asthma dying probably, which is not great. The second area we might need terminal machine learning. Generally, if you release your model out into the wild and it does something and it does something bad, you want to know why it did something bad. like your self-driving car hits a person. You need to know why that happens so you can fix it or people aren't going to want to deal with your machine learning model anymore. And finally, the area that my lab works with a lot. So I'll get some examples from this: I Think in. It's safe to say in most domains these days, machine learning models aren't like completely replacing humans. You still have humans that are using the models to some extent. and these humans have a lot of experience. They have a lot of information. Um, they're experts at their field. and now we're tossing in a machine learning model that's going to be like hey, the answer is 17. And you now need to understand what on Earth that 17 means. Especially if you disagree with it. Like if your machine learning model is coming in and telling you something that to you makes zero sense, you're not going to want to use it if you don't understand where that came from and so you can judge who's right, you are the model. So generally speaking, we need interpretal machine learning in general. Anytime we call it, the problem formulation is incomplete. What that basically means is you don't know enough about the entire world to perfectly in a perfect accuracy actually measure the quality of your machine learning model. which is in most cases, if there's any kind of risk like um, if you have a self-driving car. Clearly, there's risk. Actually, in most cases there's some degree of risk to a incorrect answer. Otherwise, why would you need machine learning at all if you don't care what the answer is. And anytime you have humans involved in decision making, Cool. So that's interpretable machine learning, are there any questions? At this point? we're all cool on Okay Cool. So this is a class on data Centric machine learning. Uh, not just machine learning in general. So we're not going to go into too much detail on interpretal models. Instead, we're going to talk about a super important part of interpretability, which is the interpretal features. Interpretability of model starts at the feature level [Music] So to motivate that a bit, let's go back to the example I introduced earlier. So we have a machine learning model. It takes in a bunch of information about blocks of houses in California This is the California Housing data set. like the location of the block of houses, how many people live there, median income. So on we're going to train some various models. We don't care too much about what's happening in this black box. We're training various models to do um to predict the median price. I'm going to go through and I'm going to show you guys just a few like explanations I've generated on this model of a or a few explanations I've generated on this data of models trained on different types of features. So here we have What's called the decision tree explanation. People like decision trees. They say they're super interpretable. Um, this is going to explain I'm going to take some block of houses and in this block of houses X7 is less than 0.12 So I'm going to head in. What is that to the right? I'm going to get okay, Um, X1 is less than negative 177 you know I'm going to work my way down the tree. I'm going to see what the prediction is So Based on this explanation, how many people here would say they understand how this model is making predictions on house prices? All right, some of you are smarter than I am, that's for sure. We have a few people kind of understanding um, personally. I Have no idea how this model is making any predictions really on house prices. If I need to explain to someone who doesn't do machine learning I wouldn't I wouldn't know where to begin. Um, the issue here is that X7 is kind of like a meaningless thing. I don't know what any of these numbers mean. Um, so I'm left knowing nothing. in reality what these numbers are is a principal component analysis reduction of the original data set. I'm not going to go into too much detail about that, but basically we've taken our features and we've collapsed them down into a smaller feature set. and the result is that I don't know what the model is doing, so this might seem a bit contrived to some of you. [Music] Um, so let's go back a step. Here's another explanation of a machine learning model on the California Housing data set. In this case, I Used a automatic feature generation algorithm to compute something like 400 features automatically. I did some feature selection, trained a model, took the most important features, and each bar in this chart represents generally how important that feature is the mall prediction. Um, so this is a bit better. I Think we all know like what the word latitude means. That's a good start. It's still a bit hard. Yes, yeah, that's a great question. So depending on the algorithm you're using to compute importance, the actual like unit could be different. So in this case it's a bit of like a unitless importance. It's a measure of how much if you modify these inputs, how much will the output change? So in this case it's like relative. So these latitude plus longitude feature if you change it a bit, will result in a pretty big change in the model prediction. um. and then the latitude mod 1 will be a little bit less so I answer your question. Um, In some types of explanations which we're going to see later, those bars actually represent exactly how much of the output we're getting from that input. Um, yeah, so this is getting a bit better. Um, but it's still kind of I Don't know I find it hard to reason about things like cosine of longitude and tangent of latitude. Also, the same information is popping up in like every feature here. So if I was a real estate agent trying to use this model and price a house based on this, I don't know that I would necessarily love this machine learning model or I want to use it all that much. I might prefer something like this Um, which has significantly easier to understand features. It's not perfect I Don't know, You know. Latitude and longitude are still a bit hard to deal with, but at least I I can understand where everything is here and it's different pieces of information. I Think it's also important to note that this model and this model had basically the exact same performance. in this case. I'm going to talk a bit about performance later on. All right, so you might still think this is kind of contrived that I'm just throwing a bunch of complicated features in, but this is the kind of stuff that we deal with in the real world. So like I said, my lab works on a lot of deployment projects. We try to deploy machine learning in the real world and see what the problem is. Um, so the first thing we learn is that yes, we do need to explain our models in a lot of cases and we also learned that interpretability of features is important. So uh, in this one study we ran, we worked with a team of child welfare screeners for the course of about a year and what these screeners did was they would take an information they'd get about a potential case of child abuse, so someone would report to them I Think there's potentially a case of child abuse here. They would consider all of the factors of the case and then they screen enter screen out meaning they either investigate the case further or they leave it for the time being. As you can imagine, this is a very high risk decision. So our collaborators came in and they trained a machine learning model that looked at all of this information about the history of the child, the past Court involvements, past referrals, demographic information. All that and it would compute a 1 through 20 risk score. So 20 means the data suggests that this child is at very high risk. One suggests there's no risk here, and as you can imagine, when you're giving somebody a number that's going to determine the fate of a child's life, they like to be confident about that information. And so they wanted an explanation of that number. So we gave him an explanation of that number. Um, and so this here. This is what I was talking about earlier. These are actual contributions. So um, a big red bar here means that that feature is greatly increasing the risk to this child. We'd also have negative blue bars which suggest that this feature is lowering the risk of the child. In this case, you can see for example, if a child is an infant generally there was a score will increase a lot because infants aren't much higher or much more vulnerable. so they tend to be investigated more. And what we found when we we ran through a series of user studies over the course of the year we went through a bunch of the stuff that's like state of the art in machine learning, explainability, and Interpol models and what we learned is that pretty much every issue that the screeners had with this was that the features to them were not meaningful. They were not useful features. So for example, we had a bunch of features that were like a number of child days that the child was in a placement in the past like 365 Days 7, 30 days, 180 days, 90 days. We had a lot of use of repeated features that they weren't used to dealing with. Um, we had a lot of this like machine learning language so lots of like one hot encoded features. We would say things like roll the child in focus is alleged victim is true as opposed to just that the role is alleged victim or like child has siblings false instead of just child doesn't have sibling. um I Think importantly, there's a lot of features there that for whatever reason the machine learning model was using kind of a lot in very specific cases, but that didn't seem important at all to the screeners like Um, there was at least a few cases where the parents didn't have their date of birth like the date birth was just saying that was considered extremely protective. Like that lowered the risk score a lot. Um, Which, regardless of why that was actually happening, the Child Welfare screen is the moment they saw that they were like I'm done with this model I'm not using this. This makes no sense. Um so after all of this study, what we did was our team, our collaborators went in. They retrained the model using a significantly smaller set of features and they actually got them all had basically the same performance and was significantly easier to use. Which brings me to I Think this important Point Some of you might have already started thinking about this a little bit, which is if you're limiting your features to be important if you're avoiding all these feature engineering tools that I talked about earlier Is that not going to affect the performance of the model? Um, this chart I have here is a Um like a plot from a DARPA paper that gets mentioned a lot as evidence that there's this thing called the performance interpretability trade-off But this plot is not based on any real data. All of those points were just drawn for. Take an illustrative example, and it turns out that in the real world, often when we care about interpretability of both the model and the features, we actually find that we get more efficient training, better generalization of models. We get fewer adversarial examples because the data we're using, the features we're using we know are actually important, so your model has less chance to catch all these spurious examples. There will be cases where if you try to factor an interpretability, you're going to get lower performance. That's inevitable. as that happens at that point, there's no good answer here for you. It's not like there's a thing you're supposed to do, it just depends on the field. So in, like child welfare, if we didn't have an interpretal model, they wouldn't use the model. Period. So interpretability is non-negotiable In other cases, if there's less risk, you might prefer a higher performing model. It just depends on your users. All right. I'm gonna go quickly now through I guess I don't have any questions here. It's a good all good. So now I'm going to go quickly through what exactly it means for a feature to be interpretable. So I gave some examples earlier. I Don't know, you know how much how much that meant to you. but um, there's a few different properties that we need to think about when we think about the interpretability of features, so be somewhat unsatisfying. Answer to the general question of what are interpretable features is the features that are the most useful and meaningful to your user. And as I'm going to talk about later, there's really no way out of it. If you want your model to be used by someone, you need to talk to the person who's going to be using it to identify what they want to use. So I'm just going to go through an example of a few different features here that have different levels of interpretability and talk a bit about different properties that we can consider when thinking about the interpretability of features. So we're going back again to that housing example. Hopefully you'll remember so inputs about information about blocks of houses output is average house price A median house price. Um, these are five features that you could use. So the first property of interpretability of a feature is its readability. This is where a lot of people end when thinking about interpretability. Features Readable means: Do I understand what this feature is even talking about at all? So in the decision tree, example I had earlier, you didn't know what x of seven was. that was not a readable feature. X12 In this case, that's not a readable feature. This is usually a very easy problem to solve. If you're going to be explaining your model, make sure you have like some sort of code some sort of description of your features. Taking a step further, we have understandability, So that's all right. I know what the feature is referring to, but can I actually use this feature in any meaningful way? Can I logic about it? Um, so something like a normalized measure of median income so like 0.8 out of one is potentially less useful than the actual median income number like 90 000 or whatever. Then depending on your domain, this might be more or less important. But relevance of features can be extremely useful if you want users to actually use your model and trust it. If you believe that your model is good and you need your users to trust it, you want to make sure that they look at the explanation and go. Yes, this looks reasonable. So if you have a lot of features in there like the missing date of birth example I gave earlier, that's not relevant. it's going to turn off your users something like common house color I'm not like an expert on house pricing. maybe common house color is actually really important, but to me I though I don't feel like that should be that important in a model that's using it heavily is going to lose my trust. [Music] And then finally, I'm not going to talk too much about this because it's sort of an issue all on its own. But sometimes, especially if your users aren't like machine learning experts, it can be helpful to collapse your information down into like very digestible Concepts like take your information with the neighborhood into something abstract. The danger of that of course is depending on how you do this, you could completely mislead your user. but something to consider. All right. Any questions here? Yes, yeah, absolutely so. I Think that's first of all, it's a very good point that depending on your users and also this might not be a great example of abstract of abstract features. I Think a good example of this is with like college rankings where depending on who you are you might prefer to get all of the information about the college. but a lot of times college rankings are shared as like just general like education quality Sports Quality. Um, and that's the kind of thing that attracts a lot of people because it's just faster and easier to parse. I Think college is also college rankings. There's also a great example of how you can completely change your rankings depending on how you do those categories. so it's something to be careful about. but it if you have complex Concepts Being able to collapse them down something abstract can be beneficial. Answer question: [Music] Um, cool. All right. So now perhaps the most practical part of the talk, we've talked about why we need Interpol features and what they are. So how do we get them? I Really like this quote from a data scientist I Think from a consulting firm in this study on machine learning explanations and deployment. Who said that feature engineering is the first step to making an interpretable model even if we don't have a model yet. So if you think that you need your model to be interpretable, you need to think about that interpretability at the feature engineering stage stage. [Music] So let's talk about a few methods that exist that sort of help with feature interpretability. The first sort of inevitable one is to include the users. and there's a few ways to do this. And by user here, I mean whoever's going to be looking at your explanations of the machine learning model. So like in our channel, for example, this was child welfare screeners. [Music] Um, I'm going to go through this sort of briefly. This is a major Concept in the human computer interaction domain, the iterative design process. This is sort of a formal way of incorporating users. Anytime you're designing anything that involves them, this is like a big topic, so you're going to get like a very quick intro to it. But basically the idea is that you're including the end users of your product in every step of the process. So after you've planned what you're going to do, you talk about the requirements of the users. So you're already from the beginning. You're interviewing your users. You're figuring out what features they find important. When you test your features or test your product, in general, you're testing your explanation. You're doing that through user studies, not just performance testing. Um, you evaluate on your users and so on. So obviously this takes a lot of time. It takes a lot of time from your users, so it might not always be possible. but there are things we can do to sort of reduce the amount of time that this takes. So one example is the field of collaborative feature engineering. So these are a series of systems that exist that make it easier for people, especially non-machine learning experts to come in and help with your future engineering. So if you have somebody who, um, is like, if you feel like a person, especially a person who's not a machine learning expert who's finding the things that are most important, they're more, It's more likely that those people are going to understand the features if they're the kind of people who've generated themselves. So I'll go through a few examples of systems for collaborative featured engineering that exist. The first is Flock. So this is a system that uses this concept of comparisons to generate to help people generate features easier. Uh, so Flock uses a system I'm about to go over to combine machine engineered features with crowd generated features. Um, and if you just ask a person to like, look at something and say why, why you think that is something to generate features, they often don't generate useful information, so this system is going to help with that. So to go through this crowd generating features concept, I'll go through an example from the paper that they use to evaluate Um. So the first or one one example of that they use in the paper was distinguishing between a painting by Monet versus a painting by other people who have similar paintings to Monet Um, So the the system starts with asking users to or teaching users a little bit about the domain. In this case, we can show them a series of Monet paintings um so that they can get a sense of the problem. Ideally, if you have domain expert users, they already know this and this is important. We're showing them. We're asking them to do a comparison, not just a description. So we show them two paintings one by Monet, one by not Monet um and we asked them all right, which one's the Monet and tell us why. And then we ask them to just write a natural human sentence to describe it so no code required, no machine learning expertise. All they're doing is is writing sentences, which most people can do so. for example, the first painting is a Monet because Lily's in it looks like modest Style the second one isn't Monet she doesn't put people in his paintings. Then we split the description by conjunctions so punctuation and and ores. We cluster those phrases based on similar Concepts And then we show these clusters back to our crowd again and we ask them to provide a descriptions like a feature label in the formal question. So something like does the painting have flowers or lilies might describe this feature and that's our feature. So we now know any. any painting that was described as these phrases has flowers and lilies And that's our final question, our final Um feature. And here clearly this is a feature that people are actually using because this is something that they described as helping them so it choose between those two classes. [Music] All right. So the results of this using this combination of machine engineered features and the crowdsourced features Uh, resulted in better performance than just using machine engineered features It results in a better performance than using the data directly, and also better than having the people just classify themselves. And like I said, it generates features that should be interpretable because it actually is the kind of information people are using to make these distinct two distinguished classes. Another example of collaborative feature engineering is Ballet. This was made by a student in the Data Ji Lab as well. Um, I'm not going to go into details about this either because this is kind of a very big important system, but basically it's abstracting away everything that's not related to feature engineering to allow somebody to focus just on the features. So the way ballet works is a person can come in. They write a single function to generate a single data set off of the data set, or offer to generate a single feature from the data set. For example, this is just all the Python code you need to convert age into a higher or lower than 30. Um, and then this automatically gets Incorporated with everyone else's features that they all write. Um. It goes through uses Automl tactics and evaluation to get immediate feedback on the quality of that feature Um, and then it automatically those features selections like the ones that are most important. So this does require people to know how to write Python code, but it only requires Python code rather than having to understand machine learning as a whole. The second method for having interpretal features in your explanation is to use interpretable feature transforms and explanation transforms. This is my research, so my team has developed this Library Pyro that handles the transformations of features automatically to give you interpretable explanations. I'm going to go through and give you an example of what that looks like. Again, can't go in too much detail about this, but just to give you the overview in the explanation generation system. in general, we start off with features in some sort of feature space. So we have some sort of data. Basically, data comes to us. We have to run some sort Transformations on that data to get it ready for machine learning. So for example, we have to one hot and code features. um Forum machine learning model. to use them, we might have to standardize. We also need to have some sort of transformations to get ready for the explanation algorithm. But I Think the important thing to focus on here is that we need to transform data for the model. And finally, there's some sort of Transformations We'll want to make the explanation more useful to users. Um, so this is this is sort of the space of the features that are interpretable. I've been talking about this whole time. so if this is confusing because I'm kind of going through this fast, hopefully this example will clarify a little bit on the top here. we have one of the explanations I showed earlier that I label as being pretty good, but we can do better. For example, instead of displaying median income as some standardized 0.123 unit list value, we're displaying it as the original input to the model. like thirty four thousand dollars. Um, instead of displaying things like longitude, latitude, which again, aren't as interpretable, we could show it as the city itself if we don't really care about the exact details of longitude latitude. Um, we can show one hot coded features as the language that we understand and Pyro handles all the transformations of the explanation under the hood to get this more useful explanation. And this is something you're going to be playing around a little bit with the lab, so hopefully if it doesn't make a lot of sense yeah, it will, then Are there any questions I Know I Went through a lot of technical details there. Yes, Yes. After you encode features for the model, are you suggesting that you then transform the features again to make them interpretable for the end user or are you making them a typical from the beginning before you try the model? Yeah, So the question there is: Are we going to like converting to the transform into the model and then back to interpretable? Are we going directly to interpretable? All right. Uh, so it's sort of in a way both. Um. So for the generally speaking, explanation, algorithms will make an explanation on your features in the model ready state. so again, explanation of how the model used the features it was given. So from there for a lot of explanations, you can actually convert transform the explanation itself into something more interpretable, while the feature values can go directly to interpretable if that makes sense. And depending on the context, that may or may not be possible. So we do it when it's possible. Um, so like in this case, these are additive Shap values. Because they're additive, you can add them together to get the the importance of two features together. Um, if that weren't the case, you might not be able to do that. That answer your question? Cool. Um. So finally, we're going to talk a little bit about interpretable future generation. So this is the idea of automatic feature generation algorithms. There's a lot of them that exist I Showed you guys an example earlier of one that just automatically runs through a bunch of different kinds of Transformations but the features we generated as I said, might not be interpretable if you don't think about it. There are some algorithms being made that keep interpretability in mind, so they promise to generate features that are a little bit more interpretable to start with. I'm just going to go through one of them kind of quickly. the mind the Gap model. So this is an algorithm that you can use if you have binary features. So a binary feature might be something like let's say we're trying to classify animals. A binary feature might be Lasix. So either true, the animal does lay eggs or Falls Um has a backbone or doesn't have a backbone. That's another binary feature. Um. So the way that this algorithm works is it starts by randomly assigning features to groups using and or or so these are three different examples of feature groups. Uh, either the animal lays eggs or doesn't The animal may have a backbone and a tail or not. Uh, and it might be tooth or be a predator or not. From here, it looks to maximize the separation between groups based on these features. So if you can imagine these plots as being like uh, dimensionality reductions of every Row in the data set down to two axes and you can imagine, Let's say yellow is Lay's eggs and blue as doesn't lay eggs In this first term, you can see that that has a pretty big separation between the yellow and blue group. Maybe the second one is backbone and tail. and in that case, we have a much smaller separation. So it's iterating over this finding of groups to maximize the separation until it finds the set of these feature groups that maximize that separation. So here you might see something like lays eggs, backbone or tail or toothed and then Predator as three different feature groups It finds that Max my separation. and then when you start to look at the rows they set, you might notice that in one cluster, lays eggs is always zero. Um, the second feature is always one and the third one is sometimes. uh, once, zero. You know. I'd write fifty percent chance and that group we can then come in and classify as mammal. All right. So if there's one thing watching us before, I Go to this. Any questions at this point? Good. Uh, so there's one thing I Hope you guys take away from today. It's that machine learning models are only as interpretables or features. If you care about deploying your model, you should care about interpretability. If you get interpretability, you should care about the interability of your features. Um, interpretal features are just whatever is Meaningful to the user, which depends unfortunately heavily between domains. And generally you generate interpretal features by including the users using Interpol transforms and using feature. Generations algorithms that include interpretability. All right. I'm gonna give a quick Spiel for the lab. Hopefully you guys have a chance to look at the lab if you remember in one of the first slides I Talked about how validation is a really important motivator for interpretability. It helps us find for example, flaws in our data. So what we're going to do or what you're going to do in the lab is generate a bunch of explanations on different kinds of feature spaces and you're going to try to find the flaw in a sample data set. Uh, you're going to be generating probably these what's called a B swarm plot. So I Want to talk just a little bit about what this means because this is a lot of information. This is kind of a really good way to visualize your data I think um, and a machine learning explanation and the way you interpret this. the color of each Dot and this is explained in the lab as well. But just to give you guys an introduction, the color of each dot represents the feature value itself, and then the it's positioned along the x-axis represents the actual amount that it contributes to this feature's prediction. In this case, the shop value is actually in the same unit as the model output. So for example, if you look up there in the upper right hand corner, you see these red dots. That basically means that in blocks of houses with very high median income, the average house price tends to be significantly higher because of that about 150 000 tire for that specific point with a very high medium income. That sort of thing makes sense. Any questions about this or anything else? All right. Well, that's the whole presentation. So if there's any questions or points of discussion, yes [Music] by other algorithms or Converses by other. All right. So the question was interpretable, Means interpretable by humans. Is there concept of interpretable by other machines? It's interesting. Do you have an example of what something like that might look like? I'm curious. Wow. Yeah. so I Personally don't I mean I generally work with interpretable for humans. Do you mean something like could another AI understand the first AI [Music] collaboration of models I think so. Collaboration of different systems I Think it's a really interesting concept. it's not one I've really looked into I don't know if anyone else has I Think all of us would still apply. Your user can be another robot and it's still going to have its requirements I Imagine a robot will be much better at understanding more complex machine-y features than a human would, but no reason it wouldn't Wouldn't apply any other questions. Cool. Thank you all very much.</div></div></div></div></div></div><div contenteditable="false" data-content-editable-void="true" style="width: 0px;"><div style="display: none; flex-shrink: 0; pointer-events: none; width: 0px; position: absolute; right: 192px; opacity: 0;"><div style="display: flex; flex-direction: column; padding: 5px 16px; width: 340px; flex-shrink: 0; height: 100%; position: relative; pointer-events: none; z-index: 1;"><div style="position: absolute; pointer-events: none; width: 100%; height: 100%; top: -5px; background: linear-gradient(white 0px, rgba(255, 255, 255, 0) 15px);"></div></div></div></div></main><span style="height: 1px; width: 1px;"></span></div><div class="notion-presence-container" style="position: absolute; top: 0px; left: 0px; z-index: 89;"><div></div></div></div></div></div><div class="notion-peek-renderer" style="position: fixed; top: 0px; right: 0px; bottom: 0px; width: 960px; z-index: 109; transform: translateX(960px) translateZ(0px);"></div></div></div></div></div><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><script src="assets/js/1172e9111a5fb396bcb8a05870b5eabf8abf221c.js" type="text/javascript"></script><div style="width: env(safe-area-inset-bottom);"></div></body></html>