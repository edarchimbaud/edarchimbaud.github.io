<html class="notion-html"><head lang="en"><meta charset="utf-8"/><meta content="width=device-width,height=device-height,initial-scale=1,maximum-scale=1,user-scalable=no,viewport-fit=cover" name="viewport"/><title>4. Data-centric Evaluation of ML Models</title><meta content="en_US" property="og:locale"/><link href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAB29JREFUWEfFVmtsFOcVPTM7OzuPfXoXPzCGBQKCbSkm4KSFlqZx8gPUplQNCVLT9keoStO0pK1ETEsVlJIGGkVVDWlTkiiPKoFELQ19uAoCKxAoiLI0foBtzCvGXoO9tne9szszOzPfVN+s1zHYJnb+dKTRjkZ37jn33HPvtwz+zxfzafFjsdgy0zS3EEJcHMftam9vj3+aXNMmEIvFyk3T/BEh5PumaYZt2wbLsoNut/tFjuN2nz9//vp0iEyLQHV19bdUVa0zDOOzlmWBghcvhmHgcrlaJUna0dra+vZUSUyJwIoVK1bpuv6EpmkPmqbpABfvW4FcLhcl8hbDMM91dnZ++ElEbkugpqamXFXVraZpPmqapkwIuQl4rAITqJGUZbk+EAjsOX78+NBkRCYjwKxatWq9oig/13V9KZW7CE4TOWBUheLzbcqUJOl0IBB45tSpU3+bKGwcgdra2ppMJrM1m81+wzCNApCDdzMosSyAYUB7T8nRX+caITZKFADHcZYgCK9FIpFdx44d6xxLZJRAbW1tWFXVJ1RV/aGu6yGadGyS0b7Dhm3ZECURsiw74FlFQU5VnXiasGjNYluK3wqCcC0UCu22LOuFeDyec+JjsRhfUVHxQCaTqdM0bTkh1N3FYgoPo4azbSiqAbh4RGeVgrEt5HIqVE2DqqoOGZZlRwsca1b6XFRKEITGYDC4M5lMfsBEo9ENkUh4n2UR0F6PrZrWUiBjQ88TZFQbi6p4LK4AmhMydOKCxAN6Po/u7h4qNTiXq5DDIU5gExtkzNTQKbIsE5Isg2XY9QzP84ui0Wj9jEj4fhsMdF0DIYUxo2ksy0Y6RxD08XjwCwJWLzDx33MDaLnuQXeKwaXreRiG7ijg9/kcBag9i74pmJc4OU3DAOfmEAgEYJrWS6lU6teOB8rKyuSSUOhXkUjkJzzvdmSlLBWNwCAsapcK+OadFozcMC4kTESCPEhew95DKcSv5CF4XA44z/PUl44RyYhxKbhlWqCV+/1++Py+rGWRrfF4fHfRM6M9W7hw4WMzIpGdPp/X1zekoTzE4jtfJJgjpdHRrcEAh6AEnOlU8a9mYDjPgeecYQDDsB8bkFZMZSc2DNMEyzAoLS2FJEkX+5PJzS0tLQ2jG/TW2SwpmXu/NxD+3drVZYs33p3CQP8g+hQWXpHF5V4VB89YuJqW4Pfy8HAFg1J5C4BkRH4bxCIwjDwEUcSsykoYptXY39//WHNzc8eEYzj25aZHY3c9soRvyA4qYdsjIZnScfCMiQ8TEkRRhOTBTT4pyE0NV+g17btpmQiHw07lyeTAH7u6uuquXr2aurXgCTehffW+Zd1H+/7SO6CVn2jV+P3NZa48I8In0M8pSPGmZqMm+3hUKTB9UVFRAa/Xh/NtbbvOnTtXN61VbB+/60vIyYfebrj2+23vYt6s6Jx1oaCMbDbn9LQ4047Di2Rs6nITHg+PyspK6Hoe8XgcPYnEVwH8c1oEzMZ1DyV7P3r12deVNXE1+oPccO8Gn8+LSGk5NC1fGNWRnhNqOIs4Lvf5fJgxoxQ3+vrQ1NSE4eFhuqK3apq2c8oE7O3bWdx7+r0rp/9T+8ZR4bXD6TuWpVKD1Wp6EDMiYcysmgOLkJHNZ8Oim5MQlIRCEGUZly9fQU9PDzRNRT5v0Lgf67rujNxE1zgP2NvBtpZ97u9cdnjtW2f5Pf+4EFqpDKfu1DXVmf2SYABz5t8Bzs0jl3PWuWM207TQ1t6OXDbnnBPZbBbpdLpvcHBwJYBLUyZAA2u//JnnN333Kz99+c/n1l3rGp6dU5L1Rj7vzLllGvDKEqJz50H2+yGJItLpDC50doJlGfj9AfqHxJG/q6vrl4qi7JgMfNwiKgbu+N6S3ctrlj6+f9+Jhz+yrxzovBT9QM8Nf553c3BxnGM2+lw1ezZssOhJ9ECSZISCQQiC4GymRCJxsq2tbQ2A9LQI2O/ApepLDl28NHDvv5v7X9l0wNhYXV29oKur+023i9S43R4HgI4eywCsi4PslREMBOH1+SB4PHRSWpqamh5KJBLttwOfUIHDdaXzqmPlp5PXhsLtCfXGgbPZ5W+cVHtKZs6sspTMi7IkrWVYFvSmJ5/HI0D2ep3qJUnC0NDQYd7j2dTY2Dhp32+7CY2X565OewLv9yVSTFoF9p0cWFf/XuZg4aMyORTQnhJE4Wcc52ZprwVRcE43wSNoSjbzW0XJPdvR0ZH5pMonPQvshvlrBgb9DcneIRgMj4MXlM3b9ibqxyYMhfwbPB5xhyiI8928GxznPpHPq09fvHjl0FSBJyVw4s27v76I6O8O3BiCyYn4azy97Rd/6n3m1sSBQGC+KIqbbNu+ZlnWq8lkcspV37YFT66s2vDk46X7ktcHYMCDs+3qc9/e27VlupVNNX7cInpnfdXD9zxQun+ofxC8i8cfjmZf+c2B7o1TTTjduHEEjmyeu3rhYt/RXDYLogN7jqSffuFI8qnpJp5q/DgCX5sJacvayCO2l1ugJNT+599Pv36kDzemmnC6cf8DZNMn5Io3zmcAAAAASUVORK5CYII=" rel="shortcut icon" type="image/x-icon"/><link href="/images/logo-ios.png" rel="apple-touch-icon"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="telephone=no" name="format-detection"/><meta content="no" name="msapplication-tap-highlight"/><link href="assets/css/e218a1aef6df86309cb95b61e446888efa3b0379.css" media="print" rel="stylesheet"/><link href="assets/css/e7df85b6a5b33bc52629df6d3bd37d197c7a873d.css" rel="stylesheet"/><meta content="Edouard d'Archimbaud" name="title"/><meta content="Edouard d'Archimbaud official website" name="description"/><link href="assets/css/e1d809d762eeca23edf0cb31bb17bf3c703085f5.css" rel="stylesheet"/><style type="text/css"></style></head><body class="notion-body"><style>body{background:#fff}body.dark{background:#191919}@keyframes startup-shimmer-animation{0%{transform:translateX(-100%) translateZ(0)}100%{transform:translateX(100%) translateZ(0)}}@keyframes startup-shimmer-fade-in{0%{opacity:0}100%{opacity:1}}@keyframes startup-spinner-rotate{0%{transform:rotate(0) translateZ(0)}100%{transform:rotate(360deg) translateZ(0)}}#initial-loading-spinner{position:fixed;height:100vh;width:100vw;z-index:-1;display:none;align-items:center;justify-content:center;opacity:.5}#initial-loading-spinner svg{height:24px;width:24px;animation:startup-spinner-rotate 1s linear infinite;transform-origin:center center;pointer-events:none}#skeleton{background:#fff;position:fixed;height:100vh;width:100vw;z-index:-1;display:none;overflow:hidden}#initial-loading-spinner.show,#skeleton.show{display:flex}body.dark #skeleton{background:#191919}.notion-front-page #skeleton,.notion-mobile #skeleton{display:none}#skeleton-sidebar{background-color:#fbfbfa;box-shadow:inset -1px 0 0 0 rgba(0,0,0,.025);display:flex;width:240px;flex-direction:column;padding:12px 14px;overflow:hidden}body.dark #skeleton-sidebar{background-color:#202020;box-shadow:inset -1px 0 0 0 rgba(255,255,255,.05)}#skeleton.isElectron #skeleton-sidebar{padding-top:46px}#skeleton .row{display:flex;margin-bottom:8px;align-items:center}#skeleton .row.fadein{animation:1s ease-in 0s 1 normal both running startup-shimmer-fade-in}#skeleton .chevron{width:12px;height:12px;display:block;margin-right:4px;fill:rgba(227,226,224,.5)}body.dark #skeleton .chevron{fill:#2f2f2f}.startup-shimmer{background:rgba(227,226,224,.5);overflow:hidden;position:relative}body.dark .startup-shimmer{background:#2f2f2f}.startup-shimmer::before{content:"";position:absolute;height:100%;width:100%;z-index:1;animation:1s linear infinite startup-shimmer-animation;background:linear-gradient(90deg,transparent 0,rgba(255,255,255,.4) 50%,transparent 100%)}body.dark .startup-shimmer::before{background:linear-gradient(90deg,transparent 0,rgba(86,86,86,.4) 50%,transparent 100%)}#skeleton .icon{width:20px;height:20px;border-radius:4px}#skeleton .text{height:10px;border-radius:10px}#skeleton .draggable{-webkit-app-region:drag;position:absolute;top:0;left:0;width:100%;height:36px;display:none}#skeleton.isElectron .draggable{display:block}</style><style id="scroll-properties"></style><div id="notion-app"><div class="notion-app-inner notion-light-theme" style='color: rgb(55, 53, 47); fill: currentcolor; line-height: 1.5; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; -webkit-font-smoothing: auto; background-color: white;'><div style="height: 100%;"><div class="notion-cursor-listener" style="width: 100vw; height: 100%; position: relative; display: flex; flex: 1 1 0%; background: white; cursor: text;"><div class="" style="display: flex; flex-direction: column; width: 100%; overflow: hidden;"><div style="max-width: 100vw; z-index: 100; background: white; user-select: none;"><div class="notion-topbar" style="width: 100%; max-width: 100vw; height: 45px; opacity: 1; transition: opacity 700ms ease 0s, color 700ms ease 0s; position: relative;"><div style="display: flex; justify-content: space-between; align-items: center; overflow: hidden; height: 45px; padding-left: 12px; padding-right: 10px;"><div class="notranslate" style="display: flex; align-items: center; line-height: 1.2; font-size: 14px; height: 100%; flex-grow: 0; margin-right: 8px; min-width: 0px;"><div class="notion-selectable notion-page-block" data-block-id="d8397a78-1321-456c-9c10-1feea7a42b60" style="display: flex; align-items: center; min-width: 0px;"><a href="edouard-d-archimbaud.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üë®‚Äçüíª" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üë®‚Äçüíª</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Edouard d‚ÄôArchimbaud</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div class="notion-selectable notion-page-block" data-block-id="bb024d08-b3b1-4dbd-bafc-0fa8d6a316c2" style="display: flex; align-items: center; min-width: 0px;"><a href="data-centric-ai.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéØ" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéØ</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Data-centric AI</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 6px; padding-right: 6px; color: rgb(55, 53, 47);" tabindex="0"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><span class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 240px;">4. Data-centric Evaluation of ML Models</span></div></div><div style="flex-grow: 1; flex-shrink: 1;"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="searchNew" style="width: 14px; height: 14px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg>Search</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0">Duplicate</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: flex; align-items: center; justify-content: center; width: 32px; height: 28px; border-radius: 3px;" tabindex="0"><svg class="dots" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden;" viewBox="0 0 13 3"><g><path d="M3,1.5A1.5,1.5,0,1,1,1.5,0,1.5,1.5,0,0,1,3,1.5Z"></path><path d="M8,1.5A1.5,1.5,0,1,1,6.5,0,1.5,1.5,0,0,1,8,1.5Z"></path><path d="M13,1.5A1.5,1.5,0,1,1,11.5,0,1.5,1.5,0,0,1,13,1.5Z"></path></g></svg></div><div style="flex: 0 0 auto; width: 1px; height: 16px; margin-left: 8px; margin-right: 8px; background: rgba(55, 53, 47, 0.16);"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="notionLogo" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 120 126"><path d="M 20.6927 21.9315C 24.5836 25.0924 26.0432 24.8512 33.3492 24.3638L 102.228 20.2279C 103.689 20.2279 102.474 18.7705 101.987 18.5283L 90.5477 10.2586C 88.3558 8.55699 85.4356 6.60818 79.8387 7.09563L 13.1433 11.9602C 10.711 12.2014 10.2251 13.4175 11.1939 14.3924L 20.6927 21.9315ZM 24.8281 37.9835L 24.8281 110.456C 24.8281 114.351 26.7745 115.808 31.1553 115.567L 106.853 111.187C 111.236 110.946 111.724 108.267 111.724 105.103L 111.724 33.1169C 111.724 29.958 110.509 28.2544 107.826 28.4976L 28.721 33.1169C 25.8018 33.3622 24.8281 34.8225 24.8281 37.9835ZM 99.5567 41.8711C 100.042 44.0622 99.5567 46.2512 97.3618 46.4974L 93.7143 47.2241L 93.7143 100.728C 90.5477 102.43 87.6275 103.403 85.1942 103.403C 81.2983 103.403 80.3226 102.186 77.4044 98.54L 53.5471 61.087L 53.5471 97.3239L 61.0964 99.0275C 61.0964 99.0275 61.0964 103.403 55.0057 103.403L 38.2148 104.377C 37.727 103.403 38.2148 100.973 39.9179 100.486L 44.2996 99.2717L 44.2996 51.36L 38.2158 50.8725C 37.728 48.6815 38.9431 45.5225 42.3532 45.2773L 60.3661 44.0631L 85.1942 82.0036L 85.1942 48.4402L 78.864 47.7136C 78.3781 45.0351 80.3226 43.0902 82.7569 42.849L 99.5567 41.8711ZM 7.5434 5.39404L 76.9175 0.285276C 85.4366 -0.445402 87.6285 0.0440428 92.983 3.93368L 115.128 19.4982C 118.782 22.1747 120 22.9034 120 25.8211L 120 111.187C 120 116.537 118.051 119.701 111.237 120.185L 30.6734 125.05C 25.5584 125.294 23.124 124.565 20.4453 121.158L 4.13735 99.9994C 1.21516 96.1048 0 93.191 0 89.7819L 0 13.903C 0 9.5279 1.94945 5.8785 7.5434 5.39404Z"></path></svg>Try Notion</div></div></div><div style="width: calc(100% - 0px); user-select: none;"></div></div><div class="notion-frame" style="flex-grow: 0; flex-shrink: 1; display: flex; flex-direction: column; background: white; z-index: 1; height: calc(100vh - 45px); max-height: 100%; position: relative; width: 1920px;"><div class="notion-scroller vertical" style="display: flex; flex-direction: column; z-index: 1; flex-grow: 1; position: relative; align-items: center; margin-right: 0px; margin-bottom: 0px; overflow: hidden auto;"><div style="position: absolute; top: 0px; left: 0px;"><div></div></div><div class="whenContentEditable" data-content-editable-root="true" style="caret-color: rgb(55, 53, 47); width: 100%; display: flex; flex-direction: column; position: relative; align-items: center; flex-grow: 1; --whenContentEditable--WebkitUserModify:read-write-plaintext-only;"><span style="height: 1px; width: 1px;"></span><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0; z-index: 2;"></div><div style="width: 100%; display: flex; justify-content: center; z-index: 3; flex-shrink: 0;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div style="max-width: 100%; padding-left: calc(96px + env(safe-area-inset-left)); width: 100%;"><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; pointer-events: none;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px; border-radius: 0.25em; flex-shrink: 0; position: relative; z-index: 1; margin-left: 3px; margin-bottom: 0px; margin-top: 96px; pointer-events: auto;"><div style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px;"><div style="height: 78px; width: 78px; font-size: 78px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notion-page-controls" style='display: flex; justify-content: flex-start; flex-wrap: wrap; margin-top: 8px; margin-bottom: 4px; margin-left: -1px; color: rgba(55, 53, 47, 0.5); font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; height: 24px; pointer-events: auto;'></div></div><div style="padding-right: calc(96px + env(safe-area-inset-right));"><div><div class="notion-selectable notion-page-block" data-block-id="897e5538-ff95-4ef3-93ea-b1f8678bb39f" style='color: rgb(55, 53, 47); font-weight: 700; line-height: 1.2; font-size: 40px; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; cursor: text; display: flex; align-items: center;'><div contenteditable="false" data-content-editable-leaf="true" placeholder="Untitled" spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">4. Data-centric Evaluation of ML Models</div></div><div style="margin-left: 4px;"></div></div></div></div></div><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div contenteditable="false" data-content-editable-void="true" style="padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right)); max-width: 100%; width: 100%;"></div></div></div></div><main style="display: flex; width: 100%; justify-content: center; padding-top: 5px;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div class="notion-page-content" style="flex-shrink: 0; flex-grow: 1; max-width: 100%; display: flex; align-items: flex-start; flex-direction: column; font-size: 16px; line-height: 1.5; width: 100%; z-index: 4; padding-bottom: 30vh; padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right));"><div class="notion-selectable notion-text-block" data-block-id="7cfe8301-63ca-49b5-9c85-17b8465c30c3" style="width: 100%; max-width: 1728px; margin-top: 2px; margin-bottom: 0px;"><div style="color: inherit; fill: inherit;"><div style="display: flex;"><div contenteditable="false" data-content-editable-leaf="true" placeholder=" " spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">Okay, maybe we can get started. Um, so today's lecture is going to cover what I call data Centric evaluation of machine learning models and really what we're focused on When we say evaluation is actually how do you improve models by improving their data. But of course, improving models really requires being able to evaluate them, which is why we titled it this way. And so this is probably the machine learning pipeline that you've been taught in most of your classes and you know you collect data. and Define the machine learning task: Explore the data and figure out what kind of variables there are and pre-process them so that it's suitable for machine learning. And then you train a model and that's probably what you did in previous classes. Um, so so far what we've been teaching you in this class is investigating the shortcomings of the data set. and often in a real application you would investigate the shortcomings of both the model and the data set in order to be able to improve the model and then the rest of the typical machine learning pipeline. Then looks something like this, fixing up the data set to address any kind of issues you found and then improving the model and deploying the model and monitoring future data for new kinds of issues that might have emerged. And this lecture is really going to focus on this critical step of investigating the shortcomings of the model and data set, as well as some of steps six and seven to improve things when you find problems. And as I mentioned, the main topic that we're going to focus on is evaluation of these models which is a prerequisite for improving them. And then how do we handle poor model performance when we do detect there's a problem in particular a problem on a specific subpopulation in the data maybe that has low prediction accuracy and then how do we measure the in influence of individual data points on the model in order to attribute back where the model is doing well or not well based on the training data that it was that produced that model. And so let's start first with just the recap of multi-class classification and this whole lecture will just focus on classification tasks. but all these ideas like many of the other ideas in this class but will be applicable to most machine learning tasks as long as it's supervised learning and so just to recap, Hopefully everyone is familiar with multi-class classification. Uh. given that we've had a couple lectures on this, but we have a data set with n examples uh and in each example we measure some features X and a label Y and we assume these are drawn from some underlying distribution over the features and labels and our goal is to use this data set to train a model that can classify a new example with new feature values X that we might have never seen before and this model will produce typically a vector of predicted class probabilities where each entry is supposed to approximate the true probability that the label would be given this class K Given an example with features X and so critical to evaluating all. this is the specification of a loss function that scores each prediction from the model and and machine learning. There's a little bit of abuse of this term in terms of loss function being used to refer to the thing that you're training your model with like the objective function that you're doing optimization over to select model parameters, as well as the thing that you're using to evaluate predictions of your model on future test data or validation data. And here we're mainly referring to loss function in terms of the evaluation on test data. So this would be analogous to the accuracy of the model saying classification. Whereas you wouldn't you know, train a model directly to optimize accuracy because it's not differentiable. Um, so yeah, we're always looking for a model that minimizes this loss and expected value over an example drawn from the same distribution, but a new example critically, and hopefully that's all familiar. There's if it's not happy to take any questions, but this process has some key assumptions. One is that as I mentioned previously, that the deployment data will stem from the same distribution as the training data. It's also assuming that the training data are independent and all from the same distribution, and that each example belongs to exactly one class. and we saw in past lectures that these assumptions might not always be true in the real world. So as I mentioned, uh, the loss function is this critical object that is used to evaluate the model predictions for a new example versus the label that was given for that example. And typically the loss will either be a function of the predicted class deemed most likely by the model for this example and so examples of that would be like accuracy, balanced accuracy which is essentially Computing the accuracy for each class and then averaging them which is a better way to have a loss that is not so sensitive to some classes being less common than others, precision and recall which are used in sort of medical applications, information retrieval applications where you care about false positives differently than false negatives and the idea is all of these losses are really used to evaluate hard decisions that you're making based on your model because the model is just predicting a single class, it's just saying I Think it's this class and you're essentially taking that decision as as the item that you are evaluating. A second type of loss function operates on the predictive probabilities from the model for each class. And examples of these kind of losses in classification would include the log loss, the Auroc calibration error, and so these kind of losses actually measure You know the probabilistic beliefs of your model and how often those probabilities actually align with the natural distribution of probabilities in your validation or test data. And so one more thing is just those are good. These are better loss functions to use if you're going to make decisions sort of that have asymmetric rewards as a result based on the probabilities from your model. And so when we're talking about reporting model performance obviously, uh, it's not ideal to report just a single number. To summarize, you know how good a model is over some diverse population, but this is what everybody does and it's one of those easy to criticize, but not easy to have much better solutions for. Beyond Showing a bunch of numbers and so the typical score that people use is just the average of the loss over many examples that were held out during training which we call the validation set, but some Alternatives you could consider is averaging a loss for every example in each given labeled class separately. the per class accuracy this is called which would give you a finer grain breakdown of you know how the model is performing on different relevant subgroups of the data or you can actually report the complete confusion Matrix of your model saying how often does it tend to predict this class when the true label was this other class which gives you even more information about the model. but again, it's not as easy to just look at a confusion Matrix which was K by K and say this model is better than this other model versus and single number Everyone Likes that because it allows them to eat more easily compared to models and so uh, it's really really important to think about how you're going to evaluate models and I want to impress that point. People like to jump in to the machine learning aspect and to improving the model right away. You know, typically just using the standard loss, but really how you evaluate these models has a huge impact in real applications and really at the end of the day that determines what model you will end up with and how you what kind of improvements you made only contribute. You know like 30 or so probably 70 of the result you get in the end is how you evaluated your models and that determines everything. And really so you should be thinking about that as much as you think about the fun questions and machine learning like what models to apply and how to improve them and what kind of tricks you can come up with. Yeah, so let's consider for example, uh, binary classification task of predicting fraud or not fraud in some credit card transactions. Can anyone tell me why? Maybe it's not a good idea to just use overall accuracy as our evaluation metric. Any other problems we can think about? Yep, Yep. exactly. And maybe we as the machine learning person don't really know that how much of an issue it is. and so maybe it's better for us to use one of those loss functions that evaluates the probabilities from the model and we can give those probabilities to somebody who actually has some sense of what the costs are in terms of false positives and false negatives. And they can choose a cut off based on those probabilities. and they can even adjust the cutoff over time. Uh so yeah, this is name is showing exactly the point that you just made. And so I'm going to get into some common uh issues that arise when uh, evaluating models. One of like the most nasty ones is this failing to use truly held out data or also known as data leakage. And so this is like you know, taught in machine learning 101. all always that you always need to split the data, never touch the test data except for evaluating your model. But what happens in the real world is you have some complex data pipelines, the data is being pulled in from all these systems, and it is often just the case that you happen to have duplicate values in your test data. You happen to accidentally have a feature that was perfectly correlated with the label that actually won't be available in the future, but for some reason it made it into your training data. And so yeah, this is just a really, really nasty problem and hard to diagnose. Besides, when you see much better than expected accuracy than you would imagine is possible and there's a big study out of Princeton showing you know how people are using machine learning and different Sciences Sciences and just showing you know how poor the practice of machine learning has really been in a lot of these Sciences where they're trying to say you know now we can predict this based on this So therefore, we have uncovered some interesting scientific phenomenon, but actually a lot of the accuracy is just due to data leakage. This is sort of the new p-value from statistics which was a big problem for a lot of Sciences before and also reporting just the average loss really underrepresents severe failure cases. For you know, if you have a really rare subpopulation that's only one percent of your data, but you're just measuring an average loss and say your your loss function is just like accuracy or something bounded, then it doesn't really reflect very well that there's this one percent of your data that's consistently say always getting the wrong prediction and also uh, as I mentioned last time, the validation data might not be representative of the deployment setting due to selection bias and so the best thing you can do is just try and always make sure your validation data is as representative as possible to the deployment setting When you're not able to get a whole data set that's super representative. that way you can at least see your model, training on one distribution and testing on another and see what kind of uh, hit it's taking. And then as we saw in the first and second lecture, obviously some labels might be incorrect because if we have data labeled by humans, they make annotation errors and so this is just getting back to that point of data leakage like in many people when they first get on the field. get very excited when they see 99 accuracy. but as you spend more time in the fields, you get more worried the more you see when you start to see 99 accuracy. Um, so now we're going to go to the subject of underperforming subpopulations. but I First wanted to see if anyone has any questions. Yeah, exactly. So the first one is about. There is some information. so about each example. like data point is supposed to be independent of the other data points and independent of the true of the label for the test example especially. And so this sort of leakage can occur. Say if you have a feature that's very correlated with your labels, only in the training data, but not in the real data. So that's an example of selection bias. Yeah, so that's like a special case of selection buys. But then there's just the straight up issue of having like duplicates and things like that in your data set. Which is not necessarily selection bias. It depends how they arose. Yeah, often some data sets literally just have duplicated data, and at least you should make sure you split the training and validation data such that the duplicates are all in the training data or all in the validation data, but not half half. That was a good question. So yeah, the the selection bias is more. General. It's just really saying somehow when you collected the data, you did something, you did something that made it not reflective of the real world, which is almost always inevitable. It's just a question of how much, how much of a problem have you introduced. Um, yeah. So there was this famous study from MIT actually where they examined all the facial recognition software from all the big cloud providers and you know all the biggest tech companies you know about and they showed like really, really big disparities between uh, like the accuracy on uh, men with light skin versus dark-skinned women and these are you know, products that are that were running live. So presumably they had gone through a lot of testing. and it's kind of interesting that nobody had discovered this. And so yeah, this lecture would hopefully teach you how to discover this sort of thing so that if you launch a product like this many years from now, you wouldn't have similar issues. So here we're going to talk about a slice of the data which is a subset of the data set that shares a common characteristic and this is in many different fields referred to as different things like a cohort subpopulation subgroup I'll try and use those terms interchangeably, but some examples would be data captured by One sensor versus another kind of sensor that you know makes the data from this one sensor all have a common characteristic in your data set. Or maybe you're getting data from one location versus another location versus another. and so all the data from each location sharing is sharing some kind of common characteristic. Can people think of other examples for like human centered data that might be ways to Define slices that are meaningful? Yeah, yeah. I should say when we're thinking about slices, these are typically things where we don't really want our models performance to vary too much across them because we kind of expect them not to play a big role in the prediction problem. Yes, yeah, yeah. So one way to think about this is like anything that on a job application you shouldn't base a decision on. You probably also don't want your model's predictions to uh, you know, really base be based on. And so another question I have is given. I Said We don't really want our model predictions to depend on what slice a data point belonged to. Do you think we can just delete all the slice information from our data set and from the feature values and then say we're good because this information is no longer in my data set? Yep, exactly. Yep. So a lot of the other features that you leave over in the data set might be correlated with the slice information and then you're still your model's still going to depend on information from the slides, just not explicitly And so it's actually better to keep it. And uh, you know, analyze what's going on with the model over the different slices. Did somebody have a question? Um, so yeah. So uh. let's now talk about improving the performance of our model for one particular slice of the data. And so here we're assuming, say the data just has. You know, like J slices that we know for example. Uh, let's say it's sex in a human data set and you know we can see the information about each person And so we can just analyze what is the loss of the model on each slice. Let's say we're just using the average loss over this the subset of the data that belongs to that slice. And so now we've seen there's a slice that's doing poorly And the question is, how can we boost our model for that slice And so in this example, let's look at a binary classification data set and uh, here the two classes are red versus blue and one way to improve Uh performance for a particular slice is simply to just use a more flexible machine learning model that has higher fitting capacity. And the reason I'm showing this example is because this is Uh, an example of a data set where if you were to fit a linear model, it just doesn't have enough capacity to fully fit the data. and therefore the linear model has to make a trade-off of what examples is it going to give High loss to right? Some of these examples are inevitably going to not be predicted correctly because this data set, the relationship is non-linear and so you'll see that many different linear models actually would work pretty well for or what would work somewhat equally well for this data, right? Because where the decision boundary is, it's sort of just going to be like a vertical decision boundary. And if it's vertical versus if it's horizontal, you'll get much different performance for this slice of data on the left there. and so yeah, uh, there's not really a way to fix that uh, easily. but you can note that you could control that with your linear model by choosing a different linear model and I'll get into that in a second. But the easiest way for this sort of data to avoid having the performance hit for a particular slice is just to use a more flexible model like a neural network type model or a non-linear model with higher capacity that doesn't have to make the sacrifice of you know Harding performance on this one subset of the data while not hurting it on the other subset. It can just say let's try and boost the performance over all the data. Another common way to try and improve model performance for a particular slice is to if this is like minority subgroup of the data set that you know has much fewer examples than we can try and over sample them or up weight their up weight them if we're using a model that can take in Sample weights and basically yeah, just make the model think that these examples are more important because there's more of them or we've explicitly increased their weight in the fit. Yep, sudden group that in general. Yes. So here when I'm talking about improving the model performance for a slice, I'm always assuming we have evaluated it on some validation data. and for now I haven't been considering the case where we are, you know, changing the validation data to so hopefully you'd be able to be able to detect this kind of overfitting because you have the validation data. But what you're saying is of course, totally right that if the minority group is really rare, it's also rare in the validation group probably. And then how do you deal with that is another question. Typically there you would try more to deal with it by reserving more of the minority subgroup for the validation data, but not over sampling it in the validation data or anything like that because you don't want to skew your metrics. Um, yeah. so to see why up waiting one subgroup might help here. we have data from two classes and it's overlapping in the feature space. say these are just two dimensional features and so there's actually no way any model can predict some of these data points. You know correctly. It always has to trade off and do I favor making a prediction for the blue class or for the orange class. It's just fundamentally a trade-off that the model has to decide on. and so by up waiting the minority subgroup, you're just encouraging the model to make the trade-off more in favor of the orange subgroup. Another thing you can do that's probably ideal is to just collect additional data from the subgroup with poor performance. Of course this is expensive and so this is also something you want to see if it might have promise first before you just dive in. And so going back to the lecture yesterday where I taught about extrapolating the performance of a machine learning model. We can try and do that, but now just fitting the model to subsets of the data where we've only down sampled this subgroup relative to the rest of the data to see sort of as the proportion of this subgroup grows in the overall data set. And then if we extrapolate that proportion to even higher relative values, does the performance of the model seem to be improving for that subgroup as well as for the overall data set, how is it getting worse or better? Yep, and there you might often see a tension between the mod, the performance starting to get better for that subgroup as you get more and more samples from it. but the performance for the whole group overall might actually start to go down. if we're in a scenario like this, it's really dependent on the application, Thank you. Another thing you can try and do is measure or engineer extra features that allow the model to perform better for the slice where it's currently not doing well. So obviously, if you can measure extra variables about your data, that's always great if they provide some information that's useful for predicting. and sometimes you might be able to measure certain things that are more useful for one slice than another, so you might not have considered adding them to the data. Until You realize that the model is not doing well on this subset. So an example here is classifying, Say if a customer will purchase a product or not based on customer and product features and so maybe the customer features are based on the history of that customer right in your in your eCommerce website or whatever. And so predictions for new customers or young customers might be worse because you just don't have much history about them so your features for them are not as informative and so one thing you could do is just explicitly engineer extra features to the data set so this is not even you know requiring you to measure extra information. it's just you as a data scientist are taking the data you're given, creating uh features using your own Ingenuity that you think will help improve the performance for this specific subgroup. Like here, you might add a feature that's say popularity of the product among young customers specifically and that could boost you know the accuracy of this classifier for this slice. So previously I was talking about the situation where you have say j subgroups in the data and you were just measuring the model's performance on each one, seeing where it was doing badly and then figuring out uh, how can I try and improve the model for that subgroup but in many data sets we might not even know what the relevant subgroups are right. Like in an image data set like this, you know what? what are the relative relevant subpopulations that we should be thinking about if we wanted to go through the previous process. And so a high level strategy that one can use to discover these sort of under underperforming subpopulations is to sort all the examples in the validation data by their loss value, then look at the examples with the high loss where your model is making bad predictions, and then finally apply clustering to just those examples with the high loss to figure out what are some patterns amongst these examples. Uh, with where which are all receiving, you know bad predictions from the model Currently, so irrespective of discovering underperforming subpopulations, step one of sorting the data by loss value is always always a good idea. Whenever you've trained a machine learning model and you want to start thinking about how you might improve it, it's always good to do that and just see what kind of data points is it making bad predictions on because those might be mislabeled They there might be all kinds of reasons that I'll get into, and this is often referred to as error analysis. But then once we've done that, we want to see what are the common patterns amongst these examples. And so a natural way to do that is through an unsupervised learning technique called clustering, which intuitively just tries to find subgroups. exactly subgroups amongst these examples. And so if we just restrict ourselves to only those examples where the model is already doing poorly, we should ideally be able to find clear subgroups where you know they share a coherent theme and the model is doing perform poorly to identify a factor that's responsible for this poor performance. Um, so here is an example of this: This method is called Domino but it's just a specific instantiation of the idea. I Talked about where you have your image classification data set say on the top left and you have you know, some images in a forest. Some images are in with a blue sky and that would be sort of a match. Maybe you discover that your model is or you're You don't discover this, but your model is actually performing poorly on images that come from a blue sky. And so if you were to sort all the data by their loss and then apply clustering, maybe it would just naturally be the case that a cluster of Blue Sky Images emerges amongst your uh, clustering in the data with high loss. Another thing you can do that's a little more advanced is make the clustering label aware, which is what this method proposes so that you're specifically uh emphasizing that the cluster should also all have the same labels to make even more coherent clusters and classification tasks. And so then yeah, once you've discovered, you know these subpopulations and you've decided they actually are meaningful. You can just go back to the previous slides and think about how you might improve the performance for each one that is particularly worrisome to you Foreign. So now we're going to get even more micro and just focus on one particular data point rather than you know a subgroup and ask, why is the model getting this one prediction wrong and uh, how can we improve things And so again, going back to the first lecture. Maybe the label is just wrong and the model was actually right And so the best thing we should do here is just correct the label in our data set. Um, or going back to the first lecture. Uh, the example might not belong to any of the classes at all, or be fundamentally not a predictable example. like if it's a super blurry or super dark image and there. We might consider just removing this example from the data. or we might even consider adding an extra class like in other or clutter class we saw before to the data set if we see many such examples and we also expect them to recur at test time during deployment. So another common issue is the example is an outlier in the data set. Like, there are no other similar examples in your data. Do you guys have ideas on what we might do for such an example? how might we try and improve our model? For example, here we have a bunch of images of clothing, maybe we're an E-commerce doing some e-commerce machine learning and then we have an image of a three in our training data for some reason. Yeah, exactly. So it's subtle. Yeah, this is not an easy answer. So if you would never see this similar example during deployment, then you can probably just get rid of it. Otherwise, you might actually want to try and collect more data for this. So going back to the first lecture and the Tesla example. they noticed the car was doing badly in tunnels in that data quality data engine that they had and so they purposefully so they weren't even training their model on all of their data because they had too much data. But then they purposefully went through their massive data set and looked specifically for tunnel scenes to increase the number of tunnels in the training data. Um, another thing you can actually do is so often you're not going to be able to collect additional data right? If you're the machine learning person and you don't have the ability to go collect additional data, you have to apply some kind of data Transformations at that point. and so one data transformation you can try is to just make the features look more similar to the model across outliers and non-outliers such that the model is not going to produce such an unreliable prediction for this outlier. And so if you have like numeric value features, you can try normalizing them to a common range via like a quantile normalization is a common one. or you can actually delete a feature entirely which sometimes you'll just have one feature that's causing particular data to be be seen as an outlier by your model. But actually the model would do fine if you just had removed that Feature from the whole data set. Another thing you can do is add synthetic data so the model starts to become less sensitive to the difference. that makes this outlier stand out from other examples. for example, if you had maybe an extreme example is if you had black and white images and color and one or two color images in your data set and those are huge outliers. Now you could turn all your black and white images, you could colorize them or turn all your color images black and white. and then basically that then your those images become less outliers in the new representation of the data set, and then finally you can up weight or duplicate the outlier if it's a really important example and you really need to do something uh Critical with it. Um, then yeah. Duplicating it multiple times and the training data is a very simple strategy and a good way to do this is to actually try and change the feature value slightly each time you're duplicating it so your model is better able to really capture a pattern around this example that it learns to predict more stably. And when you're creating these variants of the feature values, we will have a lecture on how to do this properly via data augmentation. but the key idea is not to add too much variation such that you would expect the label to change you. would you want to add just a little enough variation such that you still expect the same label to apply. Okay, another reason why the model might get a particular prediction wrong is the kind of model you're using is just not ideal for this kind of data and the these examples. and so how can we diagnose this? This is similar to going back to what I just presented when you have an underperforming subgroup and you're trying to see if uh, you can get more data from that subgroup to improve the performance. Here, you're just trying to see if you forced your model to really focus on just this examples and examples like it. would the model even get better or not? And so here you can try up waiting similar examples to this example or duplicating them a bunch of times in the data set, retraining the model, and seeing if now the model has gotten better and if it's still not better, you might really want to try a different kind of model. If you're if you're worried that you know you really want to get this example's prediction correct. And so this is kind of a model-centric approach as well that you might consider where you fit different kinds of models. you do hyper parameter tuning, you do feature engineering, but really yeah, this is obviously a problem with the model when we say the model is not a good fit for such kind of examples. Another reason that can happen is the data might just have a bunch of examples with almost the same feature values as the current example that your model got wrong. But those examples all have a different label and so in that sense, like there's not much, uh, a model can do, it's got to get something wrong. If there's two, you can imagine there's two identical data points in your data set and they have different labels. One of them will inevitably be predicted incorrectly. And so going back to the first lecture from the example from Imagenet. It has this keyboard class and the spacebar class, and the images from both look very similar. and so inevitably some models will just have a non-zero loss on certain images from these two classes because each one is just each image is only given one of these two labels. And as we saw before, one thing we can try and do here is Define the classes more distinctly. to give you know a much clearer we can tell our data annotators to when they're labeling the data, we can give more clear definitions of what this class is versus this other class. or we can merge classes such that the remaining classes are more distinct. and then another thing we can do is measure extra features that actually enrich the data set and make it such that now when you have two examples with different labels, they almost never have identical feature values because we've actually measured some new feature of the data that actually discriminates these two. Any questions on this: So yeah, these are five different strategies for trying to really boost performance of a model just by inspecting it at a very local level on the level of individual prediction. So you'll typically sort all your data by the loss that the model is incurring on it, and then you know, go through this exercise in your head. It's typically better to start with the sub populations first because you know those are big groups of data where improving your model for those can actually lead to meaningful changes. More meaningful changes I should say than improving the model for individual examples, but sometimes the individual examples really tell you a lot and actually can make a big difference, especially if there are certain high-stakes scenarios in your application that really matter. Okay, now we're going to shift gears slightly and talk about the influence of individual data points on a machine learning model. And so one way to think about this topic is imagine you have people selling your data and maybe you're doing medical machine learning and these are hospitals or patients and so their data is actually really valuable and you are going to use that data to train a machine learning model that is used in some application that makes you some money. And then the question that you that might be natural is, how do you give that money back to the people who gave you data? Obviously one way to do it is just give everybody the same amount of money for each example that they give you. but that might not reflect some of these uh, data brokers who have very unique examples that are actually much more valuable to you and maybe higher quality. For example, if they have less label errors in their data, you might consider paying more for that. And so this is really a fundamental question. And it's also a challenging question because you can imagine the value of the data really depends on the other data, right? Like some data might be super useful, but if all the other sellers have the same data as well, then it suddenly starts to become less useful to you. Or maybe you're not going to pay as much for it. And so this is a key question that has been studied a lot in economics. Just in general, when you have a bunch of people providing some kind of input, how do you allocate profits back to the people And what kind of desirable properties do you want in such allocation schemes? And so a really simple form of this that is pretty popular is to just ask: how would my model have changed if I only omitted a single data point from the data set, retrain the model, and measured how much did the model change as a result. So here there's two kinds of of change we might be interested in. One kind is how did the predictions of the model change as a result, and another kind of change we might be interested in is how did the accuracy say on some validation data change as a result, there's also a third kind of change which is like how did the parameters of the model change as a result, But that's less interesting to folks nowadays where most people are using sort of neural network type models with less interpretable parameters. So yeah, mostly focused on some kind of function of the outputs changing as a result of retraining the model on a different version of the data set where you dropped one data point. So here we have a three class classification data set. where say, we've trained a model in this data set and it has 98.5 percent accuracy on our validation data. And then let's say we drop the data point the most right data point here and we retrain this model. and now it gets a 98.3 percent validation accuracy. And so you might then conclude basically that the impact of that data point is something like per point, two percent, right? That's sort of its contribution to your model and that's uh, a natural way to think about things. can anyone think of like why this might not be sort of the end-all be-all of valuing data And why what? What issues some people might have with this form of data valuation? Yeah, um, uh, it predicted it very well because it has everything at the moment, right? Yep, so that's a good one. And the idea of this set of data maybe like that data point alone is not impacting your model hugely. but it with 10 other data points together are collectively having a big impact on your model. And so yeah, that's a key idea that this leave one out influence doesn't really capture you can imagine. In that extreme case, we have 10 data points. They're all duplicated, but they're all actually very, uh, important. Like they cover some rare issue in our data set or some rare class that's not very common. And so if we delete all 10 of them, suddenly, our model is much worse at predicting that class. But deleting any one of them is not so impactful because there's actually nine other data points still in the data set. And so there's this idea of the Shapley value and this is basically from economics. and Game Theory As I mentioned, it's a way to evaluate to provide value or to attribute value. If you have a bunch of co-workers and you're on a project, how do you decide? how much of the profits from this project should you allocate back to each co-worker And it satisfies a number of desirable axioms. And for example one Axiom that's nice is if you add an additional person to your uh, uh, Sat who doesn't contribute anything, you want to make sure that the Uh returns to each of the existing people is the same. And then you also want to consider how each of these co-workers uh is contributing in the context of the other co-workers And that's the same for these data points. So here we want to consider how is each data point contributing in the context of the rest of the data points, but not only of our original data set. Right Here, we just dropped the single data point. so we were only measuring how much is this data point contributing in the context of this particular data set that we happen to have. But that's kind of arbitrary that that we have that data and so actually what? What is a more relevant question is how much would that data point contribute in hypothetic, all sorts of hypothetical data sets. And so one way to measure that is to compute this, leave one out influence of the data point in some subset of the data set that contains that data point, right? Again, we're just removing it from that subset and seeing how the model changes. If we were to just train the model on the remaining subset versus the subset including that data point, and then we can average these values over every single possible subset of the data set And that gives you a pretty broad Uh understanding of how much does this data point contribute in different contexts where it could have been present or absent from a data set. Of course, this is very nasty to compute. so I'll get into that in a second, but just to see. Uh, Sort of. The advantages of this. Imagine you have two identical data points in a data set and actually, if we delete both, then this is going to really harm model accuracy. but omitting just one of them won't Does anyone have an idea? Sort of what the leave one out influence for these two data points would look like versus this Data Shapley influence? Exactly. Yep, so leave one Out influence will be small. Um, just because I said omitting only one of those data points does not lead to a big degradation in model performance. but the Data Shapley. So for each of these possible subsets, basically half of them will not include the other data point. And so for all of those, when we remove the remaining data point, you're going to see a big drop in performance. So 50 of the time. So then when you're averaging over all possible subsets, you are just going to get a pretty big uh, you know you know loss in accuracy and you can see that this is a natural answer to this sort of ill post question of how much is this example contributing to my model because uh here yeah you're really considering a bunch of contexts and asking how much is this example contributing in each context. So here I'm going to talk about Computing these influence functions and so as I mentioned, uh Computing either of them is not very feasible right? even the leave went out influence. To compute it for a whole data set, you would have to delete each data point, retrain a model and evaluate it. and if you have a huge data set that is not going to be very fun and then the data Shapley you can basically forget about because you are trying to average something over all possible subsets of your data set and each of those involves a retraining of your model. So yeah, that is not probably going to happen in any meaningful application. And so here I'll just go to the board. So we can imagine we have a big data set here and this is our full Uh, data set, training data set and then we have a validation. Um, this is our validation data and so what we can do is we can just draw subsamples of this data set and that gives us a sub-sample data set and we can do it again and say we do it four times. So now we've got four sub samples of our data set and what we can do is we can train a model on each one and get some kind of accuracy on the validation data. So say training a model leads to and say training a model on this subset leads to accuracy 80 percent. model on this subset leads to accuracy 85 and say on this one, it's Uh Seven, 70 Accurate. So now we have Uh four model four subsets of the data set and we have the accuracy for each one. And now let's imagine we are trying to quantify the influence of this data point right here. How would we? How might you do that given this information in a pretty straightforward way. Okay, anybody subsets that have at this point the one who don't have this point Exactly. Yep. So we can just look at uh where, Uh, say it's say it's present in this one and say it's present in this one too. but it's not not in these two. Then basically we would just take the average of these two accuracies and then we can subtract off the Uh average of the other two accuracies and that can be our approximate measure of this data Point's influence. And so how many? How many times are we going to have to retrain a model to compute the influence of all the data points in our data set this way four times. Yep, And so you can see. it really depends on how many subsets of the data are we going to draw. And if we want to compute the influence of every data point and we want to retrain the model as few times as possible, does anybody know what? How should we choose our subsets to get to that? Yep, Each time. Yeah, Exactly. Or partitioning it right? Yeah, So we want to partition it so that it's not uh, so that we have non-overlapping subsets of the data. and then we're at least guaranteed this data point will be in one of those subsets so that we can even compute this equation. but it'll be very brittle, right? Evaluation of the influence Because that data point is. you're basically going to be measuring that the effect of the whole partition because that whole partition is either present in the subset or not present. So this is not actually a good way to do it. I'm just asking. Conceptually, this is the minimum amount of retraining you would need of your model to be able to compute influence for every data point Foreign, and just to be clear, the accuracy there could be replaced with you know, any evaluation metric of interest. and then if we were interested in, how are my models predictions changing rather than the loss changing as a result of the Uh. As a result of including this data point or not in the data set, you can just measure the predictions at every single data point in your validation data set and measure how much are the predictions changing for each of these subsets. but it's still the same strategy. And then I Also want to just briefly mention that for some kinds of models, you can actually compute these things in closed form rather than this Monte Carlo approximation. And so in statistics, Uh, you know linear regression is the most famous model and this is corresponding to a regression task rather than a classification task with a mean squared error loss function and uh, In in that world the leave went out influence is just called Cook's distance and it's like a very famous diagnostic that people use to check. You know, uh, is there data points you should be worried about in your data set when you're fitting a linear regression model? Uh, you know in science papers they always do this to check that the data and then they consider removing some of these high influence points if they are seem suspicious in some way and then a recent breakthrough that was made actually was that this can be done also pretty efficiently for classification. So this doesn't work for linear classification with like a logistic regression model, there is not really a closed form analog of Cook's distance and that's because the only reason you can compute the influence via Cook's distance in closed form is because the parameters of the linear regression model are a closed form function of the data and therefore you know mathematically what each data set will translate into the parameters of the model as well as predictions from the model are closed form and therefore you can also recompute. If I were to delete something just through that formula, how would the parameters have changed? But for logistic regression, there is no analogous closed form solution for the parameters of a model. Um, But for a k nearest neighbor classifier, there was actually an interesting, very complex algorithm recently invented Uh that can do this in order n log n time and so a strategy that one can use now in practice. besides that, Monte Carlo Strategy I mentioned is if you have say, image data or some complicated data set, you can take a pre-trained neural network and get embeddings from the data and then fit a k nearest neighbor classifier on top of those embeddings. So the problem with fitting a k nearest neighbor classifier on the original data is it's really hard to measure the similarity say of two images in a semantically meaningful way, but in the embedding space. Typically these pre-trained models are able to embed images in a feature space. That's where the vectors have more close similarity based on semantics. and so then if you fit a k nearest neighbors model in that space, you can actually then go and compute the influence for each data point and actually attribute You know which images are sort of most influential in my data set. Yep, so you should. If you're able to approximate influence, you should always try and reveal, uh, inspect. You know the most influential samples in your data and those are the data points that have the greatest impact on the model. Especially if you compute the influence with respect to the model's predictions. That's really telling you exactly. If I had not had this data point, how would my model's output have changed? Like literally, how's the function my model has learned changing as a result of this data point? That that kind of information is not always as strong when you're measuring the loss version of influence like you're measuring how did the accuracy of my model change. Because even if the prediction, even if the function changed a lot that the model is outputting, it can sometimes still have a similar accuracy. And so correcting a mislabeled data point with high influence can boost the model accuracy. Say much more than correcting a mislabeled data point with low influence. Where that data point you know actually isn't even contributing much to the resulting models function that is learning. So even if I change its label, it might not make a big difference. But the issue with just using influence of course is that if we sort only by influence, we might not find, say the mislabel data or the data with other issues because sometimes you're just finding really good data when you sort by influence. And so you can try sorting by influence as well as other strategies like confident learning to sort of find the intersection of both High influence and data points with other kinds of issues as a way to improve models. that's all I have. So any questions for me and today's lab will focus on basically your creativity going through any of the strategies we presented so far to just try and improve a model on a given data set where the training data is more noisy than the test data and it's not going to be like a simple fix like you had in lab one. You actually have to use some creativity to try and improve the training data in some way and the goal is to just improve the model only by changing the training data, not by you know, changing the model itself. So we just give you some modeling code and you're just supposed to rerun that code every time you change the data. foreign. If there's no questions, thank you for coming! [Applause] [music]</div></div></div></div></div></div><div contenteditable="false" data-content-editable-void="true" style="width: 0px;"><div style="display: none; flex-shrink: 0; pointer-events: none; width: 0px; position: absolute; right: 192px; opacity: 0;"><div style="display: flex; flex-direction: column; padding: 5px 16px; width: 340px; flex-shrink: 0; height: 100%; position: relative; pointer-events: none; z-index: 1;"><div style="position: absolute; pointer-events: none; width: 100%; height: 100%; top: -5px; background: linear-gradient(white 0px, rgba(255, 255, 255, 0) 15px);"></div></div></div></div></main><span style="height: 1px; width: 1px;"></span></div><div class="notion-presence-container" style="position: absolute; top: 0px; left: 0px; z-index: 89;"><div></div></div></div></div></div><div class="notion-peek-renderer" style="position: fixed; top: 0px; right: 0px; bottom: 0px; width: 960px; z-index: 109; transform: translateX(960px) translateZ(0px);"></div></div></div></div></div><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><script src="assets/js/1172e9111a5fb396bcb8a05870b5eabf8abf221c.js" type="text/javascript"></script><div style="width: env(safe-area-inset-bottom);"></div></body></html>