<html class="notion-html"><head lang="en"><meta charset="utf-8"/><meta content="width=device-width,height=device-height,initial-scale=1,maximum-scale=1,user-scalable=no,viewport-fit=cover" name="viewport"/><title>Data-Centric AI vs. Model-Centric AI</title><meta content="en_US" property="og:locale"/><link href="data:image/png;base64,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" rel="shortcut icon" type="image/x-icon"/><link href="/images/logo-ios.png" rel="apple-touch-icon"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="telephone=no" name="format-detection"/><meta content="no" name="msapplication-tap-highlight"/><link href="assets/css/e218a1aef6df86309cb95b61e446888efa3b0379.css" media="print" rel="stylesheet"/><link href="assets/css/e7df85b6a5b33bc52629df6d3bd37d197c7a873d.css" rel="stylesheet"/><meta content="Edouard d'Archimbaud" name="title"/><meta content="Edouard d'Archimbaud official website" name="description"/><link href="assets/css/e1d809d762eeca23edf0cb31bb17bf3c703085f5.css" rel="stylesheet"/><style type="text/css"></style></head><body class="notion-body"><style>body{background:#fff}body.dark{background:#191919}@keyframes startup-shimmer-animation{0%{transform:translateX(-100%) translateZ(0)}100%{transform:translateX(100%) translateZ(0)}}@keyframes startup-shimmer-fade-in{0%{opacity:0}100%{opacity:1}}@keyframes startup-spinner-rotate{0%{transform:rotate(0) translateZ(0)}100%{transform:rotate(360deg) translateZ(0)}}#initial-loading-spinner{position:fixed;height:100vh;width:100vw;z-index:-1;display:none;align-items:center;justify-content:center;opacity:.5}#initial-loading-spinner svg{height:24px;width:24px;animation:startup-spinner-rotate 1s linear infinite;transform-origin:center center;pointer-events:none}#skeleton{background:#fff;position:fixed;height:100vh;width:100vw;z-index:-1;display:none;overflow:hidden}#initial-loading-spinner.show,#skeleton.show{display:flex}body.dark #skeleton{background:#191919}.notion-front-page #skeleton,.notion-mobile #skeleton{display:none}#skeleton-sidebar{background-color:#fbfbfa;box-shadow:inset -1px 0 0 0 rgba(0,0,0,.025);display:flex;width:240px;flex-direction:column;padding:12px 14px;overflow:hidden}body.dark #skeleton-sidebar{background-color:#202020;box-shadow:inset -1px 0 0 0 rgba(255,255,255,.05)}#skeleton.isElectron #skeleton-sidebar{padding-top:46px}#skeleton .row{display:flex;margin-bottom:8px;align-items:center}#skeleton .row.fadein{animation:1s ease-in 0s 1 normal both running startup-shimmer-fade-in}#skeleton .chevron{width:12px;height:12px;display:block;margin-right:4px;fill:rgba(227,226,224,.5)}body.dark #skeleton .chevron{fill:#2f2f2f}.startup-shimmer{background:rgba(227,226,224,.5);overflow:hidden;position:relative}body.dark .startup-shimmer{background:#2f2f2f}.startup-shimmer::before{content:"";position:absolute;height:100%;width:100%;z-index:1;animation:1s linear infinite startup-shimmer-animation;background:linear-gradient(90deg,transparent 0,rgba(255,255,255,.4) 50%,transparent 100%)}body.dark .startup-shimmer::before{background:linear-gradient(90deg,transparent 0,rgba(86,86,86,.4) 50%,transparent 100%)}#skeleton .icon{width:20px;height:20px;border-radius:4px}#skeleton .text{height:10px;border-radius:10px}#skeleton .draggable{-webkit-app-region:drag;position:absolute;top:0;left:0;width:100%;height:36px;display:none}#skeleton.isElectron .draggable{display:block}</style><style id="scroll-properties"></style><div id="notion-app"><div class="notion-app-inner notion-light-theme" style='color: rgb(55, 53, 47); fill: currentcolor; line-height: 1.5; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; -webkit-font-smoothing: auto; background-color: white;'><div style="height: 100%;"><div class="notion-cursor-listener" style="width: 100vw; height: 100%; position: relative; display: flex; flex: 1 1 0%; background: white; cursor: text;"><div class="" style="display: flex; flex-direction: column; width: 100%; overflow: hidden;"><div style="max-width: 100vw; z-index: 100; background: white; user-select: none;"><div class="notion-topbar" style="width: 100%; max-width: 100vw; height: 45px; opacity: 1; transition: opacity 700ms ease 0s, color 700ms ease 0s; position: relative;"><div style="display: flex; justify-content: space-between; align-items: center; overflow: hidden; height: 45px; padding-left: 12px; padding-right: 10px;"><div class="notranslate" style="display: flex; align-items: center; line-height: 1.2; font-size: 14px; height: 100%; flex-grow: 0; margin-right: 8px; min-width: 0px;"><div class="notion-selectable notion-page-block" data-block-id="d8397a78-1321-456c-9c10-1feea7a42b60" style="display: flex; align-items: center; min-width: 0px;"><a href="edouard-d-archimbaud.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üë®‚Äçüíª" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üë®‚Äçüíª</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Edouard d‚ÄôArchimbaud</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div class="notion-selectable notion-page-block" data-block-id="bb024d08-b3b1-4dbd-bafc-0fa8d6a316c2" style="display: flex; align-items: center; min-width: 0px;"><a href="data-centric-ai.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéØ" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéØ</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Data-centric AI</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 6px; padding-right: 6px; color: rgb(55, 53, 47);" tabindex="0"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 240px;">Data-Centric AI vs. Model-Centric AI</div></div></div><div style="flex-grow: 1; flex-shrink: 1;"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="searchNew" style="width: 14px; height: 14px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg>Search</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0">Duplicate</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: flex; align-items: center; justify-content: center; width: 32px; height: 28px; border-radius: 3px;" tabindex="0"><svg class="dots" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden;" viewBox="0 0 13 3"><g><path d="M3,1.5A1.5,1.5,0,1,1,1.5,0,1.5,1.5,0,0,1,3,1.5Z"></path><path d="M8,1.5A1.5,1.5,0,1,1,6.5,0,1.5,1.5,0,0,1,8,1.5Z"></path><path d="M13,1.5A1.5,1.5,0,1,1,11.5,0,1.5,1.5,0,0,1,13,1.5Z"></path></g></svg></div><div style="flex: 0 0 auto; width: 1px; height: 16px; margin-left: 8px; margin-right: 8px; background: rgba(55, 53, 47, 0.16);"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="notionLogo" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 120 126"><path d="M 20.6927 21.9315C 24.5836 25.0924 26.0432 24.8512 33.3492 24.3638L 102.228 20.2279C 103.689 20.2279 102.474 18.7705 101.987 18.5283L 90.5477 10.2586C 88.3558 8.55699 85.4356 6.60818 79.8387 7.09563L 13.1433 11.9602C 10.711 12.2014 10.2251 13.4175 11.1939 14.3924L 20.6927 21.9315ZM 24.8281 37.9835L 24.8281 110.456C 24.8281 114.351 26.7745 115.808 31.1553 115.567L 106.853 111.187C 111.236 110.946 111.724 108.267 111.724 105.103L 111.724 33.1169C 111.724 29.958 110.509 28.2544 107.826 28.4976L 28.721 33.1169C 25.8018 33.3622 24.8281 34.8225 24.8281 37.9835ZM 99.5567 41.8711C 100.042 44.0622 99.5567 46.2512 97.3618 46.4974L 93.7143 47.2241L 93.7143 100.728C 90.5477 102.43 87.6275 103.403 85.1942 103.403C 81.2983 103.403 80.3226 102.186 77.4044 98.54L 53.5471 61.087L 53.5471 97.3239L 61.0964 99.0275C 61.0964 99.0275 61.0964 103.403 55.0057 103.403L 38.2148 104.377C 37.727 103.403 38.2148 100.973 39.9179 100.486L 44.2996 99.2717L 44.2996 51.36L 38.2158 50.8725C 37.728 48.6815 38.9431 45.5225 42.3532 45.2773L 60.3661 44.0631L 85.1942 82.0036L 85.1942 48.4402L 78.864 47.7136C 78.3781 45.0351 80.3226 43.0902 82.7569 42.849L 99.5567 41.8711ZM 7.5434 5.39404L 76.9175 0.285276C 85.4366 -0.445402 87.6285 0.0440428 92.983 3.93368L 115.128 19.4982C 118.782 22.1747 120 22.9034 120 25.8211L 120 111.187C 120 116.537 118.051 119.701 111.237 120.185L 30.6734 125.05C 25.5584 125.294 23.124 124.565 20.4453 121.158L 4.13735 99.9994C 1.21516 96.1048 0 93.191 0 89.7819L 0 13.903C 0 9.5279 1.94945 5.8785 7.5434 5.39404Z"></path></svg>Try Notion</div></div></div><div style="width: calc(100% - 0px); user-select: none;"></div></div><div class="notion-frame" style="flex-grow: 0; flex-shrink: 1; display: flex; flex-direction: column; background: white; z-index: 1; height: calc(100vh - 45px); max-height: 100%; position: relative; width: 1920px;"><div class="notion-scroller vertical" style="display: flex; flex-direction: column; z-index: 1; flex-grow: 1; position: relative; align-items: center; margin-right: 0px; margin-bottom: 0px; overflow: hidden auto;"><div style="position: absolute; top: 0px; left: 0px;"><div></div></div><div class="whenContentEditable" data-content-editable-root="true" style="caret-color: rgb(55, 53, 47); width: 100%; display: flex; flex-direction: column; position: relative; align-items: center; flex-grow: 1; --whenContentEditable--WebkitUserModify:read-write-plaintext-only;"><span style="height: 1px; width: 1px;"></span><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0; z-index: 2;"></div><div style="width: 100%; display: flex; justify-content: center; z-index: 3; flex-shrink: 0;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div style="max-width: 100%; padding-left: calc(96px + env(safe-area-inset-left)); width: 100%;"><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; pointer-events: none;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px; border-radius: 0.25em; flex-shrink: 0; position: relative; z-index: 1; margin-left: 3px; margin-bottom: 0px; margin-top: 96px; pointer-events: auto;"><div style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px;"><div style="height: 78px; width: 78px; font-size: 78px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notion-page-controls" style='display: flex; justify-content: flex-start; flex-wrap: wrap; margin-top: 8px; margin-bottom: 4px; margin-left: -1px; color: rgba(55, 53, 47, 0.5); font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; height: 24px; pointer-events: auto;'></div></div><div style="padding-right: calc(96px + env(safe-area-inset-right));"><div><div class="notion-selectable notion-page-block" data-block-id="c5c10675-97cb-45cf-918b-5e64010116a1" style='color: rgb(55, 53, 47); font-weight: 700; line-height: 1.2; font-size: 40px; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; cursor: text; display: flex; align-items: center;'><div contenteditable="false" data-content-editable-leaf="true" placeholder="Untitled" spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">Data-Centric AI vs. Model-Centric AI</div></div><div style="margin-left: 4px;"></div></div></div></div></div><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div contenteditable="false" data-content-editable-void="true" style="padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right)); max-width: 100%; width: 100%;"></div></div></div></div><main style="display: flex; width: 100%; justify-content: center; padding-top: 5px;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div class="notion-page-content" style="flex-shrink: 0; flex-grow: 1; max-width: 100%; display: flex; align-items: flex-start; flex-direction: column; font-size: 16px; line-height: 1.5; width: 100%; z-index: 4; padding-bottom: 30vh; padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right));"><div class="notion-selectable notion-text-block" data-block-id="5c1be0d9-9aa6-4b3d-9f82-bbc311840d2c" style="width: 100%; max-width: 1728px; margin-top: 2px; margin-bottom: 0px;"><div style="color: inherit; fill: inherit;"><div style="display: flex;"><div contenteditable="false" data-content-editable-leaf="true" placeholder=" " spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">All right, let's get started. Thanks for everybody who showed up. Uh, we've got three of the course instructors here myself Anisha in the back, uh and Jonas and we all uh, and she's finishing up his final year. but we all we all were here. We all were students here. We all did our Phds here and it's good to be back. Um, if you don't know any machine learning at all, you never take an intro class, then this will be probably a little too confusing. but as long as you've seen a little Ml then you'll have a good time. And ideally you know a little bit of Python and uh, you know it like Numpy and Pandas are. So with that, let's just get started. So our goal in this course is to learn about Data Centric AI It's just like a quick show of hands prior to this course. How many people I've heard of data Centric AI Okay, all right, well get excited because you're gonna learn some great things. It'll be really good. All right. So let's just jump in. Um, so this is a picture of a of a self-driving car that has had an accident and you notice that the article focuses on when algorithms mess up, right? Um, and this is the common case, right? in machine learning. We tend to focus on the model and so it's like there's a crash. The algorithm must have done something wrong. Um, and so I'd like us to sort of think of a different uh, way of thinking about this. This is a paper many folks are familiar with which basically shows one thing and that a neural network or a machine learning classifier can learn total. Randomness It's like if you give it complete garbage data just like completely random labels, it can learn to map like images to completely arbitrary labels or text to completely arbitrary labels. So basically if you give it really bad data, it will just produce exactly what it learns even if the data is completely wrong. And so I'd like to sort of rethink this title. This article is when algorithms are trained with erroneous data. Things like car crashes can happen and that's the way we'll sort of focus and think about this course. So traditional machine learning is very model Centric right? Like when you take an ML Class So you first learn machine learning, you're in school and then you get a data set and usually the data set is pretty good. Like if anyone's seen like the Cat Dogs data set, you know it's in images of cats and they're all cats. and they're labeled cat. And then there's images of dog. and they're all dogs. And they're labeled dog. And there's no. there's usually no. like cows thrown in there, right? And there's usually not. like a bunch of dogs that are labeled cat. It's like pretty well curated. and then your goal is to you know, produce a good model, right? You want to train a model that takes in a new image that's either a cat or a dog and it predicts. is it cat or is it dog And that's sort of how we learn machine learning. usually. Um, and this is something that's standard if anyone here has taken like 6036 which has been renamed to 6 30, 390. Uh, the intro to ML Class Here and you'll learn stuff like you know different types of models and if you're familiar with neural networks, neural architectures. or you'll learn about tuning hyper parameters of a model. Or you'll learn about modifying the loss function using different loss functions and regularizations that you don't overfit. And these are common things you learn in Model Centric AI So let's just juxtapose that with the real world once you're out of the classroom. So in the classroom, usually the data set's fixed and it's pretty good. But then you go to the real world and actually the data set is not fixed, right? You have user data or customer data or real world data and you can get more or less. You can change the data and the data has all sorts of weird things in it. And so what tends to happen is that the company or the user, whoever's sort of using this model that you're trying to train it doesn't really care as much about the cool Ml fancy tricks as it does about like does it actually work in the real world And if you have really good machine learning models that work on highly curated data, but then the real world data is actually really messy, then it makes sense to actually focus on fishing, fixing the issues in the data and a lot of people have been doing this. But what are systematic ways to do this and that's what we'll focus on this course. What may surprise some of you that you may not know is that 10 of the most commonly cited and most used test sets in the field of machine learning all have wrong labels and that may be a surprise for some of you. So we'll just take a quick look at this website which I think will be pretty fun. This is <a class="notion-link-token notion-enable-hover" data-token-index="1" href="http://labelleairs.com/" rel="noopener noreferrer" style="cursor:pointer;color:inherit;word-wrap:break-word;text-decoration:inherit" target="_blank"><span class="link-annotation-5c1be0d9-9aa6-4b3d-9f82-bbc311840d2c--119618952" style="border-bottom:0.05em solid;border-color:rgba(55,53,47,0.4);opacity:0.7">labelleairs.com</span></a> and can we see? Okay, so this is a site that you can check out on your own. but so this is Imagenet which is a common image data set and these are these are in the test set or in this case a validation set which is what was released. but this is the data that is supposed to be the the most accurate data right? This is like your real world test data and so this data should be some of the most highly curated, most accurate data. And what you'll find is that for example, this you know scorpion is labeled tick and you'll find all sorts of stuff we can look at another date. this is by Google this is a drawing hand-drawn data set. This Is labeled a T-shirt and this is labeled a diving board and this is labeled a scorpion and it just keeps going and going. and there's some fun ones like this is a this is totally a cake but it's labeled a rake like it just didn't quite get the r to go all the way when they were uh, pretending they're they're writing it out. but anyway you can. You can check these out on your own. This is for any type of data. so like Text data or audio data and you'll have fun and I Think the good idea to to think about here is that this is you know, a data set that's released by Google. It's a benchmark data set and it's very difficult when you have millions of examples to know like what's the bad data and ideally if you didn't have that bad data you could train better models and so we need to learn. How can we find these errors and find these issues automatically? Um, so going back to the slides. so a seasoned data scientists the way they'll approach this problem is that it's more worthwhile to invest in exploring and fixing the data than trying to Tinker with the models to avoid this carbon garbage and garbage out issue. And the issue is that like if you have millions of data right or you have like a data set that's like 100 million like how do you do that without it being highly time consuming? All right. So we're here in a Data Centric AI course. Uh, We've called this introduction to Data Centric AI because we want it to be accessible and so we're going to cover. Just like the very intro, what is Data Centric AI How does it differ from Model Centric Ai And then we'll dive in a little deeper. So Data Centric AI often takes one of two forms. So one form is that you have Ai algorithms that understand something about data and then they use this new information that they've understood to help a model train better. So an example of this is curriculum learning and this is more of sort of something you'd encounter in grad school. So you may not have heard of it. But what curriculum It's Uh, you can check out the paper by Yeshua Benjo But the idea is you use a some algorithm that looks at data and it identifies which data is probably easy to learn. And so the corollary here is imagine you're a student and you're in the classroom All right. Should the teacher? What should they teach you like? should they teach you really, really hard examples as the very first examples. Like if you're learning addition, should they start out with like 10 051 plus 1042? Or would they start out with like one plus two right now? We know this because like, we've all learned addition. But when a machine learning model starts, it's starting from scratch so it doesn't know right from the beginning what is the sort of easiest example. And so there are data Centric AI approaches that actually estimate what is the easiest example. and then when you train an Ml model, you start with that example and then you give it slightly harder ones and slightly harder. And so it's like curriculum learning because it's like a student's curriculum. and so that's one way that you can sort of use new information. You're still training on all the same data, but you're just reordering it and using this additional information, you don't actually change the data set. Um, another sort of common form data Centric AI is that you actually modify the data set to directly improve the performance of a model. So an example of this might be something like confident learning and this is less known than curriculum learning is something I worked on. Um, and this idea here is that you want to find what our label errors in a data set and then remove them prior to training. so you just train on like correctly labeled stuff. And the corollary here in the sense of the student is that if your teacher is basically making mistakes 30 of the time like imagine as I was teaching you which hopefully I don't do today that 30 of the time I told you wrong things and then you compare that to another teacher who comes and they tell you right things you know 100 of the time then like which teacher do you think you'll probably learn data Centric AI better from And so the idea is that we want to take this bad sort of. You know those 30 of wrong things and we want to get rid of them so that you could sort of come to the classroom and redo your learning experiences if those never happened. And that's the idea here. So you just learn on the good stuff. Um, so those are two approaches. Um, and then what we'll think about now is sort of what is the difference between model Centric Ai and data Centric AI. So the the sort of normal way you hear this is that given a data set, you try to produce the best model and that's like the classical way of thinking about model Centric AI And the idea is that you want to change the model somehow to improve. You know some performance on an AI task and is is there anyone here just out of curiosity who's not familiar with like AI tasks and some of the stuff we do in AI Just want to make sure that we're on the same board like classifying things. Okay, sweet. All right. so you've got some AI tasks and you're trying to classify something and the goal is you want to improve the performance on that. So usually you'll change the model to do that um, and data Centric AI Instead, it's given some model right that may be fixed or you may change, but let's just assume it's fixed. For now, you want to improve that model by improving your data set. And so this is like the common way to think about the two and the idea is the systematically or algorithmically not have a bunch of humans changing the data set, but like some algorithmic way that you can do this. Okay, all right. So our goal is to start thinking about Ml in terms of data and not the model. And so we'll just start out with a simple example. and this is not data Centric AI But it'll get us thinking about how we can think about AI in terms of data. So just quick show of hands. Who here is familiar with Knares neighbors. Okay, great, all right. So I won't belabor this point then. So um, do some chalk. That was a good thing to check beforehand. We were here last night, but the chalk has been removed. All right. Um, so yeah. the key idea with K nearest neighbors and and the key idea to think about in the context of data Centric learning is that K nearest Neighbors imagine you have you know, a space, a 2d space which you know what? Here's what we'll do. all right. So you've got some. Um, all right sweet. So you've got Say you know some data, You've got some triangles, and you've got some some circles. you'll have to apologize for my drawing. And you've got some squares. And so this is your data set. And let's say you know these are different types of images that you've put in some 2D space somehow. Or it's text that you've mapped a 2d space and then the idea of K nearest neighbors as many of you seem to know and are familiar with is that you would have some new Point Say here. And with this, it's some weird thing we don't know what it is and we're trying to figure out. Is it a triangle? Is it a circle? Is it a square? And so if this was sort of three k nearest neighbors like then you would find the three nearest neighbors. So say this one, this one, and this one and then can somebody tell me like what the class would be and why It's clearly yeah, it'd be a square. That doesn't mean that it's not a cool Point Um, and if you're familiar with being a square, Um, so this would be labeled a square and that's based on majority voting and there's a lot of different ways to do algorithms for deciding what the label should be. If it was say five, uh, neighbors meaning five and in where K is five so it's k n then you would choose the five nearest neighbors and you would do a majority vote. Okay, the key idea here is that there is no loss function. Uh, literally. anytime you have a new point, so say I just have another new Point here. I'm not sort of. There's no algorithm that I'm passing this into Beyond just measuring a distance some notion of distance between this and the nearest points. You can do a bunch of pre-computation There's a lot of smart work actually done in K nearest neighbors. that is pretty impressive, and you can do embeddings for all of these in pre-compute distances. and you can do all sorts of fancy stuff. But ultimately all this decision is based on is the data. And so I wanted to motivate this problem and this way of thinking because this whole decision process is made just Based on data and the quality of the data is as related as possible to the quality of the prediction. So if you have really good data, you're going to have really good predictions. and if you've got a bunch of errors in here, you're going to get the wrong prediction. And so this makes it really clear why fixing the data will make it will improve your model. So is that kind of clear how this is motivating? Now, this is not a data Centric AI algorithm and we'll by the end of this like lecture. You'll definitely know what a day-centric algorithm is. But can someone tell me like what the difference is between K and N and a data Centric AI algorithm? Yeah, foreign is just doing classification and it's not actually modifying the data set? Um, so yeah, that's exactly right. All right. So what are a few other examples of like what is not data Centric AI Um, Hand picking a bunch of points you think will improve a model? Uh, so can anybody help me understand why this is not data? Centric AI Yeah, Yeah, totally. It's just done by hand Like this could. if you had a hundred million I did a set of 100 million points. This would take a long time. Um, what about doubling the size of your data set so that you can train an improved model? Yeah, yeah, totally. This is just classical machine learning. Like it's still all the model, all the work you do as a model, but you're just you're just paying more money for more data. Um, so let's juxtapose this. So what would be the data? Centric AI Versions of this? Um, and just just out of curiosity, does anybody know for the first one, what would be sort of the data Centric AI Corollary or select ones? Well, yeah, totally. And there's a whole field of research on this and a whole subsection of Ml that's called corset selection where you have a data set and let's say that you train a model on that data set and you get like 98 accuracy. But the data set is. it's like 100 billion points and so the goal is can I find like you know a million points that if I just train on those I can get like pretty close like 97 accuracy. and that's Corset selection? Um what about for number two, what would be like a data Centric AI Corollary Everyone that extrapolates me. Um yeah yeah. So the idea is. and how does anybody know of any ways that you might make your data set uh, like bigger without just getting twice as much data? Yeah, totally. So like say you have uh so data augmentation awesome and so say you have an image. You could just totally rotate that image and now instead of one data point you have like five data points or turn it black and white. Or you could shift it or skew it or take the top and the bottom. move them a little bit or add some noise. There's lots of things you can do to make data bigger and actually improve a model just by changing the data set and these are all fall within data. Centric AI Uh, are you all familiar with what's called like back translation for tax data augmentation? So this this is a pretty fun thing. I I Don't like have particularly good translation skills. Um, but like if I say like hi you know my name is uh Curtis Um, and then I translate this. Okay, so it becomes hola uh and then I translate this again it might become and now I've gotten one data point and I just got another data point. So I was able to augment my text data set to get more versions of the same thing and this could have some label and the label. could be introduction. You know I'll just end it there because I know it's hard to see on that side but this this would be like the label and this is text and so this one is the same label. We haven't changed the label at all. so this is intro and now you can see I have more label data but I didn't have to do anything I didn't have to pay more I didn't have to I just did this all computationally Yeah, Oh yeah, totally can. So if say that this label was wrong. now you have two label errors. Totally yeah. So you often want to come by and two approaches. One is like first try to fix or improve label errors before doing the augmentation and that's a really good idea. All right. So what are some examples? We looked at examples that are not data Centric AI So what are some things that are data? Centric Ai And for this, I'll I'll go through quick. Um, because we're going to learn all these in the course And so uh, one is outlier detection and removal. So um I'll just be really quick to show you. So say you have some data set and we've got. let me use this board. Yeah, and you've got say right? So you've got your sort of your uh you know, two classes and so normally you would. you would draw your classifier and then you have some new Point here and that would be labeled a negative. but say in your training data you have this really weird, you know, uh plus over here and so maybe the boundary should be something like this. But but you just don't have very much information and it's really out here and as it seems like it's not very related to the rest of the data and so what often people will do because they just don't have enough information is they'll identify this as an automatic as an outlier because it seems very out of distribution and they'll remove it. and so then you get this line which fits to all the data except for that one point. Um, in terms of some other things in data Centric AI So error detection and correction and so that's for example, you just have like a data point that is, it could be images. it's like all black or you have a label error like we saw earlier. If this you know text example instead of being labeled intro was labeled like uh, you know a goodbye clause or something. You'd want to find that and automatically correct it. Data Augmentation which is what we saw earlier. So that's like increasing the size of your data set so you have more training data. uh, feature engineering and selections. The idea here is you have um, if anyone's familiar like the early days of neural networks couldn't solve like the Xor problem, but you could always just generate the xor as another column. A more concrete example, if you haven't heard of this that one is you just have you know a bunch of tabular data. I Worked on cheating detection at MIT And you know if you want the machine learning model to learn, Say who is a cheater from a bunch of uh, you know, Education Data like where do they go to school and what's their background and what problems did they answer? It can really help if you also compute new features like how often did they submit answers within five seconds of another student? So you can always generate new features and then you pass those into the model and your model. Can you know if the features are relevant to the label you're trying to predict? They can do a lot better. We're doing outlier detection and removal. How do you know that an outline fire is the result of something that should not have happened or an indication of a very rare event That you should pay attention to I that's a that's a really fair and hard question to answer. You make assumptions and so in this case I was making an assumption right I was saying like I didn't draw very much data but say that I had like millions of data points and then this one's really far away. Then the Assumption you're making is that like it I have so much data that suggests that this is the distribution and then I have very little data that suggests that this is part of that distribution. and the biggest issue is that if your classifier is changing dramatically for one data point, but you have, just think of it as like evidence. I've got I've got a thousand or a million people saying it should be this thing and then I've got this one other person who may be right, they may be right, but they're saying that I think it's this other thing and it greatly skews the classifier and so just because you you want to trust the masses, you will say hey, that one person is really seems really off base and this is very much a choice and so what. Typically what you do is you sort of rank every data point um in terms of how in distribution it is and then you choose your cut off and that's a human decision. Um, another data Centric AI task is to establish consensus labels. Uh, so if you guys have heard of like the self-driving cars of course, then a lot of the ways that these models are trained is you'll have an image and then they want really high quality data so they'll have like 20 different people label. Is this a scene of a street? or are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The question is, when you're when you're training your model, if you're just doing a train with one label for that image or one set of labels for that image, you can't use all 20 of your annotators. You know, guesses. you have to somehow combine them into a single training label. So how do you do that in a way that maximizes model performance? Another one is active learning and this is a very classical problem. You have some data set, you've trained a model, it has 80 performance. Okay, I want now to get my model to 85 performance. but I want to pay for as little new data as possible. or I want to improve the current existing data as little as possible. What are my next steps and you can automate that process. You can actually get good signal to optimize in a way that you minimize the amount of new data that you need to collect information for or label in order to achieve that model accuracy. And then a final example is curriculum learning which we already mentioned. So these are some examples of data: Centric AI tasks, many of which we'll cover over the next two weeks. All right. So there's a lot of hype around data: Centric AI For those who are familiar with Andrewing he's a pretty well-known uh person in AI from Stanford and has been at you know, Google research and Body Research and done a lot of things found in Coursera So he's been really excited about data Centric Ai And let's look at some reasons. you know why, and some of the things that we've seen in the news. For example, he mentioned that like 80 percent of an AI developer's time has actually just spent on data. which is kind of funny, right? You know you're an AI developer. You're not like a data scientist, but yet, you're doing data science work all the time. And so there's something happening here in the real world that there isn't as much until until recently. Actually, systematic. You know, learning and teaching around. how do we go about doing this? Also, if you're not familiar, bad data is very, very trouble. Awesome for businesses and for the government and for economies. And it's estimated this is out of Harvard Business Review that it costs the U.S alone about three trillion dollars in a given year and you might see this and think, okay, that's really bad. But the good news is, like a lot of people think that we can actually solve a lot of that 3 trillion issue that bad data causes with data Centric AI techniques. And so there's a lot of hype around it because it means a lot to to a lot of people. This is a quick example: I Did this internship at Uh at Fair in 2016 and I was in John's group and Jeff Hinton came to visit. If you're not familiar, these two recently won uh, the Nobel Prize of computer Science, which is called the Turing award. Um, and so I think they're old friends and Jan has a data set. Mnist Are folks familiar with Mnist. Okay, cool. very classical machine learning data set and we've been training models on it for like over 20 years and uh, people generally assume that it has perfect labels because that's that's a very common assumption. Not maybe now in 20, you know, 23, but definitely. uh, like when it first came out and it's a very high quality data set and so Jeff Hinton was presenting at this time I Think capsule Network seems very excited about it and his aha culminating moment of his talk was that he found a label error in Jan Lacun's data set and so he's very excited to show. you know, hey, this five image is actually labeled a three in your data set. Yawn and he's like aha, I got you and so I think that it's worth mentioning that this is where we were in 2016. and now you know we're only six. Seven years later and we're able to systematically find millions of errors and data sets. And that's sort of How Far We've Come using these data Centric AI approaches. Uh, who here's familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images and they're pretty cool. Like you can generate images of pretty much anything you describe. And so if you check out the Dolly demo page and there's the link here in the slides if you want to check them after, there's a cool video and you'll notice in that video, they talk about one of their biggest challenges. And so this is just screenshots from the video. and the technology is constantly evolving. But Dolly 2 has limitations. It's taught with objects that are incorrectly labeled with plain labeled car for example. And this happens because it's a massive data set. so if you don't use data, Centric AI approaches it's very difficult to clean. You know, whatever, hundreds of millions or more, probably billions of of pairs of text and image. And so what they notice is that a user might actually try to generate a car, but Dolly will actually create a plane because it's seen wrongly labeled data and so it's very problematic for something that's deployed in the real world. Another example from that video is they talk about generating like these baboons, but they emphasize that like you can only do this correctly if you have accurate labels. and if you didn't like, you'll totally you can get the wrong thing. And so the key takeaway here is that this is a real world technology Lots of people are using today at scale, but the reliability of that model really does depend on the data quality. Another big example. Uh, that's familiar with chat GPT Ah, does anybody know? Yeah, uh. Does anybody know sort of why or like what was the big innovation from Gpt3 which obviously had a huge hype around it to chat? GPT Which has even more hype around it? What was sort of one of the big things they did between the two? Yeah, totally. And do you know what they were doing with their reinforcement learning? Yeah, totally yeah. What was happening There was they. Um, they had a lot of bad outputs. You know, like chat. Uh, Gpt3 was saying things that were like super biased or like inappropriate. Um, not even true, just wrong facts. and these outputs were tied to data it was trained on and to parameters in the model that were learned from that data. And so what they did is they did in a reinforcement setting. Which means they talked to people. They use that information to then update the model. Then the model sort of explores with new outputs. Then people see those. but what they were doing is they were having people actually rank them in terms of the quality of the prediction right? and so they were ranking the quality of this data and then using that to update the model so that it would have improved, less bias and better outputs. And so that was the key idea of chat. GPT was actually to deal with a data quality issue and if you've tried both, you can see that there is a pretty big performance boost. The downside is that they had to do this with a lot of manual work and so we went to work on ways to automate that. Um, you guys are probably familiar with Tesla So this is Tesla's data engine. This is from a talk by Andre Carpathy who was formerly the Tesla director of AI and this is their data engine and we'll just start in the top left like you. The way they're training the you know the self-driving Tesla Model is you know you have some data source and then you'll notice some problem which is like Hey we're in a tunnel and we don't have a lot of you know tunnel data. So the mo the the car is like doing weird things right and so then what they would do is they would collect a bunch more data in tunnels and then update their training data and label them and then redeploy and go in the world and then see what breaks. Then this is a very you know difficult iterative process because you have to send the car out and then see where things break and then collect a bunch more data. Or if you had a way to automate. okay, this is where my data is missing. This is stuff that's out of distribution. This is where we have a bunch of labelers and you're able to automate that process. They could have reduced those cycles and gotten the car out a lot faster, at least the AI part of the car. And so this was a big pain that that Andre mentioned. Another, Really, this is a fantastic example that Jonas shared with me. These are all examples of traffic lights. you know. So imagine that like Elon Musk comes to you and he says you know Andre I need you to get this car to navigate any Street in the world and then you know you're like, oh, that's cool. Okay, so like I need to stop at traffic lights like that Seems like a pretty simple problem and then you go out in the real world. And like, traffic lights are not a simple problem, they're really complicated and they're really messy. and this is actually like a total nightmare if you had to to do this. And so how do you find sort of systematic ways to group data together and train in a way that's robust and reliable? Like real world data is super messy and complicated. And so Andre's big takeaway was that you know he shares this juxtaposition of the amount of sleep you know he lost over in his PhD and it was like data sets is this tiny sliver, but definitely spent a lot of time on models and algorithms. and then he goes and he's leading. You know the the AI model at Tesla and it's like it's all data, you know. and it's a big shock when you make this shift. And so that's why you really want to focus on ways we can improve that. A very common use case is when you're trying to train a model with noisy labels. Okay, so this is like the the classical scenario of your your you have the dogs and cats, but now say 30 percent of your cats are labeled dog? okay and maybe 20 of the dogs are labeled cats. So how do you get a model that does as well or close to as well as if you had perfectly labeled data And we'll go into that more in the next lecture. But I Want to just motivate that I Looked at a bunch of Uh Model Centric methods and data Centric methods over the last five years out of top institutions like Google and Facebook and so forth. and we benchmarked them. and it turns out and there's a lot on this slide. But there's really one key takeaway that the data Centric AI methods all outperformed the model-centric methods for this particular task. You know on this particular data set, and this was pretty revealing and compelling That there's something here to data Centric approaches. And to be very clear, what these models are doing is they are modifying the loss function or modifying the model so that they sort of don't train as much on what they think is bad data. But within the context of the modeling and what these methods are doing is they're actually modifying the data set. They're either removing bad data or they're generating more data. That sort of. It makes the air go away. But somehow they're actually changing the data set and this is just how things stack up and you'll see the data. Centric Methods outperform model Centric Methods in this task and this is a very common task that's of interest to the field, so it's cool. It's cool to see that this stuff is is working and we're getting some benefits from it. Um, so a sort of, uh, culminating thought is, you know we were talking a lot about ways that we want to automate, But what did we do before? So obviously we've had to improve data sets in industry. and like outside of Academia in the past, it's like, how did we do it before? There was Data Centric AI and so we mostly relied on human-powered solutions. For example, we would just spend more money for higher quality data. It's like you literally just pay for more labels or you would pay for more data And that was a very classical way to improve a model. Also building custom tools. So like you saw at Tesla, they have this whole sort of data platform and this is a lot right? This is for one specific problem, a very important problem, but they had to build a lot of custom Tech around it. Another common thing is just fixing data inside a jupyter notebook. so just a quick show of hands like how many people have used Jupiter notebooks. Okay, that's really good because all the labs are in Jupiter notebooks for the most part. Um, for this lecture or for this course. So um yeah, what people do is like you just sort data by like a loss function and so you just say like the loss for this data point is the highest. So I think this is most likely to be wrong. Let me check it out and then you would look at it by hand and then you would mark it or do something with it or you take the top 20 or something. But you just do a lot of this by hand inside of Jupiter notebooks and printing things out and that was actually a pretty normal and standard way that like you know, somebody in industry or a data scientist or a grad student would try to fix a data set and so the whole idea is, we're going to look at ways that we systematic systematize these approaches so that they're more reliable, more accurate, and they work on most data sets. So this course is about the following: So Today we're just looking at what is model Centric Eye versus data Centric Eye Get the juices flowing. Think about how to think things in terms of data and the impact why it matters. Next lecture, we'll focus on label errors. So how do you actually detect label errors automatically? How do you learn with label errors? What are good methods to do that, and what are some things to think about when you're doing this? Data set creation and curation will be on Thursday And this is how do you construct a data set in such a way that you can train a good model, How do you arrange you know the classes, how do you choose good examples, and then finally on Friday which is related to data set curation. We'll look at Active Learning and potentially core sets and active learning. As I mentioned is this task where you're trying to choose the next data you want to add to your data set and you want to obtain a label for or do you want to improve some of the labels you currently have And so you're just trying to decide I have to pay a cost for new data that I'm going to add to my data set and it cost me something like either money or time. So I don't want to do it too much. So what's like the best stuff to add to my data set. Now to improve my model. next week we'll sort of have a bit of a shift and we'll focus more on data, but we'll focus on some some specific things. For the First on Monday, we'll focus on class and balance and distribution shift. So this is Imagine like it's the stock market right and like if you're trying to predict things you know on Monday or on in January of this year versus now, it would be a very different market right? and so over time data changes. and so how do you continue to produce good, reliable predictions even though data is changing and then the class imbalance is this problem where imagine that for that line over there on the left. we had like a million pluses and only a few minuses. Well then a smart classifier could actually just always predict plus and get near 100 accuracy. so often it will just ignore the minuses. So how do we get around that problem? Um, interpretable features of data? So this is who here's familiar with interpretability. All right, not as much that one will be fun then. Um, and thanks for raising your hand. Um, that should be a good class. We'll learn about how do you interpret data in a way that you can understand what's going on from a data perspective. like why is the model doing what it's doing in terms of the data And so we'll understand. Model Performance Based on data. The next class on Wednesday of next week will be on data Centric Evaluation of Ml models. So like, how do we know you know from a data perspective, how good a model is, how reliable it is, how well it's working. Um, on Thursday We'll look at encoding human priors so this is like how do we augment data and also prompt engineering. So this class will cover a lot of things like GPT and chat GPT and Transformer models and stuff like that. And then finally, our last class will be on data privacy and security. And this is a very, very interesting, especially for a lot of people in like Banking and finance. And they're using a lot of machine learning models. How do you make sure that like a model doesn't actually secretly encode the data or like somehow if you had access to predictions, you could figure out someone's you know banking info and you can imagine all sorts of things that can happen in AI? So data security and privacy is really important. Any sort of questions while I'm on this slide? Sweet. you guys excited. Okay, this look like a good course. Okay, all right. cool. Um, all right. there's a lab for every lecture. Uh, and so you can find us on the course website. Uh, which we can we can write on the board. Um, so you probably have seen it in the email, but just in case and uh, there's there's the lab usually will be Jupiter notebooks and the one for today will be a text classification task and it has some bad bad data that's gotten mixed in and this is actual data that that's been scraped um from I think Amazon reviews and it has some bad tags. Some weird HTML has gotten in there and what you'll look at is model. Centric approaches at first and you'll realize oh, it can only get you so far because like the data's not that great and so you'll have to figure out how to improve the data set. Uh, you know, so you can get a better classifier and so that's sort of today's lab. Get your hands wet with uh data Centric AI We have office hours every class uh 3 P.M to 5 P.M So an hour after the lecture ends every day and then tomorrow's lecture will focus on label errors, how to find them, and how to train better models. Um, this is the folks who are teaching the course for folks are from MIT Uh, two from Stanford and um yeah, I think it'll be a good time. Uh, really quick. Are there any questions? Yeah, like how do you know if it's like data that's the problem and not the model like. Let's say you train a bunch of times, It's still not. It's not like doing good. How do you know the data is a problem? Still, Do you just try it and then see if it improves or you can use? Is there another way to do that? Yeah, that's it's a good question. So there's a few ways So one thing you can do is you can look at a subset of the problem so say you had like a very big complex problem and there's thousands of classes and millions of data points. You can take just 10 000 of those data points in a few of those classes and actually check. Do some process to check, make sure you have really high quality data and see how well does your model perform and if your model is performing very very high accuracy. But then when you use the original data, you get a drop off that gives you a good signal. Another another thing is if you have similar data set and uh, it's like Mnis for example, you can get near 100 performance and so you have a similar data set, but you're getting like much worse performance or Reason like significantly less performance, but using the same architecture that you've seen do very well on a similar task on a different data set and that's a good indicator. You should probably look at the data and there's two more things. One is just just take a look at the data I Think it's really easy to just get a data set and your your goal is like train a model and so you're doing a lot of cool. You know, download this tensorflow package, download this hugging face package. but you don't actually take time usually to look through all the data points or like hundreds of data points and really see like does this data look like what? I Think it does Like does it seem to be kind of messy and what you'll often find is after first like you know first few hundred you're like oh there's some weird stuff in here and if you have millions of data points, that weird stuff adds up two months. So like you know, like how do you connected yeah I guess if you just look at like a bunch yeah, totally. Uh, in the in the next class we'll show ways that you can actually rank your data set so that you know what's the best example to look at first. that's probably wrong and so there are ways that you can automate this and then you can look at the initial data and that's really the right approach. Like if you just look at random then you might waste some time. But if you've ranked your data in a way that is likely to give you a good ranking on quality, and then you look at like the first 100 and there's really no issues and the data looks really good Then yeah, you might be able to just optimize the model and be okay. Any other questions, is this useful for anyone? Sort of Immediately, is anyone thinking like hey, this might be useful for what I'm working on right now. Foreign? All right. great. We'll be here for a little bit after. Uh, thanks everybody for coming. The next lectures will be a bit more technical. Um, but uh. I think it should be a really exciting course and glad to have everybody here. Thanks.</div></div></div></div></div></div><div contenteditable="false" data-content-editable-void="true" style="width: 0px;"><div style="display: none; flex-shrink: 0; pointer-events: none; width: 0px; position: absolute; right: 192px; opacity: 0;"><div style="display: flex; flex-direction: column; padding: 5px 16px; width: 340px; flex-shrink: 0; height: 100%; position: relative; pointer-events: none; z-index: 1;"><div style="position: absolute; pointer-events: none; width: 100%; height: 100%; top: -5px; background: linear-gradient(white 0px, rgba(255, 255, 255, 0) 15px);"></div></div></div></div></main><span style="height: 1px; width: 1px;"></span></div><div class="notion-presence-container" style="position: absolute; top: 0px; left: 0px; z-index: 89;"><div></div></div></div></div></div><div class="notion-peek-renderer" style="position: fixed; top: 0px; right: 0px; bottom: 0px; width: 960px; z-index: 109; transform: translateX(960px) translateZ(0px);"></div></div></div></div></div><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><script src="assets/js/1172e9111a5fb396bcb8a05870b5eabf8abf221c.js" type="text/javascript"></script><div style="width: env(safe-area-inset-bottom);"></div></body></html>