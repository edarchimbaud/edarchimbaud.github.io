<html class="notion-html"><head lang="en"><meta charset="utf-8"/><meta content="width=device-width,height=device-height,initial-scale=1,maximum-scale=1,user-scalable=no,viewport-fit=cover" name="viewport"/><title>6. Growing or Compressing Datasets</title><meta content="en_US" property="og:locale"/><link href="data:image/png;base64,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" rel="shortcut icon" type="image/x-icon"/><link href="/images/logo-ios.png" rel="apple-touch-icon"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="telephone=no" name="format-detection"/><meta content="no" name="msapplication-tap-highlight"/><link href="assets/css/e218a1aef6df86309cb95b61e446888efa3b0379.css" media="print" rel="stylesheet"/><link href="assets/css/e7df85b6a5b33bc52629df6d3bd37d197c7a873d.css" rel="stylesheet"/><meta content="Edouard d'Archimbaud" name="title"/><meta content="Edouard d'Archimbaud official website" name="description"/><link href="assets/css/e1d809d762eeca23edf0cb31bb17bf3c703085f5.css" rel="stylesheet"/><style type="text/css"></style></head><body class="notion-body"><style>body{background:#fff}body.dark{background:#191919}@keyframes startup-shimmer-animation{0%{transform:translateX(-100%) translateZ(0)}100%{transform:translateX(100%) translateZ(0)}}@keyframes startup-shimmer-fade-in{0%{opacity:0}100%{opacity:1}}@keyframes startup-spinner-rotate{0%{transform:rotate(0) translateZ(0)}100%{transform:rotate(360deg) translateZ(0)}}#initial-loading-spinner{position:fixed;height:100vh;width:100vw;z-index:-1;display:none;align-items:center;justify-content:center;opacity:.5}#initial-loading-spinner svg{height:24px;width:24px;animation:startup-spinner-rotate 1s linear infinite;transform-origin:center center;pointer-events:none}#skeleton{background:#fff;position:fixed;height:100vh;width:100vw;z-index:-1;display:none;overflow:hidden}#initial-loading-spinner.show,#skeleton.show{display:flex}body.dark #skeleton{background:#191919}.notion-front-page #skeleton,.notion-mobile #skeleton{display:none}#skeleton-sidebar{background-color:#fbfbfa;box-shadow:inset -1px 0 0 0 rgba(0,0,0,.025);display:flex;width:240px;flex-direction:column;padding:12px 14px;overflow:hidden}body.dark #skeleton-sidebar{background-color:#202020;box-shadow:inset -1px 0 0 0 rgba(255,255,255,.05)}#skeleton.isElectron #skeleton-sidebar{padding-top:46px}#skeleton .row{display:flex;margin-bottom:8px;align-items:center}#skeleton .row.fadein{animation:1s ease-in 0s 1 normal both running startup-shimmer-fade-in}#skeleton .chevron{width:12px;height:12px;display:block;margin-right:4px;fill:rgba(227,226,224,.5)}body.dark #skeleton .chevron{fill:#2f2f2f}.startup-shimmer{background:rgba(227,226,224,.5);overflow:hidden;position:relative}body.dark .startup-shimmer{background:#2f2f2f}.startup-shimmer::before{content:"";position:absolute;height:100%;width:100%;z-index:1;animation:1s linear infinite startup-shimmer-animation;background:linear-gradient(90deg,transparent 0,rgba(255,255,255,.4) 50%,transparent 100%)}body.dark .startup-shimmer::before{background:linear-gradient(90deg,transparent 0,rgba(86,86,86,.4) 50%,transparent 100%)}#skeleton .icon{width:20px;height:20px;border-radius:4px}#skeleton .text{height:10px;border-radius:10px}#skeleton .draggable{-webkit-app-region:drag;position:absolute;top:0;left:0;width:100%;height:36px;display:none}#skeleton.isElectron .draggable{display:block}</style><style id="scroll-properties"></style><div id="notion-app"><div class="notion-app-inner notion-light-theme" style='color: rgb(55, 53, 47); fill: currentcolor; line-height: 1.5; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; -webkit-font-smoothing: auto; background-color: white;'><div style="height: 100%;"><div class="notion-cursor-listener" style="width: 100vw; height: 100%; position: relative; display: flex; flex: 1 1 0%; background: white; cursor: text;"><div class="" style="display: flex; flex-direction: column; width: 100%; overflow: hidden;"><div style="max-width: 100vw; z-index: 100; background: white; user-select: none;"><div class="notion-topbar" style="width: 100%; max-width: 100vw; height: 45px; opacity: 1; transition: opacity 700ms ease 0s, color 700ms ease 0s; position: relative;"><div style="display: flex; justify-content: space-between; align-items: center; overflow: hidden; height: 45px; padding-left: 12px; padding-right: 10px;"><div class="notranslate" style="display: flex; align-items: center; line-height: 1.2; font-size: 14px; height: 100%; flex-grow: 0; margin-right: 8px; min-width: 0px;"><div class="notion-selectable notion-page-block" data-block-id="d8397a78-1321-456c-9c10-1feea7a42b60" style="display: flex; align-items: center; min-width: 0px;"><a href="edouard-d-archimbaud.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üë®‚Äçüíª" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üë®‚Äçüíª</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Edouard d‚ÄôArchimbaud</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div class="notion-selectable notion-page-block" data-block-id="bb024d08-b3b1-4dbd-bafc-0fa8d6a316c2" style="display: flex; align-items: center; min-width: 0px;"><a href="data-centric-ai.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéØ" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéØ</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Data-centric AI</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 6px; padding-right: 6px; color: rgb(55, 53, 47);" tabindex="0"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 240px;">6. Growing or Compressing Datasets</div></div></div><div style="flex-grow: 1; flex-shrink: 1;"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="searchNew" style="width: 14px; height: 14px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg>Search</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0">Duplicate</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: flex; align-items: center; justify-content: center; width: 32px; height: 28px; border-radius: 3px;" tabindex="0"><svg class="dots" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden;" viewBox="0 0 13 3"><g><path d="M3,1.5A1.5,1.5,0,1,1,1.5,0,1.5,1.5,0,0,1,3,1.5Z"></path><path d="M8,1.5A1.5,1.5,0,1,1,6.5,0,1.5,1.5,0,0,1,8,1.5Z"></path><path d="M13,1.5A1.5,1.5,0,1,1,11.5,0,1.5,1.5,0,0,1,13,1.5Z"></path></g></svg></div><div style="flex: 0 0 auto; width: 1px; height: 16px; margin-left: 8px; margin-right: 8px; background: rgba(55, 53, 47, 0.16);"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="notionLogo" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 120 126"><path d="M 20.6927 21.9315C 24.5836 25.0924 26.0432 24.8512 33.3492 24.3638L 102.228 20.2279C 103.689 20.2279 102.474 18.7705 101.987 18.5283L 90.5477 10.2586C 88.3558 8.55699 85.4356 6.60818 79.8387 7.09563L 13.1433 11.9602C 10.711 12.2014 10.2251 13.4175 11.1939 14.3924L 20.6927 21.9315ZM 24.8281 37.9835L 24.8281 110.456C 24.8281 114.351 26.7745 115.808 31.1553 115.567L 106.853 111.187C 111.236 110.946 111.724 108.267 111.724 105.103L 111.724 33.1169C 111.724 29.958 110.509 28.2544 107.826 28.4976L 28.721 33.1169C 25.8018 33.3622 24.8281 34.8225 24.8281 37.9835ZM 99.5567 41.8711C 100.042 44.0622 99.5567 46.2512 97.3618 46.4974L 93.7143 47.2241L 93.7143 100.728C 90.5477 102.43 87.6275 103.403 85.1942 103.403C 81.2983 103.403 80.3226 102.186 77.4044 98.54L 53.5471 61.087L 53.5471 97.3239L 61.0964 99.0275C 61.0964 99.0275 61.0964 103.403 55.0057 103.403L 38.2148 104.377C 37.727 103.403 38.2148 100.973 39.9179 100.486L 44.2996 99.2717L 44.2996 51.36L 38.2158 50.8725C 37.728 48.6815 38.9431 45.5225 42.3532 45.2773L 60.3661 44.0631L 85.1942 82.0036L 85.1942 48.4402L 78.864 47.7136C 78.3781 45.0351 80.3226 43.0902 82.7569 42.849L 99.5567 41.8711ZM 7.5434 5.39404L 76.9175 0.285276C 85.4366 -0.445402 87.6285 0.0440428 92.983 3.93368L 115.128 19.4982C 118.782 22.1747 120 22.9034 120 25.8211L 120 111.187C 120 116.537 118.051 119.701 111.237 120.185L 30.6734 125.05C 25.5584 125.294 23.124 124.565 20.4453 121.158L 4.13735 99.9994C 1.21516 96.1048 0 93.191 0 89.7819L 0 13.903C 0 9.5279 1.94945 5.8785 7.5434 5.39404Z"></path></svg>Try Notion</div></div></div><div style="width: calc(100% - 0px); user-select: none;"></div></div><div class="notion-frame" style="flex-grow: 0; flex-shrink: 1; display: flex; flex-direction: column; background: white; z-index: 1; height: calc(100vh - 45px); max-height: 100%; position: relative; width: 1920px;"><div class="notion-scroller vertical" style="display: flex; flex-direction: column; z-index: 1; flex-grow: 1; position: relative; align-items: center; margin-right: 0px; margin-bottom: 0px; overflow: hidden auto;"><div style="position: absolute; top: 0px; left: 0px;"><div></div></div><div class="whenContentEditable" data-content-editable-root="true" style="caret-color: rgb(55, 53, 47); width: 100%; display: flex; flex-direction: column; position: relative; align-items: center; flex-grow: 1; --whenContentEditable--WebkitUserModify:read-write-plaintext-only;"><span style="height: 1px; width: 1px;"></span><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0; z-index: 2;"></div><div style="width: 100%; display: flex; justify-content: center; z-index: 3; flex-shrink: 0;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div style="max-width: 100%; padding-left: calc(96px + env(safe-area-inset-left)); width: 100%;"><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; pointer-events: none;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px; border-radius: 0.25em; flex-shrink: 0; position: relative; z-index: 1; margin-left: 3px; margin-bottom: 0px; margin-top: 96px; pointer-events: auto;"><div style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px;"><div style="height: 78px; width: 78px; font-size: 78px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notion-page-controls" style='display: flex; justify-content: flex-start; flex-wrap: wrap; margin-top: 8px; margin-bottom: 4px; margin-left: -1px; color: rgba(55, 53, 47, 0.5); font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; height: 24px; pointer-events: auto;'></div></div><div style="padding-right: calc(96px + env(safe-area-inset-right));"><div><div class="notion-selectable notion-page-block" data-block-id="9e7de2a0-c35d-4892-a4cb-3324c71ce689" style='color: rgb(55, 53, 47); font-weight: 700; line-height: 1.2; font-size: 40px; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; cursor: text; display: flex; align-items: center;'><div contenteditable="false" data-content-editable-leaf="true" placeholder="Untitled" spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">6. Growing or Compressing Datasets</div></div><div style="margin-left: 4px;"></div></div></div></div></div><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div contenteditable="false" data-content-editable-void="true" style="padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right)); max-width: 100%; width: 100%;"></div></div></div></div><main style="display: flex; width: 100%; justify-content: center; padding-top: 5px;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div class="notion-page-content" style="flex-shrink: 0; flex-grow: 1; max-width: 100%; display: flex; align-items: flex-start; flex-direction: column; font-size: 16px; line-height: 1.5; width: 100%; z-index: 4; padding-bottom: 30vh; padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right));"><div class="notion-selectable notion-text-block" data-block-id="fd49e4e8-c675-4a9e-af45-298b95baebb5" style="width: 100%; max-width: 1728px; margin-top: 2px; margin-bottom: 0px;"><div style="color: inherit; fill: inherit;"><div style="display: flex;"><div contenteditable="false" data-content-editable-leaf="true" placeholder=" " spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">All right. well good afternoon everyone! Uh, super excited to see all of you! I'm Cody Coleman I'm a co-founder of co-active AI and I serve as the CEO and I'm super excited to be here today to tell you about how to grow and compress data sets as part of the introductory course to data Centric AI So in thinking about it, today's oh also I should I probably forgot to mention the most important important point you know I'm a fellow MIT Alum I did my bachelor's in master's degrees here at MIT and 6-2 and then went on to do my PhD at Stanford and computer science. So it's really good to be back and to see like a lot of familiar places and things like that. So in today's lecture we're going to kind of focus on or come back to labeling um, and uh, pieces of labeling. So already in the first lecture on data Centric AI versus model Centric AI we talked about why I care about labels and care about how we create data sets you know the importance of and the shift kind of in the Ml field to focusing on data in order to actually be able to have kind of practical advances and improvements. In the lecture on dataset creation and curation we actually talked about like who labels and how do we actually do that to be able to create high quality annotation information for data sets given like a an example that we want to label. And today we're going to talk about what do we label and specifically we're going to look at methods in active learning which will kind of intelligently select which data points are the most valuable or informative for us to label given like a large amount of unlabeled data. And then on the flip side, if we're in the case where we have a lot of like label data or just a lot of data in general, how can we compress that down into a smaller, more representative set called a core set selection? So in the first part of this uh, this talk in this lecture, we're going to focus a lot on Active Learning So uh, why is selecting uh uh, what to label important? So when we think about actually the labeling process, it's It can be really, really expensive depending on the task that you're doing. For example, in speech recognition, annotating at a word level can take up to 10 times longer than the actual audio recording. and if we want to do fine grain annotations, it can take up to 400 times as long just for a single annotation on that audio clip. And we see similar things in um, you know, relative or relatively straightforward tasks like name, entity recognition and information extraction. We're even locating kind of entities in a simple Newswire stories can take a half an hour or more just in these. like simple, uh, simple cases so this adds out to be just a massive amount of time as well as also like cost. When we think about the labeling process and then if we think about kind of the data set creation and creation process where we might want to have multiple annotators label the same thing, the costs really grow from there. Now if we're actually smart about what we're selecting, we actually don't need to. We don't need to label as many things. So this is a simple graph of doing binary classification for a few classes in Imagenet and a one versus all setting. What we can see is if we had all of the data, we get the dash Gray Line up top in terms of mean average precision. If we did random sampling, so we just picked examples at random and we labeled those, We get the blue line the solid blue line at the bottom. But if we actually do this process of active learning where we're a little bit more intelligent and we adapt to adapt based off of our model's uncertainty using a technique called maximum Entropy, we can actually get massive improvements in the mean average Precision that we want with the same amount of label data. So as you can see here, just like the gap between the blue line and the Orange Line, we can get massive improvements very very quickly by doing Active Learning So what is Active learning? So the goal of Active Learning is to find the best examples to label in order to improve model performance. And this is done through an iterative process, where we start off with a large amount of unlabeled data, so this can be provided to us, like by any process in the world. and then we take some kind of small amount of that, some subset, maybe it's given to us. Maybe we, um, do a random sampling or some other thing. We take this kind of small initial subset and we label those examples from our unlabeled pool. and then with this initial labeled set, we actually train a model on this kind of small subset of labeled data, and this is where the Adaptive part comes in. We actually take that model that we've just trained, and we apply that model with some selection criteria or selection strategy to all of the examples in the unlabeled data set to be able to actually quantify the informativeness or the value of those data points. And then once we have this this kind of metric or value for each one of these individual unlabeled data points, we can actually select the most valuable ones for us to label. Then we actually label those examples, and then we repeat this process. So now we have a slightly bigger labeled subset that we train our model on, and we can keep repeating this process of iteratively applying our model to all the unlabeled data to quantify the informativeness, selecting the most valuable things to label and training a model on it over and over and over again in this iterative kind of adaptive process until we reach some stopping criteria like we've exhausted some labeling budget that we have, at which point we have our fully labeled subset of the data and we can train our model kind of one last time on that full subset of data to actually use it for the downstream protection prediction tasks that we care about. And of course, we can always kind of continually pick up this process and label more data as needed or as we find kind of other things that are uncertain um, uh, in in the wild. So to actually just like write this out from like a pseudo code kind of example. If we think about it, we start off with some initial labeled set. So let me, where's the Oh perfect. Awesome! So we'll have some initial label set that will make our label data. So LR equals LR Naught. Um, so that's that seed set that we just talked about. and then we'll have some Um pool of unlabeled examples for us to pick from for selecting. And this kind of initial pool is going to be all of the Um is going to be the set of our examples from our unlabeled, our unlabeled uh, kind of Master pool that exists in our unlabeled pool and are not in our initial initial labeled seed set. And then from there, the process is actually pretty straightforward with these two sets. What we do is like while are Um, label data set is less than the size of our label data set is less than some kind of labeling budget that we have represented in T. What we do is we train a model so we'll call our model a of R or Ar and we train a model on this kind of currently labeled set foreign and then once we have this model, we can then find the maximum example of our kind of candidate pools to label from. So we want to find the the ARG Max the ARG Max that exists in our can you pull the label using some selection criteria fee that takes in each example X and our model. So we're applying every single we're applying our model and some selection criteria to all of this in order to get the ARG Max of it which we call X star and then with X star we can actually just go and update our label pool by taking kind of our existing labeled examples and we now Union that with one additional example which is a x-star comma, a label of X star and then what we want to do is to make sure that we don't end up selecting the same example over and over again. We also update our candidate pool where now we remove um uh, X star from that candidate pull and this is kind of the the Baseline algorithm that we just described there. This iterative process where we're going, we're training a model, we're finding the maximum uh, kind of most informative data point out of our entire unlabeled pool. We're labeling that and then we're kind of removing it from our candid pool. We complete this process over and over. So relatively straightforward algorithm, but it can actually lead to massive speed UPS In practice, and to actually see this, we can do a simple 1D example. So let's say we're in the case of a one-dimensional line where we have two linearly separable classes. um. On the on the left will see an active learning approach, and on the right we'll see a passive learning or where we're doing random sampling. In both cases, we start off with the same initial seed examples and the same pool of unlabeled data. So the unlabeled data shown in Gray and then the two classes are are red and blue and then what we see is um, kind of right off of the bat with active learning. It basically figures out to bisect the space in between uh, the two labeled examples that we have so far. So we see that it says that we should select the example that's right in the middle of this Red Dot Whereas passive, passive learning or random sampling, we have no control over that. so it's going to pick a data point for us to label at random and then if we do another iteration with active learning, we see that all bisected again in terms of the uncertainty and then we end up with this kind of new blue point where again, with random sampling, you know we were unlucky. We're not getting actually that much more of an informative point and we can repeat this process kind of for a few more iterations and we see them very quickly. The active learning procedure is effectively doing binary search to really hone in on what is a true decision boundary which is represented by the dash Gray Line here. whereas the paths of learning, it's still not really getting to it, we're unlucky in this kind of setup and we find that like you know, in six iterations, this active learning strategy is already at the point that's kind of honed in on what the actual decision boundary is. You can see that by the the blue and the red dots at the center actually overlapping and creating purple, Whereas like the passive learning strategy with random sampling is nowhere near this decision boundary and in fact, Active Learning can give exponential speed UPS So this passive strategy will reduce the error proportional to 1 over n, whereas an active strategy can reduce the error proportional to um uh uh to 2 to the minus n or um. So an exponential speed up in comparison to just the normal passive strategy And we can actually run this like example even further for more iterations. So if we ran to 25 examples compared to where we were with six examples from the active learning strategy, we see that still, the passive strategy is like nowhere near as far as as close to honing in on the decision boundary. If we run that for another 25 iterations, we're still nowhere near the actual decision boundary. 75 iterations. We're still not as close as we got with active learning with six examples. And it's not really until we get to 100 examples of like just randomly selecting that we even get close to this decision boundary. So you can just see the power of like how in this how active learning can provide exponential speed UPS Saving you from having to label all these extraneous data points. Question: Yeah, and looking at like So passive reporting requires more iterations. but in my understanding, each iteration is just habitable computation. or as an active learning for each iteration, need to complete the arguments. Which isn't that technically like photo of the magnitude like we are on the living space So Like, um, how would Active Learning Overall Results: Conversation: Oh yeah, sorry. um I might have misspoke there when I said that Active Learning would perform less computation. It will save you from labeling as much so depending. So this is a super great question, so it will save you on in terms of like labeling costs, not necessarily in terms of computation costs. And this is actually one of the problems that we have with like Active Learning and practice is because for very large like data sets or models it can actually be kind of intractable which I'll talk about in a actually literally the the next slide about this so so you read my mind. So so this method is great for reducing labeling costs. But when we think about kind of modern machine learning, especially deep learning, there's a number of practical challenges that come up. So the first practical challenge is that you know these models aren't necessarily like simple to train. especially when we think about the kind of era of deep learning where we have these massive models that take a tremendous amount of resources to train um, uh and and computationally can be very expensive and almost can be like to the point depending on the type of model that you're training. Um could be as expensive as like that additional labeling costs and things like that. so I'd be curious and thinking about kind of the diagram that we have here like what what becomes computationally expensive when we think about these models Yeah, exactly exactly. So first uh in this first kind of point we're going to focus on the training aspect of IT training it after every single iteration becomes super expensive because of this computational expense. and now the way that people actually kind of get around this in practice, especially once like deep learning became a very popular kind of mode of machine learning as we instead, rather than like retraining this model after every single examples, every single example that we select to label people instead will select a batch of of examples at once and label them all at once so that they don't have to retrain this model. So if we go to this, um, the code that we wrote before, it's more or less going to be the same when we think about batch active when we think about batch Active Learning versus kind of a single example Active Learning So we'll start off effectively in the same way LR not PR So we'll start off in the same way and then same looping process. This doesn't change at all and then we train our model once. Now when we do this batch process and then the only difference is that now you know four, one, two sum budget, or some budget as far as the batch that we want to we want to label, we'll now just repeat this process of doing the ARG Max to get our X-star and updating our data set and everything else. Remains the Same So that is great. This solves this like first problem of like the models being really big and like addresses this computational bottleneck. Um, however, what can one problem one problem? that we can find is depending on your selection strategy, we can end up with a lot of potentially redundant examples. So a very common form of active learning as far as the selection strategy is um, we can write it here is taking the entropy of the predicted probabilities for an example which is just the negative of the sum of the probability of a given label given an example in our current model Times log of that probability. Now this works really well in this kind of initial setting where we're selecting example one at a time and then we're updating. But can anyone think of a problem where if we just deal the uncertainty of the model and we think of a data set or like a case where this could be problematic if we're thinking about batch? Yeah, Yeah, exactly. So if we have a data set that has a lot of redundant or near duplicate examples like we could take for example, any data set in the world, and if we were to just like multiply that every example like a hundred times, then this batch active learning process actually saves us nothing. You know it'll be just as expensive as if we did the single iteration. So there's a lot of work that actually kind of goes into coming up with selection strategies that will not only take in kind of the uncertainty or the informativeness of like the model using something like entropy, but we'll actually try to um to select representative examples as well, doing things like facility location and like, um, greedy case centers in order to select examples that cover a space in our representative overall, or just ensuring that there's some minimum distance between the labels that you've already, uh, kind of, uh, the examples that you've already labeled in your data set and any new examples that you would add. So you can imagine adding an additional criteria here as like a simple fix for this of some kind of minimum threshold or heuristic distance from any example that's currently in your labeled set. But that solves one of the problems that you pointed out earlier. As far as like, you know, training, the model is very, very expensive. But another practical challenge is actually when we think about the size of this unlabeled data set. which when we think about modern data sets and data sets, in practice, we can have really like large, massive, big data problems. so this comes up all the time. In practice, when we think about recommendation systems, if you're at a large internet company, you might have uh, millions or billions of images that are being uploaded by people every single day and you want to be able to kind of identify. maybe some new or interesting Concepts Maybe you're like a care about fashion trends or some new viral like content. like a fidget spinner. You have all this data out there and actually trying to sift through it and process all of it to quantify the informativeness. and the most kind of valuable example for your label can just be computationally intractable. And we see the same thing when we think about like autonomous vehicles I Know that um Curtis on his like first lecture talked about um, autonomous vehicles and Tesla and their whole data engine there. So they also run into this problem where maybe for occluded stop signs or things like that, they want to find those examples and label them in order to kind of improve the model's performance. Doing effectively an active learning strategy, but they can't go and process all of the data that's being collected by every single Tesla that's in the world. You actually have to be kind of more thoughtful and uh, careful about the processing there, otherwise it would just be computationally too expensive. And similarly, we see this in e-commerce as well if we're trying to actually deal with Integrity problems or take down potentially bad content or bad postings. So for example, when I was at Facebook it became it was at the beginning of the pandemic and it was socially unacceptable or illegal to to sell N95. Mass Because there was a shortage within the country, it became a real problem to actually be able to accurately identify by those N95 masks and take them down. but that only represented a very small percentage of this like massive set of posts that are on Facebook's Marketplace. So when we think about this challenge of having these like massive unlabeled data sets where the thing that we care about only represents a very small fraction of that. now we get into the second bottleneck which is actually just processing all of the unlabeled data. So as an example of just like how computationally this can be, you know when I was at Facebook, we were working with uh, uh, 10 billion images which were kind of roughly collected over like a two-week span and just to like run a single inference pass with a Resnet 50 model over 10 billion images took about close to 40 extra Slots of computation or about 40 GPU months using P100 Gpus which is massive. You know, like no matter the cost of like the labeling for that single example, just the compute cost of loan a loan of processing that amount of data is going to dominate dominate your cost and it's also going to be just way too slow if you have to wait you know, days, weeks, or months in order to get an updated model to select the next example. So during my research, I actually tackled this problem working along researchers at Um University of Madison, Facebook and at Stanford as well. and what we found is that even though you have these kind of massive unlabeled data sets, you actually don't need to look at all of that data in order to process it, especially as we have better representations um, uh, from just Foundation models and things like that and instead what we can do and you'll actually implement this in your in the lab assignment that's associated with this class is. instead we can start the same way. so we'll have like a large pool of unlabeled examples and then we'll have a small subset of labeled examples that we kind of initially use to to train on and train a model. Um, But you know, instead of applying our selection strategy as we kind of described here to all of the kind of unlabeled examples and defining our candidate pool that way, what we can instead do is actually just take um, the uh, the currently labeled examples that we have this kind of initial subset and we assume that there's some kind of initial positive examples that you have for this class that you care about and we can just find the nearest neighbors to that using standard similarity search and we can effectively like take those unlabeled examples that are closest to it and have that as kind of this neighborhood around those examples that we initially search and then this is a much smaller kind of amount of data and we can kind of keep early doing this process, so as we label new examples, we can actually find their nearest Neighbors in the unlabeled pool, update our pool kind of at every step of the way and we keep repeating this process. And the surprising thing is that you know, even though we end up only looking at a very small fraction of the overall data, we find that we can still reach the same amount of accuracy as we would have if we had scanned over all the data. and this is again just kind of due to the power of like the representations that we have in these large Foundation models and things like that and the representational power that we have now we're actually like very similar or like even unseen Concepts like an N95 mask or a fidget spinner are actually like localized to a very small part of the laden space within within this representation. So kind of going back to actually the example that I showed at the very beginning of this lecture. When we think about kind of active learning doing this process on Imagenet as I explained, you know we can actually get to the same level of accuracy while only looking at a very small fraction of the data. So on Imagenet, looking at less than 10 percent of the unlabeled data and still getting to the same accuracy if we had performed Active Learning over all of the data, we see similar results for even larger data sets like open images where there are 6.8 million examples, and we find that you know we can get again very close to the same uh uh, like mean average Precision Well, considering only one percent of the data. and finally, we scaled it all the way up to 10 billion images and we found once again that you know even at this scale, we can still get to the same level of accuracy or mean average Precision while only considering less than one percent of the data, so 0.1 percent of the data. So this fundamentally makes it way more tractable to actually do Active Learning at these scales, which has been kind of a huge bottleneck to this kind of new wave of research. a new wave of kind of just like modern machine learning, and in in the the lab associated with this class, you'll actually get to see the difference firsthand, where you'll see kind of the process of kind of classical Active Learning on open images, taking you minutes in between each selection round versus that going down to seconds and almost actually imperceptible between selection rounds. so you'll actually be able to implement and see the power there, and I'll pause if there's any questions about these practical challenges or considerations within. Active Learning The nearest examples to the ones that you've already labeled yeah, so effectively. what is kind of Happening Here is that the Canada pool is growing over time and you're using the model's uncertainty to direct Geo closer and closer to the decision boundary and this kind of unlabeled space. Great question. Yeah, uh yes, so super great. Great question. Yeah, this actually um uh uh, if you're doing batch Active Learning Um, you can actually still apply the same strategies that you would have for batch active learning transparently to this kind of Seals approach and then still get the best of both worlds so you can prevent yourself from getting too close into the nearest neighbor's landscape or like um uh yeah, too close into the nearest neighbor's kind of landscape and and actually force yourself to kind of go out to like other examples. Also with the initial seed set that you have like if those examples are in like different places and things like that, it will create kind of a four. you'll have like a larger space to choose from so you might like choose one example from like this, kind of like positively label example another one from this negative the neighborhood there so it still can ensure that you have like the diversity enough that you're not always selecting the same example over and over again or like very very close to it and you can tune like the the amount of neighbors and like um that uh the amount of neighbors that you look at to actually increase like how far you can go in a single leap. Yeah, great questions, Anything else? All right sweet. So in the second part of the lecture, I'm going to kind of switch gears and thinking about you know, actually, corset selection and US compressing down these kind of large data sets into something that's more kind of more manageable and this also comes up a lot in practice. So when we think about kind of large label data sets and can come up in situations where we have systematic feedback. so for example, we might have people tagging friends and images, You might also having flagging emails as spam where you're just generating a tremendous amount of label data from people all around the world. Using the system could be rating items or movies on like a website like Netflix And also when we think about self-supervised models when we think about these kind of language models things like bird or in terms of computer vision, some CLR or dyno, these type of models where you actually Define the label for every single unlabeled example, here you have this massive massive amount of data that's basically Unlimited. but this can be actually really expensive to like. Go and process all that kind of naively, especially since a lot of data in the natural world is often very very skewed to a set of kind of small subset of like common cases. So it's corset selection. So corset selection is aims to to select a small subset of the data that accurately approximates the full data set. So when we compare to kind of the process that we had with active learning, it's actually operating in the reverse. So instead of starting with a small amount of label data, we actually have kind of a fully labeled data set or a lot of labeled data. And instead what we do is we can train some smaller model or use some type of heuristic that's very cheap to compute over all this labeled data. To say hey, what are the most kind of Representative or most informative data points for us So you can imagine something like k-means clustering or anything like that to kind of identify these kind of Representative points and then we select that kind of subset of the representative points based off of our strategy there. And then we can train a model on that like final representative subset of data. And the crazy thing is that you know, even though um, this data set is smaller and stuff like that, because data sets are often redundant, we can actually do this process without actually losing that much in terms of accuracy or other metrics that we care about. So as a practical example of this, we can actually like, do this on a data set like Cfr10, which is you know, C410 is actually a relatively small data set and very well well curated. And the fact that it has 10 classes each one of those classes are evenly represented. But even in this setting, where here on the X-axis we have kind of the training time for a single P100 GPU and on the Y-axis we have top one test error. We see that the gray dashed line represents the Precision or the accuracy that we get with, or the error that we get with uh, all of the data. If we were to train kind of a large, very accurate model on all that data, it takes about close to four hours to train that model, but we can actually do this process of course, that selection where we train some smaller fast model maybe that has fewer layers. So here we're training a Resnet 20 model rather than a Resnet One 64 model. It doesn't get to the same accuracy that we get with this kind of larger, more accurate model, but what we can do is we can actually use that model. It only takes 20 minutes or 30 minutes for us to train and we can actually filter out 50 of the data. Um, that is kind of the least valuable based off of our criteria here and then we can train our large model on top of Um on that remaining 50 of the data. And what we find is that you know, even though we're like using a half of the overall data, we don't affect the final accuracy that we get with this large model of Resnet 164. And we actually can end up speeding up the end-to-end training Time by 1.6 X So that's including training the small model, filtering out the data, and then training the large model on that subset. even in this very carefully curated data set of C410 and this is actually kind of been thinking about it like only the tip of the iceberg in terms of like the different methods that we can use in order to like grow or compress data sets. So we talked about active learning as kind of a general process for kind of figuring out what data points to label from some large pool of examples and Corset selection as a way to distill it down. But even within Active Learning there's many different variants of active learning. We talked about very simple binary classification setting here, but there's a bunch of different selection, strategies and things to think about in different domains. There's also the possibility of doing generative Active Learning where rather than having a pool of examples, you know we can actually generate the examples that are the most informative for us to label, especially as we think about kind of recent improvements in generative AI This can actually enable you to save in terms of computational costs and like the size of your unlabeled data um, and speed things up even further. There's also a problem called Active search, which is a sub area of active learning that's particularly used in Drug Discovery where we don't care about model performance at the end of the day, we just want to find as many positive examples as many successful drugs as possible. And as a result of that, there's kind of a whole separate set of like field of study and set of selection strategies ways for framing that problem. There is also a notion of hard example mining where you're using kind of other heuristics or methods in order to say what are like, kind of the most difficult examples or most valuable things to include into your data data set which is commonly used in recommendation and search problems. We also have this whole notion of curriculum learning which is the process of not only selecting which examples to label, but also what order do you train on those examples and show them to the model. So creating a full curriculum of examples to study from and much more. This is a kind of a rapidly growing field that's only becoming more and more important as we think about just the importance of data, creating very well crafted data sets and solving these problems. And I blew through the time in that lecture. but this is like the the immediate material that I prepared so thank you all I Uh, if there's any questions I can also jump back and dive into more detail on things. seems very suitable for unit classification. Yeah, yeah, it's a super great question. so like when you think about like chat GPT or any of these large language models, they also have to think about careful like data set curation. like thinking back to like the original GPT 3 paper if you read it carefully in the way that they structure the data set they they had like a lot of like general internet data but actually that wasn't the most valuable data for them to use. They actually up sampled examples or data points from Wikipedia and things like that that were more carefully curated and refined in terms of the examples. So being able to actually kind of inform like hey, maybe you don't need all of like this web data that's like inherently very redundant maybe you can take a course out of that and instead Focus Kind of more attention on the examples from this kind of curated, more informative data set like Wikipedia. So even in like the case of these large language models, there's actually a lot of work that goes into curating the data set, shaping the data set for it to be valuable. And originally when you think like Chachi or GPT gpt3 before it became chat GP uh chat GPT um also had the problem that you know they weren't Individual examples can also be very bad or provide kind of harmful biases in it. So you can also do active learning strategies and and data selection strategies to kind of mitigate that bias and to make sure that you have kind of Representative examples. So maybe you want to, um, you you find that like there's you're struggling to like have the notion of a doctor, but you can combine it with some type of generative example to be like, hey, maybe it has an associate of a doctor? Being a man, you can actually be like generate a cross or like generate um, uh, examples where the doctor is a woman or of a different kind of ethnicity or background or things like that as well. So does that answer the question? Awesome, Um, sweet. And then thinking about the lab assignment which I believe should be posted on the on the website. What you'll do in that process actually, which I forgot to mention about, was you'll actually go through this process of implementing Active Learning on this one-dimensional line so you can actually see the the speed up firsthand, actually go through this process and then kind of. After doing that is like a nice warm-up Example: go to a really large data set, go to open images, and actually Implement Active Learning there, and deal with just the computational bottlenecks of that even with a very small kind of model where you'll actually Implement seals, and you'll see a speed up of an order of magnitude in terms of the time that it takes to select individual examples. Awesome. Any other questions I can also talk. Let's see, there's a lot of other pieces of kind of interesting areas and active learning and research as far as like different selection criterias, different problems and things like that, this whole computational aspect of it of being able to create these data sets is actually kind of top of Mind as we think about kind of expanding to these large data sets and thinking about just the ever-growing thing of GPT of these like Foundation models large language models and things like that. um but yeah it's a super exciting area of research and uh happy to chat with anyone in like the the kind of 10 minutes or so that we have left in the lecture. so thank you all.</div></div></div></div></div></div><div contenteditable="false" data-content-editable-void="true" style="width: 0px;"><div style="display: none; flex-shrink: 0; pointer-events: none; width: 0px; position: absolute; right: 192px; opacity: 0;"><div style="display: flex; flex-direction: column; padding: 5px 16px; width: 340px; flex-shrink: 0; height: 100%; position: relative; pointer-events: none; z-index: 1;"><div style="position: absolute; pointer-events: none; width: 100%; height: 100%; top: -5px; background: linear-gradient(white 0px, rgba(255, 255, 255, 0) 15px);"></div></div></div></div></main><span style="height: 1px; width: 1px;"></span></div><div class="notion-presence-container" style="position: absolute; top: 0px; left: 0px; z-index: 89;"><div></div></div></div></div></div><div class="notion-peek-renderer" style="position: fixed; top: 0px; right: 0px; bottom: 0px; width: 960px; z-index: 109; transform: translateX(960px) translateZ(0px);"></div></div></div></div></div><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><script src="assets/js/1172e9111a5fb396bcb8a05870b5eabf8abf221c.js" type="text/javascript"></script><div style="width: env(safe-area-inset-bottom);"></div></body></html>