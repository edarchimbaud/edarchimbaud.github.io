<html class="notion-html"><head lang="en"><meta charset="utf-8"/><meta content="width=device-width,height=device-height,initial-scale=1,maximum-scale=1,user-scalable=no,viewport-fit=cover" name="viewport"/><title>4. Label Errors and QA</title><meta content="en_US" property="og:locale"/><link href="data:image/png;base64,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" rel="shortcut icon" type="image/x-icon"/><link href="/images/logo-ios.png" rel="apple-touch-icon"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="telephone=no" name="format-detection"/><meta content="no" name="msapplication-tap-highlight"/><link href="assets/css/e218a1aef6df86309cb95b61e446888efa3b0379.css" media="print" rel="stylesheet"/><link href="assets/css/e7df85b6a5b33bc52629df6d3bd37d197c7a873d.css" rel="stylesheet"/><meta content="Edouard d'Archimbaud" name="title"/><meta content="Edouard d'Archimbaud official website" name="description"/><link href="assets/css/e1d809d762eeca23edf0cb31bb17bf3c703085f5.css" rel="stylesheet"/><style type="text/css"></style></head><body class="notion-body"><style>body{background:#fff}body.dark{background:#191919}@keyframes startup-shimmer-animation{0%{transform:translateX(-100%) translateZ(0)}100%{transform:translateX(100%) translateZ(0)}}@keyframes startup-shimmer-fade-in{0%{opacity:0}100%{opacity:1}}@keyframes startup-spinner-rotate{0%{transform:rotate(0) translateZ(0)}100%{transform:rotate(360deg) translateZ(0)}}#initial-loading-spinner{position:fixed;height:100vh;width:100vw;z-index:-1;display:none;align-items:center;justify-content:center;opacity:.5}#initial-loading-spinner svg{height:24px;width:24px;animation:startup-spinner-rotate 1s linear infinite;transform-origin:center center;pointer-events:none}#skeleton{background:#fff;position:fixed;height:100vh;width:100vw;z-index:-1;display:none;overflow:hidden}#initial-loading-spinner.show,#skeleton.show{display:flex}body.dark #skeleton{background:#191919}.notion-front-page #skeleton,.notion-mobile #skeleton{display:none}#skeleton-sidebar{background-color:#fbfbfa;box-shadow:inset -1px 0 0 0 rgba(0,0,0,.025);display:flex;width:240px;flex-direction:column;padding:12px 14px;overflow:hidden}body.dark #skeleton-sidebar{background-color:#202020;box-shadow:inset -1px 0 0 0 rgba(255,255,255,.05)}#skeleton.isElectron #skeleton-sidebar{padding-top:46px}#skeleton .row{display:flex;margin-bottom:8px;align-items:center}#skeleton .row.fadein{animation:1s ease-in 0s 1 normal both running startup-shimmer-fade-in}#skeleton .chevron{width:12px;height:12px;display:block;margin-right:4px;fill:rgba(227,226,224,.5)}body.dark #skeleton .chevron{fill:#2f2f2f}.startup-shimmer{background:rgba(227,226,224,.5);overflow:hidden;position:relative}body.dark .startup-shimmer{background:#2f2f2f}.startup-shimmer::before{content:"";position:absolute;height:100%;width:100%;z-index:1;animation:1s linear infinite startup-shimmer-animation;background:linear-gradient(90deg,transparent 0,rgba(255,255,255,.4) 50%,transparent 100%)}body.dark .startup-shimmer::before{background:linear-gradient(90deg,transparent 0,rgba(86,86,86,.4) 50%,transparent 100%)}#skeleton .icon{width:20px;height:20px;border-radius:4px}#skeleton .text{height:10px;border-radius:10px}#skeleton .draggable{-webkit-app-region:drag;position:absolute;top:0;left:0;width:100%;height:36px;display:none}#skeleton.isElectron .draggable{display:block}</style><style id="scroll-properties"></style><div id="notion-app"><div class="notion-app-inner notion-light-theme" style='color: rgb(55, 53, 47); fill: currentcolor; line-height: 1.5; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; -webkit-font-smoothing: auto; background-color: white;'><div style="height: 100%;"><div class="notion-cursor-listener" style="width: 100vw; height: 100%; position: relative; display: flex; flex: 1 1 0%; background: white; cursor: text;"><div class="" style="display: flex; flex-direction: column; width: 100%; overflow: hidden;"><div style="max-width: 100vw; z-index: 100; background: white; user-select: none;"><div class="notion-topbar" style="width: 100%; max-width: 100vw; height: 45px; opacity: 1; transition: opacity 700ms ease 0s, color 700ms ease 0s; position: relative;"><div style="display: flex; justify-content: space-between; align-items: center; overflow: hidden; height: 45px; padding-left: 12px; padding-right: 10px;"><div class="notranslate" style="display: flex; align-items: center; line-height: 1.2; font-size: 14px; height: 100%; flex-grow: 0; margin-right: 8px; min-width: 0px;"><div class="notion-selectable notion-page-block" data-block-id="d8397a78-1321-456c-9c10-1feea7a42b60" style="display: flex; align-items: center; min-width: 0px;"><a href="edouard-d-archimbaud.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üë®‚Äçüíª" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üë®‚Äçüíª</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Edouard d‚ÄôArchimbaud</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div class="notion-selectable notion-page-block" data-block-id="bb024d08-b3b1-4dbd-bafc-0fa8d6a316c2" style="display: flex; align-items: center; min-width: 0px;"><a href="data-centric-ai.html" rel="noopener noreferrer" style="display: flex; text-decoration: none; user-select: none; cursor: pointer; color: inherit; min-width: 0px;"><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: inherit; line-height: 1.2; min-width: 0px; padding: 2px; color: rgb(55, 53, 47);" tabindex="0"><div style="display: flex; align-items: center; min-width: 0px;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px; font-weight: 500;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéØ" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéØ</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 160px;">Data-centric AI</div></div></div></a></div><span style="margin-left: 2px; margin-right: 2px; color: rgba(55, 53, 47, 0.5);">/</span><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 1; white-space: nowrap; height: 24px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 6px; padding-right: 6px; color: rgb(55, 53, 47);" tabindex="0"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px; border-radius: 0.25em; flex-shrink: 0; margin-right: 6px;"><div style="display: flex; align-items: center; justify-content: center; height: 20px; width: 20px;"><div style="height: 18px; width: 18px; font-size: 18px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; max-width: 240px;">4. Label Errors and QA</div></div></div><div style="flex-grow: 1; flex-shrink: 1;"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="searchNew" style="width: 14px; height: 14px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg>Search</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0">Duplicate</div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: flex; align-items: center; justify-content: center; width: 32px; height: 28px; border-radius: 3px;" tabindex="0"><svg class="dots" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden;" viewBox="0 0 13 3"><g><path d="M3,1.5A1.5,1.5,0,1,1,1.5,0,1.5,1.5,0,0,1,3,1.5Z"></path><path d="M8,1.5A1.5,1.5,0,1,1,6.5,0,1.5,1.5,0,0,1,8,1.5Z"></path><path d="M13,1.5A1.5,1.5,0,1,1,11.5,0,1.5,1.5,0,0,1,13,1.5Z"></path></g></svg></div><div style="flex: 0 0 auto; width: 1px; height: 16px; margin-left: 8px; margin-right: 8px; background: rgba(55, 53, 47, 0.16);"></div><div role="button" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: inline-flex; align-items: center; flex-shrink: 0; white-space: nowrap; height: 28px; border-radius: 4px; font-size: 14px; line-height: 1.2; min-width: 0px; padding-left: 8px; padding-right: 8px; color: rgb(55, 53, 47);" tabindex="0"><svg class="notionLogo" style="width: 18px; height: 18px; display: block; fill: inherit; flex-shrink: 0; backface-visibility: hidden; margin-right: 6px;" viewBox="0 0 120 126"><path d="M 20.6927 21.9315C 24.5836 25.0924 26.0432 24.8512 33.3492 24.3638L 102.228 20.2279C 103.689 20.2279 102.474 18.7705 101.987 18.5283L 90.5477 10.2586C 88.3558 8.55699 85.4356 6.60818 79.8387 7.09563L 13.1433 11.9602C 10.711 12.2014 10.2251 13.4175 11.1939 14.3924L 20.6927 21.9315ZM 24.8281 37.9835L 24.8281 110.456C 24.8281 114.351 26.7745 115.808 31.1553 115.567L 106.853 111.187C 111.236 110.946 111.724 108.267 111.724 105.103L 111.724 33.1169C 111.724 29.958 110.509 28.2544 107.826 28.4976L 28.721 33.1169C 25.8018 33.3622 24.8281 34.8225 24.8281 37.9835ZM 99.5567 41.8711C 100.042 44.0622 99.5567 46.2512 97.3618 46.4974L 93.7143 47.2241L 93.7143 100.728C 90.5477 102.43 87.6275 103.403 85.1942 103.403C 81.2983 103.403 80.3226 102.186 77.4044 98.54L 53.5471 61.087L 53.5471 97.3239L 61.0964 99.0275C 61.0964 99.0275 61.0964 103.403 55.0057 103.403L 38.2148 104.377C 37.727 103.403 38.2148 100.973 39.9179 100.486L 44.2996 99.2717L 44.2996 51.36L 38.2158 50.8725C 37.728 48.6815 38.9431 45.5225 42.3532 45.2773L 60.3661 44.0631L 85.1942 82.0036L 85.1942 48.4402L 78.864 47.7136C 78.3781 45.0351 80.3226 43.0902 82.7569 42.849L 99.5567 41.8711ZM 7.5434 5.39404L 76.9175 0.285276C 85.4366 -0.445402 87.6285 0.0440428 92.983 3.93368L 115.128 19.4982C 118.782 22.1747 120 22.9034 120 25.8211L 120 111.187C 120 116.537 118.051 119.701 111.237 120.185L 30.6734 125.05C 25.5584 125.294 23.124 124.565 20.4453 121.158L 4.13735 99.9994C 1.21516 96.1048 0 93.191 0 89.7819L 0 13.903C 0 9.5279 1.94945 5.8785 7.5434 5.39404Z"></path></svg>Try Notion</div></div></div><div style="width: calc(100% - 0px); user-select: none;"></div></div><div class="notion-frame" style="flex-grow: 0; flex-shrink: 1; display: flex; flex-direction: column; background: white; z-index: 1; height: calc(100vh - 45px); max-height: 100%; position: relative; width: 1920px;"><div class="notion-scroller vertical" style="display: flex; flex-direction: column; z-index: 1; flex-grow: 1; position: relative; align-items: center; margin-right: 0px; margin-bottom: 0px; overflow: hidden auto;"><div style="position: absolute; top: 0px; left: 0px;"><div></div></div><div class="whenContentEditable" data-content-editable-root="true" style="caret-color: rgb(55, 53, 47); width: 100%; display: flex; flex-direction: column; position: relative; align-items: center; flex-grow: 1; --whenContentEditable--WebkitUserModify:read-write-plaintext-only;"><span style="height: 1px; width: 1px;"></span><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0; z-index: 2;"></div><div style="width: 100%; display: flex; justify-content: center; z-index: 3; flex-shrink: 0;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div style="max-width: 100%; padding-left: calc(96px + env(safe-area-inset-left)); width: 100%;"><div class="pseudoSelection" contenteditable="false" data-content-editable-void="true" style="user-select: none; --pseudoSelection--background:transparent; pointer-events: none;"><div class="notion-record-icon notranslate" style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px; border-radius: 0.25em; flex-shrink: 0; position: relative; z-index: 1; margin-left: 3px; margin-bottom: 0px; margin-top: 96px; pointer-events: auto;"><div style="display: flex; align-items: center; justify-content: center; height: 78px; width: 78px;"><div style="height: 78px; width: 78px; font-size: 78px; line-height: 1; margin-left: 0px; color: black;"><span aria-label="üéì" role="img" style='font-family: "Apple Color Emoji", "Segoe UI Emoji", NotoColorEmoji, "Noto Color Emoji", "Segoe UI Symbol", "Android Emoji", EmojiSymbols; line-height: 1em; white-space: nowrap;'>üéì</span></div></div></div><div class="notion-page-controls" style='display: flex; justify-content: flex-start; flex-wrap: wrap; margin-top: 8px; margin-bottom: 4px; margin-left: -1px; color: rgba(55, 53, 47, 0.5); font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; height: 24px; pointer-events: auto;'></div></div><div style="padding-right: calc(96px + env(safe-area-inset-right));"><div><div class="notion-selectable notion-page-block" data-block-id="cdb30631-fd6a-47a5-ae58-24d81509b303" style='color: rgb(55, 53, 47); font-weight: 700; line-height: 1.2; font-size: 40px; font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; cursor: text; display: flex; align-items: center;'><div contenteditable="false" data-content-editable-leaf="true" placeholder="Untitled" spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">4. Label Errors and QA</div></div><div style="margin-left: 4px;"></div></div></div></div></div><div style="width: 100%; display: flex; flex-direction: column; align-items: center; flex-shrink: 0; flex-grow: 0;"><div contenteditable="false" data-content-editable-void="true" style="padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right)); max-width: 100%; width: 100%;"></div></div></div></div><main style="display: flex; width: 100%; justify-content: center; padding-top: 5px;"><div style="max-width: 100%; min-width: 0px; width: 900px;"><div class="notion-page-content" style="flex-shrink: 0; flex-grow: 1; max-width: 100%; display: flex; align-items: flex-start; flex-direction: column; font-size: 16px; line-height: 1.5; width: 100%; z-index: 4; padding-bottom: 30vh; padding-left: calc(96px + env(safe-area-inset-left)); padding-right: calc(96px + env(safe-area-inset-right));"><div class="notion-selectable notion-text-block" data-block-id="1894739e-446d-4ab5-baba-18e79dbdd983" style="width: 100%; max-width: 1728px; margin-top: 2px; margin-bottom: 0px;"><div style="color: inherit; fill: inherit;"><div style="display: flex;"><div contenteditable="false" data-content-editable-leaf="true" placeholder=" " spellcheck="true" style="max-width: 100%; width: 100%; white-space: pre-wrap; word-break: break-word; caret-color: rgb(55, 53, 47); padding: 3px 2px;">All right. Uh, so let's get started. Thanks everybody who's back from yesterday and uh, welcome new folks! Um, so this is the first like real technical lecture. This will be more technical than last lecture. Uh, please ask questions. It makes it more fun for me and more fun for you. Um, it's going to be pretty jam-packed This one will be faster paced than last time, so just ask me questions. Uh, some of the stuff that I'm presenting I did my PhD on and so questions are great because I've probably thought about this stuff too much. So uh, just feel free to be as interactive as you like. Um, all right, let's jump in. So let's look at some label errors. These are all from the <a class="notion-link-token notion-enable-hover" data-token-index="1" href="http://labelhairs.com/" rel="noopener noreferrer" style="cursor:pointer;color:inherit;word-wrap:break-word;text-decoration:inherit" target="_blank"><span class="link-annotation-1894739e-446d-4ab5-baba-18e79dbdd983-259396461" style="border-bottom:0.05em solid;border-color:rgba(55,53,47,0.4);opacity:0.7">labelhairs.com</span></a> website that we looked at uh yesterday and on the top we've got uh, like. across the rows we have different types of errors and then across the columns we have different data sets so we can see like Mnist on the left and those are three errors from Mnist and these are all labeled. issues and label issues can take many forms and so I just wanted to start out with an image to get us thinking and we can focus on the bottom row. These are what are non-agreement and so for each of these we had a given label and then we had another label that we got from like Mechanical Turk where we showed them what we thought was the actual write label and then the label that was given in the data set and then they would choose either both neither or the corrected one. Or they would say the original was right and so at the bottom. This is where the that. We had many people five for each example and they just couldn't agree. And so what I want to show here is that there's different types of label errors and in this case these are just like really hard examples like this: Uh, is that like a four or a nine? You know it's just hard. It's tough to say um, is this an automobile or an airplane? Um, and this is like a it's kind of a yo-yo Maybe. but it's probably frisbee. It's just hard to say. and so, um, it's probably a frisbee, right? but could be a yo-yo and it just they're just tricky, right? Is this a bandage or a roller coaster? And so there are different kinds of examples. And uh, when we're presenting methods, it doesn't mean that this method will work for every single data point ever like. There's definitely real world data is tricky, but what we want to do is create robust methods that can detect a lot of situations. Um, another example is like when you have something that's out of distribution. So for example, that's an L but this is a digits data set so it just doesn't fit. It could be a one, but it's just unclear and just examples. Those are like two people, but the two options were rows and apple and so you can see. That's another issue. We have a label, but it just doesn't. The actual distribution of the data doesn't belong to the class, so no label will work. These are examples where you have actually two labels that were in the data set, but they are both in the same image and it's a single labeled data set. So for example, a hamster in a cup, which is very cute. But the class hamster and also the class cup are both in this data set. But it's a single label data set. So supposedly there's only one label per image. so what do you do? and then finally, uh, correctable. So this is where there's only one clear label in the data set as far as we can tell. and then there's a given label that we think is wrong. And so this is sort of the simplest and clearest case, and this is one that we're going to focus on. How we can detect these automatically in this lecture, so this will be sort of the focus. Okay, so in this lecture, we're going to cover a bunch of stuff. We'll learn about label issues. We just saw a bunch of types, but we'll learn about more kinds noise processes. like how do labels become noisy, how do they become erroneous And then we can sort of backtrack that process to reverse engineer what we think is the correct data set. How to find label errors and label issues. Some mathematical intuition for why this is happening. and if you have a that sheet and how to rank data. So like if you order all the data in a data set by the probability or some likelihood of it being a label error, how would you do that? How to estimate how many issues there are in a data set thank you, and how to train a model on on data that has noisy labels. And then at the end we'll look at what are the implications for machine learning benchmarks when you have test data that has errors in it. So our overall goal, though, is just like improved machine learning models when you have data that has labeled layers. This is a very relevant goal because in the real world most data actually has labelers. All right. So um, what's uh, yeah. So what is, Um, this is. This is a poorly written title. so what what is sort of not? Why can you not find label Errors By Sorting by loss. So uh, let me just actually edit the title because it's too confusing. Basically, uh, let's just look. finding label Errors By Sorting by loss. So um, yeah, so can. Has anybody thought about this before? So you have a bunch of data, you're training on that data and then you have a loss function. If it's if your model has a loss function and most do, and now you have a loss for every single example. So this is sort of what was the predicted probability of whatever label it's predicting for that example or the given label and that usually is the loss function for like cross entropy for example. Um, so you could sort every data point by that amount and then you could look at which has the highest loss. And does anybody have any idea why this is be tricky. So if you have a data set to be able to sort of know what are the errors? Yeah, going to use that data, Simply trying to models with them all might get biased in a certain way that they can give loss functions to things that are actually correct. Correct, Yeah, So what can happen is like if you don't regularize your model well and you're training in Sample so you're like it's seen that data many times. Uh, like one form of bias is just like overfitting to that data and so you might actually have zero loss like a model can get to zero loss so it would look like nothing is a label error. And we've seen for example, that like you can train a model to fit on totally random labels and totally random data and like get zero loss so that definitely can be an issue in practice. Actually, it turns out you can. You can sometimes get away with doing this, but it just depends on the model and it can definitely be a problem. Say you had a data set of like a million things and you want to find all the the label errors in that data set and now you've sorted it by loss. Like how do you actually know where the cutoff is? like how far down do you go and then that's where the label errors end. So that's the problem. So that's why this is like a tricky thing. So what we need to know is we need to know how much air there is in a data set. We should be able to characterize the error in a data set and then use that information to be able to use things like sorting for example to say okay, I think this is like the top label errors and this is how many I think there are and then the combination of those two things can help you find label issues in a data set. So does that make sense? from like the setup problem. Okay, cool. Um, so this will be sort of our road map and so we'll talk about confident learning, which is a framework to do these sorts of things and it's got a bunch of theory and algorithms. I'll try to go pretty light on the theory I'll focus on one of the sort of easy to explain algorithms. um, and I think it'll be fun. So uh, most of the stuff I mentioned can be handled with confident learning. Yeah, find the label errors by using the loss, but the true? the proof ground. or I think that you are sure or correct. they might be wrong as well. Like you have your prediction and you have the labels that you want to compare to get your loss function. But what if that's false to begin with? What if you have errors in your entire set, in the training and in the testing sense? Yeah, we'll actually look at that at the end of the lecture. The whole intellectual focus on that question. if you have air in the test set and then in the training set. the whole idea is if your model is learning something, it should be predicting a label that's very different from the given label. And so that's why the loss would end up big if you had like a well-trained model. Yeah, we'll look at it, it'll be good. Uh I think this will become clear in about 10 slides. Um, okay so this is some of the stuff you can do with confident learning. You can find label errors in a data set, rank data by issue a likelihood of being a label issue learning with noisy labels, and then we'll really focus on the fourth one, which is like completely characterizing the label noise in a data set, and we're focused on classification. Uh, with single labeled data. It's also useful for data curation, but we're doing that in the next lecture and so I'll chat for about five minutes at the start of that and then I'll pass that lecture off to Jonas. The key idea is that with confident learning, you can use any models predicted probabilities and you're going to see how that's going to come up. So this is a good thing because that means in the future when we invent the next thing you know after Transformers you want to still be able to use that. And if you build something that only works for, say, you know Lstms and then Transformers comes out. That's kind of a bummer because you know models keep changing and so you want things that will work for sort of any model. And we call that model agnostic. And ideally they should work for as many data sets as possible. So they can also be data agnostic. And that's what the idea behind conflict learning is. It's a framework that will work with both any model and then most data sets. All right. So some quick notation. Why Tilde is the observed noisy label. So anytime you see that that's like a noisy label, that's what's given. And this, if you just have a data set, then your labels are always Whitetail day because you don't know what the ground truth is. You just have like millions of labels or thousands of labels and then y Star is your true label. So this is like the magical unknown ground truth. If you had millions of data points, you would love to have voice star, but like you usually don't. Um, sometimes there is no true Y star, but we'll assume that there is. Um, this is the set of examples that are noisily labeled I and the true label is J. Okay, so X is like your data set and then if there are things that are like mislabeled as cow, but they're actually pictures of foxes, then the white tilde would be cow and the Y star would be Fox. This would be the set of all those images in your data set that are noisily labeled cow but are actually images of fox. and then C is just the counts the number of those things. So this will build a matrix. If you have 10 classes, 10 different possible labels, it would be a 10 by 10 matrix of all the counts where you have like a wrong label and then what's the true label and then can anyone tell me like what's on the diagonals if I equals J Yeah, yeah, those would be for images because they're just easy to think about. For images that would be images where the label is correct, that would be the diagonal. Okay, and then this is just the joint probability of these two things co-occurring So if you were to take all those counts and you were to have your Matrix and sum them all up and divide by them, you would get sort of the joint probability and that's that's what p is. Okay, um, yeah. and then the last thing is that this is the probability of flipping. So if you have some image of a fox, you want to know what's the probability of it being mislabeled as a cow or you have some text that's like positive. What's the probability of it being mislabeled as negative, then this is that flipping rate. Are we good on notation? Sweet. Okay, um. all right. So there's a bunch of, uh, different types of noisy labels. Where do they come from? So they can be just like a wrong click like you're on Facebook You hit upvote or down vote. You know you hit the wrong button or you're on Amazon and you accidentally like Mark the wrong Stars. It can also be mistakes. So you're when you're labeling something you just like. make a mistake or you pay some labelers and they make mistakes and they're working quickly. Mismeasurement: You're like out reading you know the geiger counter and like you're a little bit off and so you put it in the wrong class. Um, it can be incompetence. like you actually just don't know the task and it's going to totally happen Like if I went, if I was required to like do Radiology reports that would be very incompetent and I would totally screw them up and give bad labels. Sometimes people will have two Ml models one Ml model will use to generate a guess at a label. They'll use that to label data and then they'll use some other model with confident learning to be able to find label issues. But that problem is that first model like might make a lot of label errors in its predictions. Um and then you can also just like corruption or data gets corrupted or things get changed and then errors get introduced. All of these result in some of the labels being flipped to some other label and so that would be just like dog is mislabeled as fox or you could have some tweet that's like hi welcome to the team and it was originally labeled like friendly language but it gets mislabeled as toxic language and from this you build up this Matrix of counts and this this Matrix will tell you sort of all the number of times that one true label got flipped to some noise label. And this is a thing that we want to estimate because if we can estimate this, then we know sort of how many labels there are in the entire data set, how many label issues there are in the entire data set, and that would be the off diagonals. We know how many things are labeled correctly, and in terms of sort of flipping rates, we've fully characterized the label noise in a data set. So once you have this thing, you can do a lot with it and we'll see what you can do. You can find label issues, train with label noise, and so forth. So this is like something really good to have and we'll see how we can use it a bunch. Okay, so there are different types of label noise out there. we're going to focus on the most General case. but I Just want to show you there are different. You can have different types of noise. So this is an example of a of A of the same Matrix right as this previous one. It's been normalized so it's been normalized by column in this case, all the columns 71.. So it's just showing the flipping rates and what it's saying is the zero class is always correct 60 of the time, but it's flipped to every other class 0.1 percent of the time. And so what this is saying is that there's sort of a 0.1 percent chance of any class being flipped to any other class. Um, or just a 10 chance of any class being flipped any other class. um Can Can anyone tell me why this? This is sort of. uh, say you had a million a million images and that rate was like a little bit smaller like it was. Uh, you know you have a thousand classes and it's like ten percent. uh. flip? You know, error rate does any? Can anybody tell me why that ends up being actually pretty easy to solve Or just general ideas of what you think maybe have income uniform? Random flipping. Um, may not be what happens in the real world of yeah, yeah, it was a really good answer. It's better. I'm glad I asked you. Um, it was a really good answer. Yeah, so like the probability of uh, you know a seat being mislabeled as a couch is like pretty high and the probability of like a fox being mislabeled as a dog is pretty high. but the probability of a banana being mislabeled as like a water bottle or a laptop is like lower. and so it's not. It's not uniform. Um, uniform is actually pretty easy because if you have say a thousand classes you can have for each class, you could have 999 errors and that would mean that you just had one error. One flipping. One example was flipped to all all the other classes so all you have to do is just find the one and you have like all the other data to be able to find that one that's perfectly labeled. Um, so that's why it's not so hard because you're spreading out the noise uniformly. But if you have all the noise in just a few places, it's very difficult for a model to learn because you're just really confusing the model in a few cases. Okay, so what we'll focus on in this lecture is the general case where that Matrix can be anything. um, and that's much more real world. And then there's also instant dependent noise. And that's where it doesn't actually depend on just the class. But like every single example has its own noise distribution and that requires a ton of assumptions that usually don't happen in the real world to make those assumptions true. So we're going to ignore it in this lecture. Um, cool. The last sort of thing to learn about before we get really going into methods is what is uncertainty. So uncertainty is the opposite of confidence. actually show of hands. Who's heard of uncertainty? Okay, so so pretty mixed. So who's sort of confidence of a model like in a model's prediction? Okay, all right the confidence would be just like the how high is the predicted probability for whatever it's trying to predict and then the uncertainty is how high is the or like how low basically is the probability for whatever it thinks the label is. And so you can think of uncertainty as the opposite of confidence. and it's the lack of confidence in a model's prediction about some data point. And it depends on sort of two things that are defined in terms of two different types of uncertainty. One is how difficult the example is. So if you have a really hard example that's just very hard to get right, then a model probably won't be very certain about his prediction. And that's called aliatoric noise. And there can be many ways that happens. It can have a bad label, it can be a weird data point and then also a model can have. It can be difficult for a model to actually understand the example because either it hasn't been trained on enough data or it's just never seen an example like that before. or it's under fit and that's called epistemic uncertainty. Okay, so we'll see in a second. why why I'm talking about these two things. and just to really make it clear, Aliatoric uncertainty is like label noise in the case of machine learning with noisy labels and epistemic uncertainty is model noise. Uh, in the in the case of machine learning, uh with label noisy labels and so you can remember it like the M and the M and the L and the L. Okay, so we'll see why that matters on this slide. So here's your predicted probabilities: okay and is everybody familiar with this type of notation? So we've got the probability of uh of the labels and the only labels you have are the noisy ones. That's the probability that the noisy label is some class I given your data X and some model. Okay, and what this this notion expresses is both the noisy model output. So these are predictions. Just to be really clear. like say you have your dog, fox cow, you have some image and you know it's got like I don't know what this is I'm not a great artist Um here wolf. um I should do the ears down. So then you have a model right and it's going to Output some some distribution over the three classes and say it's like 0.8 0.1 and 0.1 and so this right Here this output would be your predicted probabilities for this example which is called X and this would be sort of your breakage probabilities. Your predicting probability of Y equals dog given X and some model would be this guy right here. 0.8 Is that clear? Okay, sweet. so that that equation is expressing your just your model's prediction. Okay, and so if your model is really uncertain, it's going to be like a very low probability because it's just not very certain. But also X has a true label and this is the probability of the noisy label. So it's also the probability of some noisy label given some data which actually has some true label. and so it also encompasses the probability that it has a label error in the same exact equation. Um, it's like deep down X has it's given x and x does have some true label and so this also is expressing what's the probability of the noisy label. You Observe given some true label, so the flip, the noisy flipping rate, and the remote. so that's the noisy flipping rate, is your elliotoric uncertainty and your model's uncertainty is also in the same equation. So we have to somehow get those away. And the way we'll do that is, we'll remove X All right. So we have to disambiguate the two and the way we'll do that is we'll make this assumption and this assumption is that X is uh, X is going to be basically once you know the true label, then you don't need to know the data. So so can anyone tell me like a pro or a con of this sort of assumption I Want to make sure we understand it? Okay, like yeah, what's up? Oh yeah, so we're just looking at this. We're making an assumption here. I Just want to make sure we understand the Assumption So on the left, that's the same thing as like a model output. Uh, that y star has been added in there. but every data does have some true label and so I've just added that into the equation. If that part is confusing, that's okay. no worries. But I Just want to be really clear about conditional. Independence So what we're saying is if you knew the true label, so given the true label, there is some constant flipping rate to every other class and it doesn't matter what X is and that's an assumption and I'll show in a second why it's reasonable to make that. But basically if you have images and they are layer labeled, the true label of that image is Fox then we're saying there's a constant probability that that fox is labeled like dog and it doesn't matter what the fox looks like. That's what that equation is saying. and that's an assumption. and I'll show you in a second Why? That's pretty reasonable. Okay, I'm going to move ahead because there's really cool stuff and I'm worried that this is too detailed. All right. So here's this is actual real data. This is from Imagenet and you can see some real world images of bores that are mislabeled as pigs and you can also see that for example, there are no like missiles or keyboards that are mislabeled as pigs. And so what you'll notice is that regardless of the data, regardless of the pig, it's very commonly mislabeled as bore. And if you look at a data set at scale, you'll see that you can have all sorts of different figs. But like in general, pigs get mislabeled as bore like a certain amount of times. and so you'll observe this in real data and it's the same idea of like a banana is mislabeled as a yellow cylinder more often than you know, a chair. It's the same idea. and so this is something realistic that we observe in real world data. And so that's the model that we're taking. that's the Assumption And that's why we have this Matrix that just has you know the noisy labels and the true labels. just the two the two things. Okay, so just in the real world we observe this class conditional noise. and this is called class conditional because it just depends on the class, not the data. Yeah, um yeah, you can have adversarial examples that are like just intentionally really hard to guess. Totally the way you would do. that is if you you add all your noise in the space so say you have, you add a lot of examples that look like dogs and foxes so like right on the boundary of two classes and then you can flip the labels of all of them to one of the classes and you'll make the distribution shift outward. Um, okay, so what does. If so this is class conditional noise. This is what we're observing in the real world and then what uniform noise would look like is just like the pig would be mislabeled as a whole bunch of things. So you'd have sort of the label Pig But the true label is like all sorts of stuff. but this doesn't really happen in the real world. You would not expect this. so this is a fictitious example just to show you what's not normal. Okay, all right so the question is, does any of this stuff matter? So I've been doing a lot of background because I want to make sure that we like are covering what this stuff is. Uh, but I want to like really quickly? Just there's a bunch of work that shows that with enough data and learning, then you can just use deep learning and like solve any problem regardless of label noise and you'll notice that there are many papers that make this claim like no matter the amount of label noise, like deep learning sort of solves everything. But a key thing you'll notice across these papers and you'll notice that the dates are older because this is recently sort of been brought to light is that most of them focus on uniformly random noise, which we've already seen is a pretty easy problem to solve. And when you have this class conditional noise which is much more real world which is what we'll solve in this lecture. Uh, this doesn't turn out to be the case and so yes, you do have to take this stuff into account and finding label issues will help for most settings. Okay, we won't focus on noise and data. We also won't focus on when you have multiple annotations. those will be in future lectures and there are a bunch of methods. So these are model Centric methods for learning with noisy labels. You can change the loss so you can like modify the loss. You can use one neural network to help you find errors in another neural network. You can do a whole bunch of stuff. We've seen that in benchmarks these don't perform as well as data Centric methods. So we're going to focus on data Centric methods which change the data by finding label errors and then learning on clean data. Okay, so that'll be the focus of this lecture and you can check out those papers on your own time. Okay, so we're ready to actually go into how it works. Thanks for uh, you know, hanging in there with the background, but we're like in a good position now to actually understand how these algorithms work. Okay, so you've got this: Matrix This is our gold truth that we want to estimate. This one's been normalized. So these aren't the counts. This is the probability of these things occurring in a joint distribution. Okay, so 0.14 would mean that in a total data set, 14 of the of the data is like mislabeled Fox but actually should have been dog. Okay, and if you have this 0.03 and you have 100 data points, that would mean three examples are mislabeled cow. but they're actually Fox. So are we good with this thing? Because we're going to estimate this right now. Yeah, yeah. and I think you'll then, well, the next slide See what you think? Let me know if the next, like few slides don't make that really clear. Um, so we're going to run an algorithm to estimate this Matrix And once you have this matrix, it's important to know so you might be like Okay, this lecture had a lot of detail and there's a lot of like things going on. Why do I care? Once you have this thing, you can solve everything so you can normalize by The Columns And now you have the flipping rate the probability of any single example in any data set being mislabeled as any other class like you just divide by the columns. It's a joint distribution, so you can always compute marginals and be able to estimate anything. So this fully characterizes class conditional noise. That's that's why we want to solve for it. Okay, so here's the method. You're going to need two things. You need predicted probabilities. those can come from any model and noisy labels. And the first thing we're going to do is we're going to find class thresholds. Okay, so this is sort of one of three or four equations in the lecture and all this is saying. It's pretty simple is just for some class, some noisy label J We're going to look at all the examples that are in that class. So if it's dogs, we'll look at all the dog images and we'll see what was the predicted probability of that dog image from your model and we do it in an out of sample way. meaning you would use cross validation if you're familiar or you would train on some other data set and you predict on this data set where you train on part of it and then predict on this. But you get this predicted probability of an image and you say what is the predicted, what is the average for all of your dogs of the predicted probability of it being dog is that cool. This is just an average and these are all the pretty probabilities for all of the class J and we're just going to average them all. So just to make sure we're all understanding, say that for the class dog that this would this was one. What would that mean if like T sub dog J equals dog equaled one? Yeah yeah, and what does that mean in the model about the model and images that are dogs? Yeah, totally everything that was labeled dog it thinks is a dog with 100 confidence. Yeah, and if it was and then we do. We do this for every single class. So we'll just compute these thresholds for every class. and that gives us an idea of how confident the model is in each class. And that's going to be really useful because if the model is not very confident in the class, but then there's some image that with has a really high probability of being that class then relative to the model's confidence with respect to that class, it's super confident and it's probably labeled that label and that's how we're going to find what the true labels are, even if the label that's given is another label. So this is why we need this thing. All right. So here is a data set. We're going to find all the label errors, and we're also going to estimate that joint distribution. And we're going to do all this in one method. That's pretty simple, and we're going to do that right now. Okay, so each of these images has been. We've taken some classifier we've used. cross validation. If you're not familiar with cross-validation then just imagine this is a test set and you've trained on some train set, and you're trying to find errors in the test set, and we've just all we've done is taken a trained model. We've taken each of these, run it through the train model, and spit out those predicted probabilities. and those are displayed here. So we've got our predicted probabilities, we've got data, and we've got labels, and now we're going to start finding which of these data are likely mislabeled. Okay, and there's our class thresholds. Those have been computed by averaging these predictive probabilities, so are we good with the setup? Okay, so here's sort of the simplified version of the equation. This is the second main equation in the talk. so this is predicted probability of class J and if it's greater than the threshold for that class, then we're going to label this thing the Y Star as class J. But the true the or the given label for the example was I. So I'll just say that one more time. you've got an example. It's labeled I but it has a really big probability of being J bigger than sort of the average confidence of all the other examples in that class for of class J. So it seems like a lot like it's class J. But the label given is I and we're going to count that. We're going to have a set of all the examples that are like that. We're going to count those up. Are you guys with me? Okay, so once we're as we're counting these things, we're going to build that Matrix I showed you. and we're going to start it with zeros. We want to work our way through the data set. So this is our first example. and the class here is uh, what's given is dog. so I is dog and the predictive probability for dog is. So in this case, where J is where dog and I is also dog. and we're gonna see that point three is not greater than the class threshold for dog, which is 0.7 So we're not going to count this. We're just going to keep looking at the next class. So the next class uh, the next sort of prediction is fall. You know in the soft Max The next class is Fox. The given label is dog is Fox greater than or equal to uh to um, the class threshold for Fox. The answer is yes And so now we count this in our Matrix as being white y Star Fox and the noisy label dog. Okay, and then we were a computer so we don't know better. So we'll also look at cow even though it's zero and of course that doesn't get counted. Okay, so we'll do another example to a few more so you have in this kit. we'll just look at the highest one to be a little quicker. So this is an image of a fox. The predictive probability for Fox is greater than the threat class threshold for Fox. So we're going to count this as given label Fox and true label fox in our Matrix and you see that number gets updated to one. Okay, and we're going to keep going through and doing this Is anybody. Is anybody like me to go through one more kind of slowly? Okay, yeah, this data is like our training unit, right? Or it can be either training. it can just a data set. Any data set can be training data or test data and the model that we like. What this data could have been used to like train the model that we are using. Now yeah. so if you want to get the predictive probabilities out of sample, then you would train on like two-thirds of the data set and you'd predict on the remaining one-third and then you would then choose a different two-thirds of the data set and predict on the other one-third and you could do that three times. And then you would get predictions that are out of sample. So you get predictive probabilities that are out of sample for your whole training set. Yeah, Um, so we'll look at each example and we'll see which predicted probability is greater than the class threshold if any. And that's how we we compute this. Matrix and so we would do this in a you know, a computer can do this very quickly, right? So you could do this for millions of data points Very, very quickly. Like in less than a second? Yeah, available. Um, yeah. So what we do is we if that's called a collision. Um, if you're curious and because this is assumed to be a single label data set, then we would only choose whichever one was the highest. So whatever the highest Gap is and there's different methods, there's different ways to approach this. If it's a multi-label data set that's handled differently, you can have two labels for a given image. Yeah, that's a really good question. It's like a that was a good catch. Um, Okay, so this is the dog. You know we will count this one dog dog. so this will be on the diagonal and then this cow is has a very high probability of cow so it'll be on the diagonal. Now this image is interesting. Does anyone see why this is an interesting image? Yeah, Yeah, totally. And it's also just like a farm so it doesn't have it, doesn't have anything in it right? So the probabilities are low and so what happens is none of these probabilities exceed any threshold and it just doesn't get counted and so the computation of this joint distribution is actually robust to outliers. Okay Cool. So let's just look. I'll just run the whole thing now. so I'm gonna I won't I won't be controlling this I'm just going to press go and you'll see this build up. and the off diagonals are label errors that are found in the data set and the diagonals is correctly labeled data and so we'll just let it go. So does that make sense? Yeah, usually you you hold it in some outlier set or like at a distribution. there's also we'll we'll look in next week we'll look at outliers and auto distribution data and uh yeah, there's a bunch of ways to rank data and it actually is kind of similar. It's basically looking at like how uniform these probabilities are. There's a bunch of methods we'll look at those. Oh, that's good. You're thinking I like it so it will work. Uh, so the reason is because the thresholds are just averages of the predicted probability. So if your thresholds are really low, that means that your printed probabilities are really low. Uh, the only way that can happen is if you have like a lot of classes. Um, for them to be low for every class, you'd have to have at least like at least more than two classes, right? So um, what happens is if you have really low probabilities, then when you're doing this comparison here, it'll actually be a very small probability and also a very small threshold. and so relative to each other. There's no difference. That's just because the threshold is an average of those small numbers. That's why the algorithm is constructed the way it is. It's pretty simple, right? Like, maybe simpler than you expected, but that should be good. Any questions? Okay, so let's look at some cool stuff we can do with this. So here are the counts and then you can normalize by you. Basically just sum them up and you can divide by the total and you can do a few other tricks to normalize the fewer things. But we'll skip that for now and this is what you get. So this is your joint distribution and this joint distribution will tell you like a full characterization of the label noise in the data set. Um, and I Think it's worth mentioning that everything that I showed you. that's one simple version, but there's you can do a lot more. You can expand this and this is just like one line of code and that's pretty nice that you can run on any data set and so you can play with this and then the lab assignment. You will play with this this code and so you'll Implement A simple version, but you'll definitely be able to just do everything. I showed you in a single line which is useful. Okay, you can also rank label errors. So what I showed you is how to find them specifically on the off diagonals. But once you have, remember our first question. the loss. Like if you rank things by loss, how do you know where to stop? Well once you have this this Matrix estimate. What do the off diagonals tell you? So they tell you the label errors. So if you were to count those up, you would have an estimate of how many label errors are in your data set and so say it was 52. Okay, then you can now Rank by your loss and you would take the top 52 with the highest loss. Does that make sense? Okay, and I'll just really quickly show two other ranking functions. So another one is you have the probability of your given label. So some given label. um, you know For for some example, that's in the data set of uh, where your given label is like equal. Let's say class. I. Then you would say what's the probability of I Given you know that example and this tells you what is the sort of self-confidence of the model of the given label. Does that make sense? So this is actually a score of how confident the model is in the label. So this is one that means the what is it If this is one, Is it likely to be a label error or is it not likely to be a label error? Yeah, That means the model is like as far as a model thinks, it's 100 confident that the given label is is the right label and we'll that's one. So this is called self-confidence and then there's another way you can rank errors and this by the way for cross entropy. Uh, if you're familiar with that loss function, this is the same as the loss in some settings like for multi-class and then one other one we'll look at is you can also. So this is the probably the given label but you can also do the same thing. So probability of the given label say the given label was I. This example is an Uh is in class I and then you can do minus Whatever the ARG Max is okay. Um, or let's just do the max of the probability of Y tilde over like all classes and whatever your set of classes is. So this is your basically your biggest probability. Okay, and so what this does is it gives you a score that takes into account how confident the model is that it's something else and it also takes into account how confident the model is that the given thing is true, all in a single scoring function. So this one's really good. if you have a single label data set and all the label issues are correctable. Does that make sense? This is pretty useful. So you could just sort all the data by this score and then you could take the top. You know, 52 by using that previous method that we found to know that there are 52 label issues and then you would take just what are the 52 with the smallest? It's called the normalized margin. Are you guys with me? Is it cool? Is this interesting? I mean at the culminating points And so I Want to make sure that it's clear Any questions samples besides just lost the self-confidence and that's their willingness. Uh yeah. So the normalized margin and you just clarify like how what they're what they're doing? Yeah for sure. Yeah, so what you do is you have let's say you had like three. You've got. Let's say let's say you only have three examples. Okay, so we just have uh, let's I'll just call them like example One, example Two, and example three and then what we do is for each one. Okay, so we'll look at example one. We were just let's say that we're doing this one. We would say okay, this is example one is labeled uh like dog So we'd say what's the probability of dog because it's labeled dog for that example and then for this one, say this is labeled dog two. So we'd say what's the probability of dog for this one. And so the given labels dog given labels dog and we'll say the given label here is cow. So for this one, we'd say what's the probability of cow and then what we do. So this gives us scores of how confident the model is and whatever the given label was and then we would just sort them and now whatever the smallest one is is the one most likely. As far as the model is telling us, that is a label error and that's it. And then for this one, it's the same thing. except you just take this number here and you also subtract out whatever the highest probability was as long as it's not the same. So if it had a really high probability of being Fox for example, then you would subtract the probability of fox and this will tell you the confidence that the model thinks it's a dog minus how certain it is that it's actually Fox. Oh yeah, my notation here is super shorthand. If you want a better notation, we can totally do that. but I can also just show you. Is it okay, Okay, or we can just show you so this is going to be the max over Y equals I Given some example X and some data set, sorry if you can't see this I'm going low to try to dodge the board. um X and uh, this will be y actually equals J and this will be J inside whatever your set of classes is. So like if your set of classes is a zero one two up to however many classes there are n minus one and then this will be X in class like I and also I is not equal to J I Just didn't want to write all that before because I didn't want to be too confusing. but basically it's any other class that's not I We're going to look at all of those. We're going to find the max value of those for your example, which is labeled high and then we're going to take that value and subtract that. Cool. Okay, so you have like at least three methods to find label issues and label errors and data sets. Now you can Rank by those two things and then take the top number of issues. Or you can just use the off diagonals of that joint distribution we saw. Sorry, That's cool and it works for like any model and can be done automatically so that's cool. Okay, all right so we've seen this graph of other methods. What I wanted to show is something pretty neat. Um, the on. Actually I think yeah, I'll just show very quickly. So this column on the right is saying that 60 of the off diagonals in that joint distribution are all zero. meaning most of the noise is in just a few flipping rates, so that's much harder. But it's also much more realistic, right? And so what you'll see is most methods actually have a drop off in performance on tasks where you're trying to learn with noisy labels as you get more realistic data. And so you want to use methods that will be robust to this. and what's happening is we're actually estimating those off diagonals. And because we're estimating the off diagonals directly, it's working well for when you have like, uh, the noise rates only in a few of the off diagonals because you've estimated them directly. So I just wanted to show that these methods that we are showing are outperforming other methods, even in more realistic settings. Okay, um, cool. So some quick intuition on like why this stuff works. Um, what we've shown is that the method I just showed you will can exactly find label errors in some cases in real world data, the probabilities are usually pretty bad so you won't get exact label error finding. but if the model is like really accurate and the predicted probabilities are really good, like you'll get pretty close to real exact label error finding. Um, and so what we can show is just a quick intuition. So these things I'm going to skip: There's a bunch of principles and papers behind the method I showed you you Can check those out later if you're interested. But I want to show you a quick key idea. So the prior. The way: if you've ever heard of like modifying the loss function in order to deal with noisy labels, this is a pretty standard thing you would do. So if the model had like a very high loss on the example, then what you would do is you would actually just not. You would down weight that example during training and that's a way that that's sort of a model-centric approach and what we want to see is why is this data Centric Approach probably working better than the model Centric Approach So this is sort of your predicted probabilities and then you have some weight that's dependent on that predicted probability and if your predicted probabilities are low for that example then it like if it has a very low self-confidence then you would actually down weight the loss function. that way you don't train as much on that example because you think it's wrongly labeled. so this is the standard. This is not what we're doing, but this is an approach that's commonly taken in the model-centric point of view. and what was what I want to show you is that when you do this, that means that any error in your model's breakthrough probabilities and if you're not familiar models don't give you perfect predicted probabilities. it's just an estimate. They can be pretty noisy. all that noise now is now multiplied directly into your loss function and propagate it directly into your loss function if you're familiar with neural networks and back propagation. And so now when you do your SGD you're updating your weights of your neural network. All of the error that you just introduced into that loss function gets directly propagated into your weights. and you're just going to get a noisier model. And so this is the downside. Basically, the more erroneous your predictive probabilities are, there's little. there's no support for robustness to that in the model-centric perspective. but if you just didn't have those data errors, those labelers in your data set at all, Or if you had corrected labels, then you wouldn't have this error propagation. And so that's just a a way to think about this problem and why it's a good. It's a good idea to actually correct the data versus doing it for a model-centric perspective. Does that make sense? Okay, sweet. so that's one big takeaway. So yeah, pruning the errors avoids re-weighting and that avoids air propagation. Another big sort of takeaway. Um I think for a sake of time I'll let you look at this one on your own if you're interested, but you can see sort of how we can be robust and separate Ali torque and epistemic uncertainty. And then there's one more I Want to show you I think is pretty compelling. So um, there this. There's this idea that the Um like what would be a perfect predicted probability for a model. So what we've seen is a method that works for like real world data and real world models where you have imperfect breakthrough probabilities but what would be the perfect probability predictive probability for example. and as long as we're assuming that sort of class conditional error then the perfect probability would actually be the flipping rate. So the predicted probability for some example X that's labeled I where the true label is J is actually just whatever. the probability is that that J gets mislabeled to I actually takes a second to wrap your head around. but just there is a notion in this setting of like a true predicted probability And what we've shown is that confident learning can can exactly find labelers and the methods I showed you can exactly find label errors when you have perfect predictive probabilities. So let's see what happens when you have imperfect predictive probabilities and that'll be what I'll write on the board now. okay so um oh but first I'll show just a really quick inter intuition. So if the threshold is 0.6 and your sort of perfect probability is 0.9 then you know basically your your actual probability that you estimate can be up to 0.3 wrong and you will still find this as a label issue. So that's the intuition of why this method is robust to noisy model outputs. Does that make sense? So in the previous, remember before I was saying that like the loss function propagates the noise when you do model Centric But from a data Centric perspective because we're using this sort of thresholding, we just have robustness to these predictive probabilities. and then we're going to toss that example out, totally not train on it and not propagate the loss. But the key idea here is just that we're using thresholds so we have robustness so it doesn't have to be an exact value and we don't use the exact values and this thing can be like up to 0.3 off and you'll still get it right? Okay, so the question is, does this result? Can you still exactly find label issues even when you have uh, like miscalibrated models or your model is really overconfident in some classes? And this goes to your question that you asked earlier like what if the probability is really small for some class and so we'll look at that now and I'll just write this on the left board. I've given this board too much love. Um, so here's our here's our confident joint. So this is just that. Matrix of counts. Sorry to introduce a new word. this is just the Matrix of all the counts and this is just a set of things right that are labeled uh as I that have some probability. Uh, J that's really big, right? Okay, so that shouldn't be new. We've seen that before. Um, can you see? Okay, it's all right. Okay, and then we know our threshold formula is just an average right of all the things that are in class J Our class. let's do class I. Um, and we're going to average over all those examples. Okay, so this should look familiar, right? So we have just the average of all the predictive probabilities for class J for all the things that are in class J And then we're going to divide that by this should be a J sorry by the total. So it's just an average and we've seen this formula before, too. Okay, so then the question is what happens when we take, uh, all the predicted probabilities for a class. Uh, uh for class J And we make the model overconfident. So we just add error. All right, That's what you want to see. Does this thing still exactly find label errors? The method that we looked at today And so we'll do that. Now, let's take our predicted probabilities here and let's just add some air to all of them. So the model is like really overconfident in class J And so it gives a high probability for everything in class J. Okay, it's like super sure of dogs for example. Or just like it always thinks everything's a dog. Okay, and so what happens to this threshold when you do that? Well, this you're adding this term within this sum and so you actually get that the new threshold uh, with respect to the error is actually equal to the old threshold for that class. Plus the error term. Does that make sense? Okay, and so now let's go back to our our estimate of all the errors and what we'll see is now we just have this: This probability has changed right now. This is going to be increased. um by this Epsilon term, right? So this or sorry, let me get rid of that. So this threshold now has been updated. The new threshold is threshold plus Epsilon J but also all the predictable predictive probabilities increased by Epsilon J And so what happens is they cancel out and you get the same formulation that you had before, which we proved exactly finds labelers not in this lecture. but you can check that out in the paper. and so you can just see that even when you perturb all the predicted probabilities or you make the model overconfident, you still get the same formula formulation. It's robust to that. That's a good thing. So you can have like a miscalibrated model. or you can have an overconfident model in some class and it should reasonably work. Well Still, okay, does that make sense? All right. Um, say in this slide, you can check this out if you're more familiar or if you want to I'm going to go about 10 minutes over. so thanks for everyone who's who's still here, but there's some cool stuff I Want to show in the results that you can get once you have this set up and thanks for going through this slowly with me. I Hope the pace is the pace Good. It's okay. Okay, um, all right so we've seen a lot of intuition I Think it's cool to actually see some results and failure modes, so this is an example of it'll. mostly be like results from here on out now that we've seen the methods and how to think about them. So this is sort of, uh, this is all synthetic data and you can generate these with like Scikit-learn these are four data set distributions and then uh, nine different types of models and what we wanted to do is just run the method I showed you on all of these different data distributions and using all these models. And this sort of just shows two things. One is that this works with any model that can do predictive probabilities and two is that it should work somewhat reliably um in many situations and so you can check this out later in the slides, but you'll just see that you get like a reasonable performance in learning with noisy labels. Using this approach. in all these settings, these are Easy Settings So this is not showing that this solves challenging problems, it's showing that it works reliably In a lot of settings. and then it's model agnostic. Okay, there's also a bunch of hard examples. Um, and you can. It's just always good to keep in mind when you're talking about label errors that sometimes there really isn't any true label. And so those examples. We'll talk more about those in future lectures like Outliers and out of Distribution. But these are all things that were found when we ran this type of method on a bunch of data from like Imagenet and Mnist. And they're just fun to look at. So like, is that a potato Or a pear? like Imagine if that was your final exam question and getting that right determined. Like whether you pass like you'd be pretty upset, right? or just like any of these. Like is that on that left you know that's like, is that a sewing machine? It's kind of weird, right? I Have a fun question Anish Helped me find this one. So this second example here. uh, can anyone tell me if that is automobile or an airplane or neither or both? Yeah, is that right. a B2 Airplane? Yeah, yeah. I Think that's right. Yeah, Okay, that's very impressive. Yeah it's from inside the cockpit looking out. Were you able to tell that or we? Oh okay. I'm not sure if it's a B2 but it is from with inside a fighter jet and it's like looking out through the window. But yeah, that took us like I Don't know how many hours we spent just to answer that one question, you know? Um, so anyway, data's hard and sometimes there's not a clear true label. These are the results from the labelers website. If you're interested, you can see that there are a lot of test errors across these and you can the method that we looked at today, how many uh, label errors are sort of guessed by that method and you can use that to then estimate the fraction of these test sets that are erroneous. So you can just keep applying this and we're gonna. We're getting more meta now, right? So what we're seeing is for example, like Google's Quick Draw data set has a minimum of like 10 error and this is a minimum because we didn't check all the data and so there's probably more errors that automated methods uh, like what we showed today don't find um so I Just want you to see what were the results from that And then the question that we're interested in is, if you know that a bunch of test sets in machine learning have label errors, then how does that actually affect us as a field? Like how are the benchmarks affected if you're using test data that is erroneous. So our practitioners, people who use machine learning uh, you know, every day and are using these test sets, are they actually benchmarking on erroneous test sets? and does that affect those benchmarks? And we'll see that actually it does. and it can lead to issues when you your model is not doing what you think it's doing and you can use things like confident learning and error estimation to be able to figure out like hey, is my test set actually pretty accurate or should I consider improving the test set so I can get better benchmarks. So this is a data set and we'll look at sort of a traditional view of a data set. You split it into a train set and a test set and then you train a classifier. You measure that classifier's performance on a test set. In this case, it gets a hundred percent accuracy and then we'll look at a real world view. We actually have some label errors cases they said before. Now we have labelers. So what's inside is what's observed and the outside is the ground truth. So this is ground truth. but you have labelers. outside is the ground truth. Inside is the labeler. And then we have some test set that we get from that data set. A real world data set that we split into train and test because that's what ends up happening is people Benchmark on data that has labelers in it. And then you create a train and test and now your test set has label errors in it and as to your question that you asked earlier. So now what happens? All right? So we train a classifier on this training data that has label errors in it. and then we predict on some test set that also has labelers in it and we see that we get 100 accuracy right? So we have a trained model that has like 100 test accuracy. But then you have some real world distribution that actually the real world doesn't have any notion of label errors. The car just drives off the cliff or it doesn't. You know it's not like it's labeled card. You know there was a cliff here or not. the car like literally will just go over and that's what will happen in the real world. And so the real world is always true in terms of what what happens, the evaluation is just like what happens in the real world. So what you notice is that the real world accuracy is actually 67 percent. So this is a big problem. You thought your car would never drive off the cliff and it's driving off the cliff like you know 30 of the time. Okay, so that's not good. That's what we want to make sure is not happening and in some cases for for data sets with a lot of mislabel data that's going to happen and so we're going to see that that's been happening. Um, I'm going to skip some of the details of how we corrected the data set, but it's in the slides we used Mechanical Turk and we had some agreement, but we did some approaches with real folks and we simplified the problem to get a good set of reasonably accurate correct labels. They're not perfect, but they should be much better than what the given labels were. and then we use that to find out how many examples we can correct and then we want to use these corrected these corrected labels in order to figure out if we have correct labels. Do our benchmarks change compared to if we use the original test labels? All right. So are we all together and what we're doing? We have test sets. We want a benchmark on those test sets. Originally the test sets had label errors in them and we benchmarked on them and we saw Which models were best. Now we've taken those test sets, we've corrected them. We found they had labelers. We corrected those labelers and now we want to see do benchmarks change? Okay and what you will find across the board in like papers and research on this topic is that if you look at the entire test set or validation set, you'll see that actually it's a linear line. original and true. meaning that the rankings are the same. There's no like big ranking change. So all these dots are models and you're plotting on the y-axis the axis on one data set like either the original labels and then the corrected labels or two different types of validation sets or whatever. And people have noticed that it's always linear Meaning, there seems to be no change and so, uh, you know. We were curious and we were like, okay, well, it doesn't seem to have an impact, But then we ask the question, what if you actually just look at the examples that were corrected? Okay, so just the subset of examples that were corrected. how does the model's accuracy change based on that And so we'll look at that in a sec. So here we have on the bottom we have just the correctable set. Okay, so this is the set of all the examples that could be corrected and on the left, this is also the correctable set. So these are all the things that could be corrected and what we see is on the left. This is the accuracy of the corrected labels and on the bottom is the accuracy of the original labels. And you see how this exam does. Anyone know what Nasnet is? Uh, Resna 18. Anybody heard of Resident 18. Okay, Resident 18. I See some Nodding Heads is a very small model and Nasnet is a very big model. It's huge. It's like a meta learning model, so Nasnet should be much more complex and able to fit to more distributions. But it also if there's a lot of label noise in your training set, it will also learn that too. And so what we noticed is that things like Nasnet, which normally would greatly outperform Resnet 18 as a much smaller model. If you, if you, what you'll notice is that Nasnet actually underperforms Resnet on the when the labels are correct because it's over fit to that noise in the training data. But when you look at Nasnet versus Resnet on the original labels, that's outperforming. So can somebody tell me why this this big, really powerful, awesome model is actually outperforming. A simple model. Are underperforming a simple model on the corrected labels? Yeah, a simple model doesn't get a chance to overfit to the incorrect or labeled data. Yeah, that's what we think and that's a hypothesis. and there's some other reasons too. But that's like a reasonable hypothesis. And then you'll see that that big model does outperform Resnet. It's a very low accuracy, but it's overfitting because it is learning some more of that noise and the accuracy is really low. Because these are all wrong labels, They were all corrected so like it shouldn't Ideally, it's learning something so it's not taking making too many mistakes. but it makes. uh, it's It agrees with the wrong labels more than the smaller model. and so this is interesting, right? Because you actually have this opposite effect where the the better performing models are actually doing worse than the smaller performing models. So your benchmarks are completely inversed. Uh, and so that's the problem. And so then and so we were curious. Like does this matter for just Imagenet and Cfar And it turns out that you get the same thing and you can see that in the slides. But the last thing I want to show is what actually happens. Is there a certain amount of noise at which point that Benchmark starts to get flipped and like common data sets we work with. And so we took a look at Imagenet for example. And so this is Imagenet when you look when you increase the amount of noise in the test set and the way we did. this is just by reducing the good data. We knew stuff that was wrongly labeled and we stopped. We started reducing the good data until we increased the amount of the proportion of the test set that's wrong. And on the left, what we have is your accuracy on the original labels and you'll notice that things like Resnet 50 and Resin 18. These are two different types of models. Resonant 50 is bigger. It usually performs better. You'll notice it just keeps performing better than Resin 18 and this is the graph you would expect. The better model always outperforms It's always on top of the smaller model. So the question is, well, what happens if you change the Benchmark so you're looking at original labels and what if you instead change it to the corrected labels and then you'll see a very different thing happens once you've corrected the labels you'll see once you get to about like nine percent noise. it starts out at three percent. So if you just bump up the amount of error by six percent, you'll see that actually Resnet 18 will start outperforming Resnet 50. And this is on the corrected labels that you don't have unless you correct them. And so the the benchmarks that we were doing and we meaning as a field in machine learning, we're actually benchmarking on data. And if image that for example, it had six percent more label errors. It's a pretty highly curated. It has a lot of issues, but it's a pretty highly curated data set. But the point here is that in the real world data sets tend to have more label noise than in like academic data sets and so six percent increase in label noise you would actually have What you would observe if you hadn't corrected your test set is that Resnet 50 outperforms Resin 18, which is what you expect. But what's actually happening underneath with the true labels that you don't have unless you correct them is this graph, which is that Resnet 18 starts to outperform Resonant 50.. So the question is, if you correct the labels in training Yeah, not necessarily yeah. So if you corrected the labels and training and also correct the labels and test, then ideally you just get the better model, trains, better on better data, and then You if you have a corrected test that your benchmark should be valid too. So there you're addressing both problems and removing like data distribution shift between the two. But a good A good question is is a lot of this happening just because you're changing the test distribution relative to the train distribution and the answer is sure. But the thing is is what we want ultimately is our test data to be as close to real world as possible. so we don't want test data that matches training data and distribution which is what you're commonly told. It's like what we always learn traditionally in machine learning. but if your your training data actually has a whole a lot of errors in it, label errors and like bad data that's not reflective of real world performance and your goal is to see how well does my model do in the real world. you just want to test data set that reflects that. Yeah so this is the sort of the last result I Wanted to share with you guys. there's some inclusions that I'll let you all check out if you want to look at the slides after and then I want to share briefly what is our the lab for today? So on the lab there will be a pretty easy and simple data set that's great. This is like you know for exams and student data and there's three exam scores for each student. Each row is a student and there are some notes and then there's a letter grade. so like what was the grade assigned to the student and the later grade is noisy so sometimes the Tas they were sleepy, they had been up too late and they made some mistakes on the letter grades. This is a synthetic synthetic data set. This has been created for the purposes of the lab and also for other demos, but you'll see that if you use things like confident learning or clean lab that's why I Wear the shirt today then you'll see that you can get an improvement on these label errors. So or on on your model that you're training with noisy labels and so what. this last thing is showing is some examples of like some label issues that are in the data set so you can see for example like this person got a zero on one of the exams but they still got to be and they even cheat on the exam but they got you know they gotta be so you'll find some interesting things in the data. You'll be able to improve the model quite a bit and it should be fun. Um, any questions so big takeaway. You should feel pretty good now about different types of label issues and data and some ways to find them and how to estimate and characterize the noise in a data set. and once you've found them, you can remove them, trained on clean data or you can correct them, Train on good labels and get better models and the lab will help you explore what you learned in this class in practice. See Thanks guys!</div></div></div></div></div></div><div contenteditable="false" data-content-editable-void="true" style="width: 0px;"><div style="display: none; flex-shrink: 0; pointer-events: none; width: 0px; position: absolute; right: 192px; opacity: 0;"><div style="display: flex; flex-direction: column; padding: 5px 16px; width: 340px; flex-shrink: 0; height: 100%; position: relative; pointer-events: none; z-index: 1;"><div style="position: absolute; pointer-events: none; width: 100%; height: 100%; top: -5px; background: linear-gradient(white 0px, rgba(255, 255, 255, 0) 15px);"></div></div></div></div></main><span style="height: 1px; width: 1px;"></span></div><div class="notion-presence-container" style="position: absolute; top: 0px; left: 0px; z-index: 89;"><div></div></div></div></div></div><div class="notion-peek-renderer" style="position: fixed; top: 0px; right: 0px; bottom: 0px; width: 960px; z-index: 109; transform: translateX(960px) translateZ(0px);"></div></div></div></div></div><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><textarea aria-hidden="true" style="opacity: 0; pointer-events: none; position: fixed; left: 0px; top: 0px;"></textarea><script src="assets/js/1172e9111a5fb396bcb8a05870b5eabf8abf221c.js" type="text/javascript"></script><div style="width: env(safe-area-inset-bottom);"></div></body></html>